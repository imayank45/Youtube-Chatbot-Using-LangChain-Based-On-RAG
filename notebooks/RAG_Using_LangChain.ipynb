{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install necessary libraries"
      ],
      "metadata": {
        "id": "r14idy9EJAv0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vBYR3jQI8Mp",
        "outputId": "3722b702-af90-4011-acc8-f39cb8ba0528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/485.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.1/485.1 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.2/471.2 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.4 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q youtube-transcript-api langchain_community langchain_openai faiss-cpu tiktoken python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "import os\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "S_fLI7hGJs6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "s9AknDpFSHt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step1: Indexing / Data Ingestion"
      ],
      "metadata": {
        "id": "9VjQCREiK8rz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Loader"
      ],
      "metadata": {
        "id": "gKeX7ChnLCt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_id = \"p3sij8QzONQ\"\n",
        "\n",
        "try:\n",
        "    # Fetch transcript directly (returns FetchedTranscriptSnippet objects)\n",
        "    transcript_snippets = YouTubeTranscriptApi().fetch(video_id, languages=['en'])\n",
        "\n",
        "    # Flatten into plain text\n",
        "    text = \" \".join(snippet.text for snippet in transcript_snippets)\n",
        "    print(text[:500])  # print first 500 characters for preview\n",
        "\n",
        "except TranscriptsDisabled:\n",
        "    print(\"No captions available for this video.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7y0erFuKXOf",
        "outputId": "c95d9e80-b79e-4c2b-c866-0eec87155a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learn to build a complete large language model from scratch using only pure PyTorch. This course takes you through the entire life cycle from foundational concepts to advanced alignment techniques. You'll begin by implementing the core transformer architecture and training a tiny language model. From there you will modernize and scale the model with production ready enhancements like RO mixture of experts layers and mixed precision training. The course then transitions to the full alignment phas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "y1RBoubfMRNz",
        "outputId": "22f49fc9-b1cc-4b2d-e57a-286ed24b71d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Learn to build a complete large language model from scratch using only pure PyTorch. This course takes you through the entire life cycle from foundational concepts to advanced alignment techniques. You'll begin by implementing the core transformer architecture and training a tiny language model. From there you will modernize and scale the model with production ready enhancements like RO mixture of experts layers and mixed precision training. The course then transitions to the full alignment phase where you'll implement supervised fine-tuning and build a reward model. To complete the life cycle, you'll use proximal policy optimization or PO to align the model with reinforcement learning from human feedback. By the end, you'll have the deep hands-on experience needed to build and customize your own LLMs. >> Hello everyone and welcome to LLM from scratch, a hands-on curriculum in PyTorch. Uh this is going to be a long practical journey where we build modern large language model components entirely from scratch, one step at a time. I'll be speaking at a fairly slow pace. So if you're comfortable, I'd actually recommend watching this video at 1 and a half times or 2x the speed, but just do whatever feels natural to you. So why this course? Now there are many tutorials that show you how to use large language models. Um there are many who show you how they are built too. So u the likes of Umar Jam, Sebastian Rajka have all been a source of inspiration to me to learn how LLMs work and they've built some superb material around this and I highly recommend you check them out as well. Uh the objective of this course however is to show you the end to end workings of modern LLMs in one place through code. We will use the minimal necessary force when it comes to theory. In this course we'll start from the absolute foundations. How transformers work how the attention is computed how to train a tiny toy model on your laptop. And then step by step we'll add all the modern tricks. Rotary embeddings, RMSOM, mixture of experts, supervised fine-tuning, reward modeling, and finally reinforcement learning through human feedback. By the end, you won't just know how to run a model. You'll understand the building blocks of what makes models like GPT work, and you'll have implemented them yourself if you follow along. Now, let's talk about setup. The requirements are actually pretty modest. We need a laptop with Python, PyTorch, and optionally CUDA if you have a GPU. But a GPU is not mandatory to follow along. You can do everything that we do here on your CPU itself. They just run slower. Uh here's how the course is structured. So we'll start with part one, the core transformers uh core transformer architecture. We'll implement the core transformer architecture from scratch. uh the attention heads, the multi-headed attention, the feed forward layers, the positional embeddings, the layer norm, and stacking everything into one block. Part two, we'll actually train a tiny LLM. Uh we'll do this on a small text file. Uh but we'll do all the bells and whistles around the tokenization, the batching, we'll understand the cross entropy loss, the sampling, and the evaluation part of it. Uh part three will modernize the vanilla transformer architecture. Um we'll include RMS normalization, rope, swigloo, KV cache, uh sliding window attention, and more. Uh part four, we'll scale it up. So we'll we'll go from bite level to BP pair encoding tokenization. Uh we'll add gradient accumulation. Uh we'll explore how to do mix precision uh training. uh we'll do checkpointing logging. All right. So part five will be mixture of experts. Uh we'll do some theory over here. We'll explore the routing, the gating, the stability tricks and hybrid dense plus blocks. Part six will cover supervised finetuning. So we'll dive into u instruction data sets, formatting prompts and responses and evaluating outputs. Part seven, we'll implement a reward model using pair-wise preference data sets and we'll explore a few loss functions like Bradley Teddy and margin ranking loss. Uh and then finally in part eight we'll bring it all together with RHF using PO proximal policy optimization adding a value head to our policy scoring completions with our reward model and optimizing a KL penalized objective. This is the same basic recipe that part the first versions of Chad GPT. This course is designed to be hands-on as possible. Every part comes with a modular well commented code that you can run, tweak and extend. So if you see here, uh we've got all the parts figured out. I'll link the GitHub in the description. So every part consists of uh each module that we're going to pick up here. Um it's all organized into orchestrator.py. That's your entry point. So if I look at part one, I'll try to find this orchestrator.py. Part two, same thing goes. So the orchestrator.py will be the starting point of every module and then you can work your way inside from there. Again, as we'll explore each and every module, things will become clearer. That's the road map. I hope you're excited to dive in. Grab your laptop, set up the environment, and let's start building from scratch. Welcome to part one of LMS from scratch. Uh in this video we'll talk about uh the overall architecture or the workflow that we'll follow along with hyperfocus on just the transformer part uh the vanilla transformers as we know it. So uh if you look at this this GPT assistant training pipeline and it's split into four major stages and we'll cover all of them uh in our series today. There's this pre-training. That's step one. Now, this is where you spend the most amount of money. Uh this is where uh a sizable chunk of the internet gets processed to tens of trillions of tokens and then they uh bring out what is called a base model which then is used for further processing. This is the step where thousands of GPU and months of training and all of the big models come into the picture. The supervised finetuning stage is the second stage from there. Uh relatively sparse data requirements and compute requirements and this is where we go from the base model behaving like a purely next token predictor to more like an assistant. uh from there the alignment stage starts where uh you don't want the LLM to present biased political views or u something along the lines of uh abuse and quality and security. So you come up with a reward modeling step and then finally the reinforcement learning from human feedback. Um in this video we're only going to focus on the vanilla transformer as the paper um in the year 2017 attention is all you need came out with. Uh so we have on the one hand the transformer architecture. Uh this consists of the encoder and the decoder. We'll only focus on decoder only because LLMs are mostly decoder only transformer architectures. So the input is first tokenized and then it goes through an embedding step and then the embeddings go through a positional encoding step and then from there it's split into three parts to Q andV and the zoomed in version of this box masked multi head attention is over here. So the three splits that we had for the embeddings, they go through their own linear layers and then a scale.product attention concatenate the output of all of the heads, all of the different heads and then finally through a linear layer. Now this is the zoomed in version of the scale.product product attention where the QKV goes through a matrix multiplication and then a scale component and then uh we will use the mask and the filling component because we are focused on the decoder only architecture. So if you were to implement an encoder on the architecture, you won't be focusing on this and then finally put it through a softmax and then matrix multiply it with the V and out comes uh so whatever comes out of here is actually this and then it's all concatenated. So the different heads spit out different outputs that are all concatenated sent through a linear layer and then finally whatever comes out of the multi-headed attention is then um added or a skip connection is present with the previous input and then normalized. So we go through layer normalization in this case then through a feed forward and out with the softmax and the loits and probabilities. Uh again I wanted to keep this um a very theory light course. So we'll only go through the bare essentials of what is required so that we can proceed towards implementation. Now this entire block is called the layer of a transformer and you can stack up however many layers as your desire to implement a bigger and bigger network. So the more layers you apply the bigger your model gets. So we'll jump right into the code with that perspective and then hopefully once you see how the attention mechanism and all of the neural network is configured it'll provide you a better perspective. So we only going to focus on part one and this is the repository layout for part one. Uh as always for each part the entry point will be orchestrator.py which runs the different demos and the tests and you can go doubleclick deeper uh from the orchestrator.py Pi treat that as your starting point. So we have the different modules here. So we got the POSOS encoding. Um there's a small attention numpy demo for perspective. What a single head of attention looks like. A multi head, a feed forward network. Um what the entire blocks block looks like, the attention mask and some visualization utils to help you visualize things along the way. Um so what we first do is uh we add in this argument uh for visualize and it will write out some mattplot lib uh visualizations on our out folder. So if it doesn't exist we make them and then first things first we run the uh dummy attention demo but we'll skip that for now. We'll try and understand the actual attention mechanism that we just studied. So uh it all starts with demo mha shapes.py. So we'll open that up and first off we import the multi-headed attention and then we set the hyperparameters for our model. So B is going to be the bat size. T is going to be the sequence length of a particular row of data. You can assume this as the context length. The D model is going to be the embedding dimension. And then we also define the number of heads um that are going to be configured in our scale.product attention mechanism. Now the core idea is the D model gets split into um the number of heads that you configure here. So if your D model has a dimensionality of 12 and your N head is three then each head it'll first be split into three heads and then each head will be of size four. So it is essential that they be divisible by each other otherwise an error should be thrown. All right. Uh so we make some dummy inputs. So x equals torch of u random distribution random normal distribution of the batch size the token sequence and the dmodel and then we pass it through a multi-headed self attention. So uh in order to invoke a block for the multi-headed self attention we take a dodel and the number of heads that's all that is required. We log things along the way and once we run this we'll be able to visualize all of the output. Now what I've done here is uh written all of the log output in a file so that it can be studied later. So once our attention block is defined we pluck out the QV component of it and then we run it through our input. So imagine this to be the output of the embeddings of text. So whenever text goes into a large language model first uh it's first tokenized and then from there each token is converted to an embedding dimension. We will study these two things in great details uh later down the parts but we are not going to build uh tokenizers and an embedding uh layer in this particular part. So we uh push it through the QKV part and then we get our QKV back. Now we'll actually have to split it to get our uh individual QKBs. So we'll have to configure them right along bat size token size and then three of n head cross d head. So um if you have n number of heads it will be split along um the number of heads that you have and the d head that you have and the combination of the d model. So once you unbind your QV, you actually can unpack your individual Q and K and V. Uh the Q and K will get transposed ready to be matrix multiplied and then we compute the scale. So again if you focus on the equation over here then the Q gets multiplied by the transpose of K and then it is scaled by D of K. Now the DF model is actually the global dimensionality of the embeddings and DK is the dimensionality of each attention head that you use. So in our case uh our embeddings are configured to be of 12 embedding dimension and then we have three heads. So D of K in our case will be four. So we configure the scale parameter like so. So it's the inverse of the square root of each dimension of a particular head and then we finally compute this part. So the Q transpose and then divided by the scale parameter. So we're focusing on the expression over here. So if you see we are matrix multiplying Q with a transpose of K and then multiplying it by the scale. Now since I've already taken the inverse as our scale I need to multiply it and not divide it. So I've computed the part where Q and K transpose are computed along with the scale. So I'll take the soft max of these scores and then directly multiply it with the values matrix which will give me the output. So um once I get this output, this is the output of the attention mechanism. I just have to push it through a feed forward network and then finally I get my output. Right now we skipped a bunch of steps here. uh we haven't gone inside what a multi-headed self attention is and what are all of the different components like the attention mechanism the QV and the project we'll do that right now so this file multihead.py Pi represents one multi-headed attention module. So this is again importing from the NN module and the important thing to understand here is the flow or the dimensionalities that change along the way. You'll get an input of bat size U sequence of tokens and a D model as input. Now once you push it through the QKV you're going to make three copies of X of the above matrix. So what you will end up with is a batch size token but three times D of model. So you'll get three copies of the QV and then you push it through the three linear layers and then you uh get your three individual Q K and V where each is N cross head times D cross head for each token and batch right so there'll be four dimensional tensors over here and they will be split along this dimension the QV then uh we'll have to transpose them and then compute the Q * KRpose and then scale it by the square root. We've already seen this before. What you'll be left with is again a batch of N head cross T cross T D T D T D T D T D T D T D T D T D T D Tensores and then finally we'll have to soft max. We'll have to apply soft max on top of it and then multiply it with the weights matrix which will give us a batch of N head cross D cross D of head. Again, if you follow the simple rules of matrix multiplication, you'll be able to get this very quickly. And then finally, uh the merge step happens where we concatenate all of the different heads together. So this will simply be along the third dimension the number of heads that were there and the dimensionality of each head which will bring us back to the DF model. So this is the output of every multi-headed self attention mechanism. Uh if we go through this, we initialize our module through an assertion statement first and if this itself is satisfied uh is not satisfied, it will raise an exception saying uh the D model must be divisible by the end head. So nice guardrail to have. uh there's an end head which is passed by the um user when they are invoking. So in our case uh we passed the n head as three. Uh then there's a d head and d head will simply be the dmodel divided by the number of heads the quotient part of it. That's why you see the double slashes. Then there's a qv. So each Q and K and V will have their own set of linear weights. So what this takes as input is a D model and it has weights outputting three times D model. Right? So uh we're going to do this matrix multiplication in one shot. So we don't have three linear layers for each of Q, K, and V, but we have one linear layer that represent all of Q, K, and V. and then we'll split it in retrospect and that's why um post the QKV happens there's a view and a split method that we're going through after this right u from there the projection layer is just a linear layer u optionally you can keep a dropout so uh it's configured to zero by default so we won't have dropout unless you configure it explicitly and then trace shapes true uh literally gives you some um some good bookkeeping facilities of uh plotting and printing all of the shapes so that you can track them along the way. Now this is what a forward pass looks like for a multi-headed uh self attention mechanism. So we have the BTC uh split into uh the X dot shape and then from there it goes through the QV. We rewire the QKV to unbind it to give us our individual Q and K and V. Then we apply uh transpose to all three of them. And then finally we configure our scale matrix multiply Q and K and then put it through a causal mask. Now this is the important part where we had mentioned that there's a masking step. So Q and K gets multiplied and then gets multiplied by the scale and then there's a mask step that is optional uh which is nothing but the upper triangular matrix. Uh so this essentially is a boolean value that will return true if you configure it so and then that true will change the matrix you apply it on and set it as zero. So let me try and explain this a little more. So what we want to essentially do is if we have a matrix like so and this has let's say various elements in it and I want to apply a mast fill that essentially means the upper half of this diagonal should be set to zero. So this part so what we are essentially implementing is we invoke an upper triangular matrix that simply says uh true in these cases. So it's basically um a matrix that is filled with booleans where u the elements that we want to essentially remove are set as true and then when we do in pytorch what is called a masked uh fill then we will pass this matrix over here configure it with this the mas fill we'll uh we'll see this in a minute and then What will happen is whenever this is true that particular element is set to zero. So if this is true this will set to zero. If this is false the element will stay as is and the values won't change. So in our case uh we will substitute it with minus infinity because soft max of minus infinity simply gives you zero which again implicitly turns off u all of our parameters to zero. The reason why we do this is uh in large language models, you essentially don't want to see the tokens that are further away or in the future of the current token that you're processing. So if you have a sentence uh the cat sat on the mat then if you're focusing on this token and trying to go through your decoder only architecture to get an output you don't want to look at these tokens right so uh if I'm processing these tokens all I can look at is uh the previous tokens including itself so if I'm processing this token I can look at everything here and previously till my context window. So by the time I get to Matt, I'm probably only looking at this window, right? So I if there are words beyond the word Matt, I cannot and should not be able to look at them. So that's the core idea of uh the mask fill. So if we come back to our code, uh the causal mask function simply returns a boolean of the desired dimensionality uh where it fills up with once and this will be treated as a uh boolean for whether to apply a mast fill. So we'll watch that in a minute. So I take my causal mask over here and then I do the attention matrix that I got from previously and then I do a masked fill with the mask as the input. So I'm informing where all I want to replace this matrix with minus infinity. So uh where all this mask matrix will be true the attention matrix the positions in the attention matrix will be replaced by minus infinity. And then when we take the soft max of the attention matrix the minus infinity will go to zero. And that's how we we call it turning uh the attention off or turning these tokens off. Then finally there's a dropout. Again in our case it's not applicable and then we matrix multiply the weights by the values matrix. Again if traces is true we print the shape. We'll watch it and then um we'll reconfigure it to put it through the projection layer. So here we're transposing uh the dimensionalities one and two and then making it continuous. So um fun fact uh transpose actually uh breaks memory allocation in PyTorch and that might be a problem when you're doing from when you're trying to do further processing down the line. So it's actually essential for us to apply this uh continuous part right. All right. So torch dot tensor.ontiguous is easy to understand. It returns a continuous inmemory tensor containing the same data that you pass in. Uh if it is uh already in the specified memory format, it returns the tensor itself. So it'll be the same data but the memory allocation will be continuous. Now why do we need this? So the view here will just uh return the same data but with the reconfigured dimensionality that you specified and for view to work you actually need the matrix to be continuous. Now if you want to avoid using two operations uh one after the other you can also use reshape which returns a copy of the data. Now this is a nice thing to do. It saves us some memory but I don't think it matters eventually because uh continuous uh again reallocates the data in memory and then view uh just rewires it with the same data. I think you can achieve the same thing through reshape and then finally we return the output uh which brings us back to our multi-headed self attention and everything else follows uh like we just discussed. So uh in the multi-headed detention we went through the causal mask and that I think was the only piece that was missing for us to understand uh this properly right uh couple of things that we have skipped is the positional encoding. Uh so let's go through that. So the idea is uh whenever um you send text through large language models again they get tokenized and there's an embedding but they also incorporated with positional embeddings and this will give it a sense of uh it matters a word appears in what position. So uh what we have implemented here is an absolute positional encoding. Uh which essentially means that uh if you again have a sentence the cat sat on the mat. then this position will be given a certain positional embedding and that will get added to the original embeddings of this word itself. So if the cat has a certain embedding it will be added to u this the positional encoding that we have um come up with for the even position or the second position. So there'll be one for the third one for the fourth and so on. So each position will have uh an absolute positional embedding and it doesn't matter whatever word comes its embedding will be added to that corresponding positional embedding. So how do we calculate uh the positional embedding? Well, there are two ways to do it. Uh we'll go through the syosidal positional encoding because that's the that's the one implemented on the 2017 paper. Um I've got one more that you can go through. Again, we'll eventually evolve this to uh doing uh rotary positional embeddings. So, might not matter. Um so, so the for the forward uh we take what you give me. So, this will be B cross D cross D model and then I'm just adding uh my positional embedding that I have for uh those particular dimensions. So, I come up with torch to zeros. So this is for the context length and this is for uh the embedding dimension and then I arrange them. So this a range goes through zero max length and then unsqueeze uh meaning it adds one more dimension to uh this particular vector. Uh and then this uh calculation again it's mentioned on the paper. So I have the original paper open in front of me and here's what the positional encoding suggests. So given a couple of formulas uh again there's a formula for odd positions and there's a formula for even positions. Uh so depending on uh the word was seen on the second, fourth, sixth or uh the first, third or fifth uh the positional embedding will be slightly different. So here uh we come up with a uh division term and then depending on uh whether it is the odd position or the even position we apply uh sign or cos uh then position times the division term exactly as it mentions on the paper and then uh we register this as a buffer. Uh this is another interesting Python property which essentially says that uh this will be used in the forward propagation but we don't need to store the gradients. So these are not learnable parameters they are uh calculated once and they are absolute. Uh so we uh again take the words tokenize them uh create embeddings and then from there uh we add our uh sinosonal positional embedding to get uh the inputs for the attention mechanism. Uh finally in order to visualize the attention mechanism better I've also implemented this in uh numpy. So let's say the output of the embedding uh is our x will be bat size one and a sequence of uh three tokens or sequence of three and a dmodel of four. So uh each token will have four dimensions here. So those are the three tokens that are coming in as input. Uh I have the uh linear layers WQ, WK and WV. So I'll multiply them with the individual ones. Uh now remember that in our PyTorch implementation we did this in one shot and then split the QV. Uh we're doing the opposite here. Uh just so you have more visibility into this. Uh and then we finally uh print the different shapes. Uh we calculate the scale by hand. Uh and this is just uh I think a more simplified and a better view that you can run over and over again uh in numpy in order to uh in order to visualize the attention mechanism better. So feel free to tinker around uh once you get a hand on this. And then finally uh we machine learning engineers a sub uh a subset of software engineers uh further hate writing tests but I think they help a lot. So I've written a couple of tests here and there will be some tests that wherever I feel uh I struggled with it personally I have included unit tests so that uh you can try them on your own as well uh just to make sure that the math is correct on this one right so uh the x goes as input and I come up with random qv now what I'm trying to test for uh is the shape of it right so if I set a seed uh I get a tensor uh x. So I create a torch tensor out of my numpy array. Uh there's a single head self attention mechanism uh that I've implemented here. So this is what it does. So it's basically uh a subset of the multi-headed self attention where uh we don't do the multiple heads. It's just one single head. It's just a simplified view of the multi-headed attention. Right? So if I come back, so we take a single headed attention uh block and then the Q in the single head self attention uh will be filled by these weights. So the WQ over here will be copied over to the weights of the Q that we have configured here. Again, if I open the single head, uh one of the components is the Q part. So the weights of this linear model will be copied over to the Q part in this attention mechanism. So the same goes for K and V. Then finally we run it through the attention mechanism to get the output and the weights and then we assert that the shape is going to be 1a 3a 2. Again uh based on our inputs uh this should be the right answer and then hopefully everything works out. So what we'll do now is also go through some visualization uh utilities uh that are written. I won't be going through them in detail but this will help us visualize attention better. So without further ado uh we'll actually uh run the code and then see the output for ourselves. So I'll go uh Python orchestrator demo and then I will pass it through the visualize parameter. So again, if I come back to the orchestrator, all of the instructions on how to run and what to pass are uh given over here. So I'm already inside my part one folder. This is where you need to run it. And then once you run it, um it runs the attention numpy demo that we've already seen. So it goes through each and every matrix uh giving you complete white box visibility into each shape uh so that you can visualize it yourself. And then finally we run our demo multi-headed attention shapes.py. So the input over here goes as uh 1A 5A 12. So where 1 is the batch size, five is the token sequence, the number of tokens and 12 is the embedding dimension of each token. Now if you push that through qkv what you'll get is three times uh what was there before. So 1a 5a 36 that checks out. Now if I have to uh split it into QV, I need to view it in five dimensions. So I take this and I split it into one which is the batch size. Five again is the token sequence. Three is literally hardcoded to Q and K and V. The second three is the number of heads and four is the dimensionality of each head. And then from there each q k and v will be 1a 5a 3a 4 respectively. Uh again if I transpose only dimensions 1 and 2. So these five and three get swapped to 3 and 5. So we'll get 1354. And then from there uh if we multiply Q and the transpose of K then we'll get 1355. Again Q with the transpose of K. So uh this dimension along this dimension if you multiply then we'll get uh 1355 uh and then finally we apply the soft max that does not change the dimensionality. Uh then we'll multiply it with the v and v is again 1a 3a 5a 4. Uh we multiply that with again 1a 3a 5a 5. If we do that uh we'll be left with 1a 3a 5a 4. uh then we merge each of the heads. So with each head uh we'll get three separate uh 1x 3x 5x4 matrices and then if we concatenate them along the dimensions then uh we'll get 512 and the final projection will be uh 1512 and then we've included a small legend for each of the acronyms and what they stand for and then there's a uh there's an entire visualization module as well. So if I come back to VS code uh you will see that there's an out folder which also logs uh everything to a file so that uh you can view it. It's more organized that way. And then uh for each head we had three heads. Remember for each head the key and the query have been uh put together as a scatter plot. Now since again we are using a decoder only LLM uh you'll see that uh the upper triangular part has been turned off. Uh so for each uh token so we had five tokens going in so 0 to four uh you'll be able to visualize the strengths of the weights between the key and the query and that will again give you more perspective uh on what you're doing. Right? So that brings us uh to the end of the first part. Uh so we started with um the decoder only attention mechanism. Uh we have not covered tokenization and uh uh creating embeddings should be fairly simple but we've parked it aside for uh later down the parts. Uh we went through the most vanilla positional encoding. Uh we opened up the multi-headed attention to understand uh the different the QKV and the concatenation uh all of that and then we opened up the scale. product attention mechanism as well. Uh went through a bunch of math and hopefully now you have good perspective on how to implement your own attention mechanism with PyTorch. Hello and welcome um welcome to part two of the LLM from scratch series. uh in this part we are going to try and train a tiny LLM uh with bite level tokenization data set batching and shifting for next token prediction cross entropy loss uh we'll uh do the training loop from scratch no APIs required uh we'll look at some of the generation parameters like uh temperature top k top sampling then evaluation based on a validation data set so as always our part two also starts off uh with the orchestrator py where uh the entire layout uh is laid out like so. Uh we'll go through some of these aspects and then go very deep inside uh the train.py where we'll be spending most of our time. So the orchestrator.py runs three scripts. Uh the training.py the first parameter for which is a data set. Now you can take any text file that is lying around on your laptop. Um I've chosen Hindi. Um there's a bunch of poems over here, but I've also included in the repository some English text um that you can use in order to train your model. So once the data is locked in, we're going to pass on these hyperparameters. So we're we're going to be training for 400 steps. We're going to sample after every 100 steps so we can evaluate for quality. We're going to evaluate after every 100 steps. Our bat size is going to be 32 and our block size is going to be 128. Block size is essentially the context window of the operation. Uh we're going to have uh two layers or two blocks, two transformer blocks in our model. Uh we're going to go for multi-headed attention where the n head is two and the embedding size will be 128. So without further ado, let's start from the train.py. So uh I've got an exhaustive list of parameters here some of which uh we've passed to from the orchestrator.py. So the data the out directory I've hardcoded uh to be the runs folder where we'll save our model weights. Uh the block size we passed is 128 batch size 32 and layer 2 and head two and embedding which is our embedding size is 128. you've chosen not to use dropout, but you can. Uh the steps, uh the learning rate, the weight decay. Again, we're going to be using the atom optimizer. Uh we'll also use uh gradient clipping to stabilize our training so that uh one particular batch of data that was really off from the rest of the training data does not throw our model. Of course, uh with eval interval, we're going to uh set it to 100. That's what we passed from the orchestrator.py. by the validations. Sample every that means uh every 100 steps or so. Um we'll take the latest checkpoint uh and then sample a bunch of tokens just to see the quality how many tokens we want to sample the temperature top k these are the generation parameters if this is passed no matter whether you have a GPU or not this will only train in CPU uh whether we want to compile our model or not. Um and then AM stands for uh automatic mix precision training. So we'll explore all of that as we go along. So first things first, if you have a GPU lying around, it'll be automatically picked up. So we're saying it's CUDA if torch.cuda is available and argu.cpu. So this has to be uh false uh and uh you have to have a GPU available. then uh you default to CUDA otherwise falls back to CPU. So the first things first you we're going to make a tokenizer the bite tokenizer and then we're going to make our data set which is uh this text file based on our tokenizer and then we have a GPT model. So let's go very deep into these three aspects first and then we can come back and continue the flow. So first things first the bite tokenizer. So this is the implementation of the most basic tokenizer uh that is there. Uh it literally takes every character and treats that as a token and it will return the uh the bite information of that uh position in the UTF8 space. uh and then what will happen is uh you'll have a very finite vocab length but you will have a lot of tokens to process. So this is not exactly the kind of uh tokenizer you would want to use in an industrial strength setting. Uh but it's a very good tokenizer to get started with and further down the parts we will explore u uh an industry grade bite pair coding tokenizer. We'll go through the training and the merging steps as well. So this has an encode step where I'm literally returning the UTF8 encoding of that particular string which is a list of characters and then decoding is the exact opposite right so uh I'm returning the character of a stream of bytes if passed to me and then it has uh a property that we can use called the vocap size. So that's our tokenizer. Now next I'll need to take my text file and create a data set out of that based on this tokenizer. So here I've got data set.py. So here uh literally what I'm doing is uh taking the path of the text file that uh will be passed and then we'll read the bytes information off of it. So uh in this case we don't need to use our tokenizer because reading the bytes itself ensures that the text file is getting encoded in UTF8. So the data variable will just be a stream of bytes that are already tokenized given that we are using a bite tokenizer. Uh we convert that to a torch tensor and then in order to train test split uh we're going to take a 9010 split. So 90% of our data will go on training the rest of it in validation and then the block size is nothing but the context window. Now in order to sample a batch from this uh what we're going to do is uh you're going to send a parameter called which choose to so this will either be train or validation uh and then the batch size along with the device. So I'm taking a buffer here says uh self train if which equals train else self. And then I'm putting in a small assertion here that the length of this buffer should at least be more than the context window that you've chosen. So essentially when you get your hands on this piece of code, the way you want to run it is you want to choose a data set whose 10% is more than the context window that you've set. So 10% of this text file should be more than your context window and that will ensure that uh this condition will be satisfied. Now if it isn't the case you can just take um the subset of the block size that's fine. Uh so from here uh we're trying to get a random batch and there's a trick that we're using over here. So we'll take a random integer from zero to the length of the stream minus the block size. And then from there that integer is seeded from the start to a block size for the number of indices uh that you sample over here which is essentially this batch size. So this will return 32 integers and each integer will be between the starting point which is zero and then however many characters you have in your text file minus the block size. Let me try and explain this further. So let's say you have 1,000 tokens at your disposal in your text file and you've set the block size as 128. Now each of these 1,00 tokens or characters will have their own token ID. So there'll be a list of tokens uh a list of 1,00 tokens. So it can be 0 1 and so on. Right now uh I want to make sure when I sample an integer I sample an integer from 1,00 minus 128 minus one. Now what this will return is an integer between zero to this. So what I can do is now let's say this is uh iix. Now I can take this random integer and then take it all the way from this integer to the uh block size that I've set. What this ensures is it does not overflow. So if let's say you sample the 999th character here, uh I don't want to add the block size and then spill over this list. So I'm keeping a buffer somewhere here. So that even if it samples the last integer from here, I still have a lot of leeway to go till my bat size. So I select 32 such integers and then from there I take them from the index to the index plus the block size uh for each element in my batch. Then I stack them up and then what I want to do is for every token X its corresponding Y will be the next token. So the starting point for my Y will be I + 1 and then I + 1 + self.block block size. So that sort of plus one is uh taken care of here by the minus one. So again, we're just trying to ensure we don't spill over. So we put it onto that device and return that as a batch. Uh so we're back to train.py. Hopefully we've understood what the tokenizer is and the bite data set is. Now another thing to keep in mind is that you would not be keen to use this again uh uh at scale because we're literally loading all of the data in memory. So what you will have uh as an example will be partition park files probably if you're dealing with data at scale. So you want to load it partition by partition as a batch and not read the entire data set in memory and it'll just be you'll just get an out of memory exception for the terabytes and trillions of tokens you'll be trying to process. So then finally uh we get to our model invocation and for that uh we're passing the vocap size which in this case we're taking from the tokenizer and if you remember we had set that to 256. So our vocap size will be 256. Uh the block size we've set it to 128. We're only going to do two layers, two heads, 128 embedding length. And then dropout will be default which is zero. So we're not using any dropout and then transfer that over to the device again CPU. So let's dig deep uh inside the GPT model that we've got. Now the the heart and soul of this GPD model is of course going to be the attention mechanism that we've already studied in the previous part. So we'll just have a very summarized uh explanation for the causal self attention part but then we'll also try and put that together to look u to see how the entire model looks like. So for example we did not cover uh layer normalization as part of our previous part. We're only focused on the attention mechanism. So, uh some of the properties that we need to set for this class are coming in from uh the parameters are the initialization itself. So, we'll set the block size. Uh we'll set the token embedding where the input to this embedding module is the vocap size and the output is the embedding length. So, in this case, 256 by 128. The same goes for the positional embedding. And then from there uh drop out optional. Then we have a list of modules which is nothing but blocks. So in this case our end layer is two. So we'll come up with uh a module list of two blocks over here. Uh and then the pytor layer normalization is something that we've chosen to use not implemented from scratch. And then finally uh the head which is the linear layer goes from an embedding to the vocap size. And this is what will eventually get decoded. Uh this will be the loits. Then the loits will go through a softmax and then diffused across probabilities. So uh we also uh apply the initial weights. So um if uh we see that a module is an instance of a linear layer then we uh initiate that uh initialize that with uh a mean of zero and a standard deviation of 0.02. So as per a normal distribution uh if the bias is not none I think we have not chosen any bias for this. uh we just initialize that with zeros. And then if it's an embedding, we're going through again an initial of uh a mean of zero and standard deviation the same as the linear layers. You can choose to mess with these uh and see if your model converges faster. Now we'll go through the forward loop and then uh we'll probably park aside the generate once we get to the sampling. So what we are going to get as part of the forward propagation is uh a batch and each batch will contain a sequence of tokens. Now what I want to ensure is that the sequence of tokens that we get at each row does not exceed the context window. Now if it does in this case I've chosen to raise an error. But if you want to let this pass silently, what you can do is clip those sequence of tokens as per the batch size. So uh you can do something like um if t is uh greater than equal to or you may even need the if because you're clipping, right? So t can just be minus self.block block size and then a colon. So it ensures it takes the last 128 tokens and does not spill over. I've chosen to uh raise an error. Now uh for the boss variable I'm doing an a range of 0 to t. So getting a placeholder for myself uh for the position. And then from there uh this is going to give me all of the positions. So uh for a batch again if you have u a sequence of tokens one has three one has five one has seven it will give me a matrix uh that is full of uh 0 to 5 0 to 7 uh depending on your sequence length. Literally just give gives me the integer positions so that I can use them to compute the positional embedding. Uh I unsqueeze it. So squeeze reduces the dimensionality. Unsqueeze introduces another dimension. Just uh helps with the math uh adding the dimension. So I'm going to take my list of ids and then pass them through the token embedding, pass them through the positional embedding, and then I'm going to add them up. Uh drop out if any. And then uh once I have the embedding with me, I'm going to pass them through however many blocks I've configured here. So in this case only two. So uh again the variable needs to be the same is an important aspect of this because the input to the second block will be the output of the first block. So you want to keep the variable names consistent here. And then once it goes through the blocks it goes through a layer normalization. Again ln_f is nothing but a layer norm. And then from there we use the head which is the linear layer uh to get our loits. Now if it is a training loop there will be targets associated with it and we can compute our cross entropy loss uh against uh the loits and the targets otherwise uh a none will be passed back which essentially indicates we're trying to inference and not uh train. So uh what we will do now is go inside each block and open it up. So this is what a block looks like. And this is essentially the block that we have seen before in our part one. So this is the part I'm talking about. This one is a block. A block consists of addition and normalization layers and a multi-headed attention which again opens up over here which leads to a scale dotproduct attention here. Now I have chosen to uh add the layer normalization before we apply the attention. You can choose the reverse as well. So the input comes in, it goes through a layer normalization step and then it passes through the attention mechanism which in this case is a causal self attention mechanism and then the output of that is added to the original input. This is where the skip connection happens. Right? So the input is added to the input layer normalized and then passed through attention. So that's your skip connection right there. And then this is in turn passed through another layer of uh normalization uh and then a feed forward network and then again a skip connection component to it. Then finally you return the x. So this is what a block looks like. I'll correlate this with uh the diagram again for perspective. So we go through the multi-headed attention or the causal self attention as we call it. uh the X or the input is passed through and added to the output of the mass multi-headed dimension and then it propagates like so till it hits the feed forward uh network and then it's further added and normalized. So the input to the feed forward is added and normalized with the output of the feed forward here as well. Now if we open up the causal self attention mechanism uh things should be extremely familiar. So uh we again asserting like the previous part we did that the number of heads is evenly divisible by the embedding length that you have set and then from there uh we're going to take a QV which is three times the embedding length and then uh go through the split over here. This is just a copy paste the exact same thing that we've seen before and then finally it goes through a scale. product attention a pytorch implementation this time and then uh the transpose continuous view which then goes through a projection layer to return the y. So again if I were to correlate with what we've seen before so it goes through the qkv all of these are just one linear layer and then we choose to split it and then the scale dotproduct attention uh concatenated with the linear layer. So on the feed forward uh network it's just a simple sequential neural network uh with a linear layer uh that blows up the embedding length by four. So uh n embedding comes in four times n embedding is the number of neurons in the hidden layer. It goes through a gaussian error linear unit activation. It's just a smoother version of relu. Delu has hard cutffs which again messes with the stability of the training and then it goes back to the same uh linear uh but in reverse almost uh resembling an autoenccoder and then a dropout. So once I have that I hope everything here starts making a lot of sense uh so we can get back to our uh train.py. So we've gone through the tokenizer the bite data set and the GPT model. So we can proceed to how our training loop looks like. Now uh if I pass the compile argument as part of the training, I will compile my model. Now in a sense uh a small sidebar over here is uh torch became popular uh because it was dynamic and it was much more debugable uh eager execution, right? So you can build the neural network and debug things along the way literally with the inbuilt Python PDP Python debugger uh which TensorFlow did not have in the initial years. It was just uh a session graph that was very hard to debug. So somewhere with torch.compile uh over the years torch has moved closer to TensorFlow 1.0's initial behavior and with TensorFlow 2.0 into introducing eager execution. TensorFlow went back to um more simulate the behavior of torch. So I think somewhere u these two packages uh are converging and I don't subscribe to the pytorch versus tensorflow uh cult debate as you want to call it. Uh I think you should use the right tool for the job as all right. So um you can choose to compile your model that makes it a static graph. Again uh it can um it can sort of uh find its own set of optimizations by uh fusing layers and making things more um more faster basically. So from there uh I'm declaring what my optimizer will be. In this case it's an atom optimizer. So pass the model parameters and the learning date uh the betas and the weight decay again pass through an argument. Now this next one is going to be interesting. So what we are going to be attempting is an automatic mixed precision training. So uh PyTorch is aware of what all modules that you're trying to use in your neural network. So especially for the linear and the convolution layers, it understands the fact that uh reducing the precision of um the weights from FP32 to FP16 will significantly speed up training. But what can happen is uh we can encounter what is called a vanishing gradient problem. So if you take an FP32 weight and then approximate it to FP16, you might lose a lot of precision and the weights can go to zero and we call it an underflow problem, a gradient underflow problem. In order to tackle this uh what PyTorch does is uh it provides us with a gradient scaler. So what it will do is uh scale the loss with the help of this gradient scaler back propagate and then scale the loss back so that you don't need to adjust the learning rate yourself. So in this way uh your gradients won't underflow and your training will be stable. So you have slightly more uh perspective here um on the pyarch documentation itself. So if the forward pass for a particular operation has float 16 inputs and the backward uh for that operation will produce float 16 gradients uh we'll have small magnitudes representable by float 16. These values will flush to zero or the underflow. So the update uh will be lost. Now to prevent this uh it will scale again uh gradient scaling multiplies the network's loss by a scale factor and invokes a backward pass on the scaled losses. Gradients flowing through backward u flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude so they don't flush to zero. Uh we'll see elements of this uh in the code. So we predeclaring our scalers. so that we can have stabilized nonvanishing uh automatic mix precision training. Uh we set our best valuation to float of infinity um time step and then set the model to train and then the training loop starts. So we'll go from step one to whatever however many steps that uh we have configured. So for each step we're going to get a random batch from the training data because this is a training loop. Uh our batch size is 32. So we get the corresponding x and y and again a quick refresher. So get batch literally returns the x and y with all the uh list slicing gymnastics that we've gone through over here. Right? So we get a batch and then uh we wrap it up with the am context manager here. So we say torch.guda.mp autocast uh enabled. um we go through the forward propagation. Now again uh doesn't matter if you're even using a CPU and this won't apply but it's nice to understand uh how a real world training actually happens. So we go through the model and now this will just invoke the forward propagation of the GPT model with all of the layers that we've already discussed. So uh we have the loss over here because we have chosen to pass the targets and then optimizer.0 grad so that u the gradients can be set to zero and they can be computed. Now this is where the magic happens. I do scaler.scale the loss and then do a backward propagation. Right? So this is an important step. Uh again this is only because we're doing am. If you don't choose to do AM uh good luck blowing your memory and not using or not introducing this unnecessary complexity. Uh if the gradient clipping has been configured to be greater than zero and this is the other side. This is the exploding gradient problem where uh I'm passing a gradient clip and if that is set to greater than zero which it is the scaler will unscale and apply that to the optimizer and then uh torch has a nice uh clip grad norm uh over here which again acts on the parameters making sure uh that it clips the gradients corresponding to each of the parameters. is if they are greater than that particular value. So in our orchestrator I think uh we pass one or by default it's configured uh as one. So the gradient clip here has been configured as one so that we don't make uh dramatic updates uh to our weights. So it will get clipped and then finally the scaler.step uh with the optimizer and the update will happen. So uh just a small step that we had to introduce uh to sort of show you how uh you can train your models with the MP applies to pretty much uh all settings doesn't have to be localized to LLM training in general. Now every 50 steps uh we will log uh so that we can track what are uh corresponding losses and whatever metrics we are interested in what they are. Now if I'm to configure this uh eval interval which I've set it to 100. So every 100 steps I will estimate my loss uh print it and if that loss is less than the best loss that I've seen so far. So essentially think in the first step this will always be true uh because we've set the best val as plus infinity. So the best val will be set to the validation loss that we're seeing right now. uh the model will be saved uh in the checkpoint path. So we have the runs folder over here where uh the PyTorch weights will be saved. Uh if the directory that you've configured is not present, we'll just create it. And then we're saving a bunch of these parameters with the model state dictionary as well so that we can exactly reproduce our checkpoint when we're loading our model. So with this um we've completed the training loop and now uh we will sample uh so if you've configured the sample every parameter then uh we will actually sample and print stuff out uh the console um based on whatever parameters that you've uh configured right so let's say your sample every is greater than zero and you've set it to 100 so when we hit the 100 step. Uh this will get invoked and this is again um a way to get the um a list of uh indices that uh that serves as a seed for our prompt. Uh this is the exact same implementation that you have already seen in the get batch uh get batch API. So we start from the zero we go through uh we go till the training length minus the block size so that we can sample a block size and then we take an item and then from there uh we take or uh we take a subset of the training data from that integer to the block size uh unsqueeze it so that it matches the dimensions and this becomes our prompt to generate and then we do model generate uh we set the seed set the max new tokens. Uh this is again coming from an argument. I think this is set to 200 by default. Uh we're setting the temperature top p top K. And then uh we're decoding everything back to text and then printing it. So uh finally we're saving the model once we train it. And then uh this should just be over uh the training loop. Right? So what we'll do now is go deep into how the generate happens. uh if you remember we had uh taken the model.py and skipped the generation step entirely and this is where uh it's sort of useful. So when you load the model from the checkpoint and are trying to generate a bunch of tokens uh you can set certain parameters and what I'm interested in explaining the most is the temperature top k even if you've uh heard it and you know what it does it'll be interesting to explore how uh these things are achieved. So we have the generate and from the utils uh we'll explore the utils in a minute. We've imported the talk of P filtering we are setting the model to eval. And now if you do not pass the prompt uh so in this case from the train.py I am passing a prompt which is nothing but the seed but if you invoke the generate method uh separately from another module and you send an empty prompt I'm just filling it with the default list of prompts so that generation happens anyway and nothing gets errored out. Um so in our case um it won't be an empty prompt. So for the max number of tokens that you've considered uh I'm going to clip off the list of ids that you have sent me by the uh context window. So again, if you send me a large enough prompt, I'm just going to uh start from the end and go back all the way till my block size number of tokens are satisfied and then clip the number of tokens and then just process them. So from there uh this list of ids uh goes through the forward propagation which will give me the logits and since I'm only going through the generation the loss will be none. So if you look at the forward API uh takes a bunch of indexes uh forward propagates them through the model and then uh returns the loss if only the targets are there otherwise returns a none. So I'm not interested in uh saving it. Now here is where something interesting happens. So we take all of what we get back and then we slice it like so. We're taking the zero dimension and the second dimension as is. But the first dimension we're only taking the last element and then doing something to it. Now in order to understand this we really need to understand how logits are returned uh given a batch. Right? So now let's say you have a batch of uh tokens. So let's say this is your batch right? Now when you forward propagate this patch through the model what it returns is actually so if you pass uh in this case four in this case three in this case uh five what it gives you back is the exact same number of tokens but just right shifted. So if the position of this token is 0 1 2 3 then the predictions that you will get for this corresponding sequence is the prediction at position one uh given given the token 0 the prediction of position two given the token 0 and 1 the prediction at token 3 given the token 0 1 and two all the way till uh goes till four. So since we have four tokens over here in the input, it will also give me uh four tokens in the output and this is the extra token uh that I'm interested in. So again, if you have configured this to be uh 200 output tokens, it will give you 200 more tokens here, but it will also preserve all of what you gave in the input. Uh again, just right shifted. This is an important point to understand. So that's why the batch is a dimension but then for every sequence it also returns the prediction for every right shifted position. So that's the second dimension and for every position prediction that you get it will actually be a long list of all of the vocabularies that we have configured. So in this case we have 256 uh tokens which is our vocap size. It will return the logets. So from here uh what we will do is uh we're going to make a note of the batch dimension and uh each and every token. Now if you're going to sample these tokens one by one which is what happens in an LLM, right? So if you've passed 0 1 2 and 3 as an input uh what will happen is uh the fourth token will get uh you'll get as output and then the entire sequence will be passed and then from there the fifth token will be sampled and so on. Right? So when you want to do that for a particular prediction, you just want to grab the last token in the sequence. That's all you're uh interested in at this particular point. Right? So that's why uh when you're looking at this so we're only grabbing the last token which is the literal prediction uh that we've gone through. So again we're doing predictions given each token. So we're running this loop for each new token prediction and we're only interested in the latest token which is nothing but the prediction token. Now this prediction token will have in our case 256 loits which is our vocap size and then I'm scaling it by the temperature parameter that you have passed. Now note that the implementation of the temperature because it's a division operation is a max of temperature and a very tiny number. This is just to protect the zero division error if you pass temperature as zero. Now as far as my understanding goes and I can't confirm it because uh it hasn't been brought out um but the closed source uh LLMs that you use u the Geminis and the OpenAI chat GPTs of the world have a similar implementation which is why even if you set the temperature to zero they do not guarantee a greedy token selection approach. Um it's based on a best effort basis. So the proof for that is uh I've literally opened the Gemini API docs. So uh the temperature is used for sampling blah blah blah uh where it says is lower temperatures are good for prompts. A temperature of zero means that the highest probability tokens are always selected. In this case responses for a given prompt are mostly deterministic but a small amount of variation is still possible. And I think it's because of the one E minus 6 um a very tiny uh number that we've passed. When this happens at scale, there is a possibility that uh there will be slight variety in the generation even if you set the temperature to zero. So we've done our best to adopt uh what might be going on in all of these uh uh industrial strength implementations. Uh so we're going to take the loget and diffuse them by temperature. Now you can easily see that the higher the temperature uh it gets the logets gets uh diffused more and more. So uh lesser probability tokens will have relatively higher probabilities now or they'll have an even playing field when you're trying to sample tokens. So that's why um for anything creative uh it's recommended that you use a higher temperature. So it will sample uh more unseen tokens in the training data. But if you want to be conservative really use a low value for temperature and then from there uh it goes through the the top kit top filtering that we have configured. Uh we'll look at this in a minute the implementation. Uh so we've got the utils.py PI and this is our top kit of P filtering mechanism. So what we have is we're getting the logets and we need to uh have a filter mechanism. So let's understand the top kit of P first uh and then we'll probably come back to the implementation. So what we have uh is at uh every step you will get uh 256 predictions. Uh so for each token that you want to select you have uh a candidate list of uh in our case 256 tokens which is the whole cap size. Now what is a uh top k filtering is an extremely easy thing to understand. So if you set the top k as 10 it will uh sort it by the probabilities. Uh reverse sorted by probabilities choose the top 10 and then sample any one from this distribution of 10. So instead of sampling from 256 tokens weighted by their probabilities, it only samples from uh the 10 that you've configured as top k. Now the top t can be set to something like uh 0.95 for example. What happens here is again you reverse sort uh your list of tokens list of candidate tokens by the probabilities and then what happens is uh you want to uh you want to compute the cumulative sum. So let's say uh this is.1 this is 2 uh this is 01 and so on all the way everything will sum up to uh 1 because we're looking at a softmax distribution and then from there uh the idea is to reverse sort it first. So the 0 2 comes up first the 0.1 then the 01 then and then the rest of it and then uh you calculate the cumulative sum. So at the first step it'll be 02 at the second step it will be.3 at the third step it will be 0 uh 31 and so on and then you want to take it all the way uh till.95 98 and then maybe we hit the one. So the idea is to cut off here since our top piece is uh 0.95 and then sample only from this distribution and then uh discard all of this uh all of the long tail below. So that's the core idea of topk. Let's understand how this is implemented in pytorch. So in PyTorch uh I'm first implementing the top K if configured if even if it is or not uh I'm also going for the top P implementation. So first things first I get my loits and take a clone of them. Now if the top key is not none and the top K is less than the vocap size itself. So by mistake if you have set the top key to be north of 256 nothing will happen here. If it is less then it will get filtered. So torch has a nice talkie function where I can pass my uh the copy of my logits and then pass the number of tokens I want to filter out from them and it automatically handles the sorting and the filtering. So from there uh what you essentially want is uh you want to take the top values and those that are not the top values. So you take the cutff here um you set it to float of minus infinity. Again uh this is a neat trick that we have seen uh so that when we pass it through softmax uh it just becomes zero. So that's the top key implementation. Easy enough. Now for the top P we're doing a little more uh custom handb implementation. Uh so we're first sorting them in descending order and then from there uh we'll need to compute the softmax and then we are computing the cumulative sum of the probabilities. We are creating a mask off of the cutout that you've specified in the top P and then uh we're setting uh the first token to false so that we make sure at least one token is selected. So if you set your top P to be a very small number and not even a single token is selected, we make sure that the first token is selected. And this is a uh again a very cool implementation uh in PyTorch where you can take a tensor and it doesn't matter how many dimensions it has two or three or four you keep the rest of them as is and you only deal with the first uh index of the last dimension. That's all you're interested in. Right? So with that uh we apply the mask. Um so we set everything to float of minus infinity. Again uh this is the same as top k. Now in top k we had set certain tokens as minus infinity. And then in the top p also we are setting certain tokens at minus infinity. So uh we're going to just combine all of that. Uh this is the scatter step. So uh from here uh we are doing uh uh taking a replica of uh the filtered matrix uh filling it up with minus infinity and then using torch.scatter to stab the values of minus infinity with the sorted loits and uh sorted indices that we've got. So what does scatter do? So it writes all the values from the source tensor at the indices specified in the index tensor. So uh we're using the tensor.scatter. So this will be the source and then from there uh I pass on the indices where I want to write the dimension along which this is supposed to act. Uh and then it should just work out. So uh I want to do it along the dimension one. These are the indices and these are the values which will be imposed upon this filtered which has been initialized to be uh minus infinity everywhere. So it returns the filtered matrix after the top k top p implementation. So you see uh we're done covering this step and then from there uh we take the softmax of the logits to get our probabilities. Uh and then now we know that these are the filtered versions of those uh probabilities where the top p top k has already been applied. So we're going to sample from this distribution based on these probabilities. So I passed the list of probabilities and for the tokens that have not been selected with the top p top K scheme the probabilities will be zero. So we we've ensured they'll never get selected. I only want to sample uh one from here. So in implementations like beam search you might want to sample more than one. Uh we are not covering beam search. Uh we've left left it as an exercise. Uh so from there what you want to do is you want to take your original prompt and then uh you want to take your next id prediction and concatenate it with this and then finally return the index. So if I come back to uh train.py a lot happened between uh between uh setting the prompt and uh putting it for generation. Right. So I've got the vocap size back. Now all I need to do is uh decode and print them out. So without further ado, uh we'll run and see how this looks like. So I'm already in my part two folder and all I have to do is python orchestrator.py. So it first takes uh the train.py and runs it against this data set. So for every 50 steps, we can see that the loss is going down. So our training is working well and then at every 100 steps um a sample gets generated of how the text looks like and you'll see that uh after every 50 steps there's a slight qualitative improvement in the generation uh because again the the loss seems to be uh going down steadily right so we went from 1.2 2 to 1.1 and so on. Uh and we are saving the checkpoints along the way. Uh the checkpoint saving strategy is something that we will go through in part four as well. Um so once we have the training complete uh we're going through sample.py Pi and for that uh we've provided an English prompt which doesn't make sense but anyway all we wanted to see was the generation happening and uh at um at no scale can we expect a two-layer uh transformer architecture to work uh right so we're going through the evaluation loss and for this I think uh we've skipped the estimation of the evaluation loss so here you can see that I've got an estimation loss uh after uh every uh evaluation step that we've configured over here. So you'll see that uh at the evaluation interval we are trying to estimate the losses. So the way to implement that is uh put the model in eval mode first and then with no gradients we just want to forward prop uh for however many iterations that we've configured the evaluation for we'll get a batch we'll forward propagate it uh and since we know our wise we will get the loss back we don't care about the predictions here we're just interested in the loss and then we uh append the loss to the list that we've configured here and then for every valuation iter uh iteration we're just going to average it out and then put the model back on training so that our model can continue training and then uh return. So that's our estimation of the loss uh that we had sort of skipped through. So this is what we have. Um this brings us towards the end of uh part two. So hopefully we've covered all the topics in great details and I hope you had a lot of fun doing it. Thank you. Hello and welcome to part three of the LLM from scratch series. And in this part uh we'll look at what we have implemented and try and modernize the existing transformer architecture. So step by step uh we'll introduce all of these concepts and also look at how we can put these together in code to achieve um one of the more modern forms of the LLM architecture that we have. So first things first we'll start concept by concept. What I want to do is spend some time doing some theory u picking up each of these concepts apart and explaining them separately and then we'll figure out a way how to put them together. So first we'll look at RMSOM or root mean square normalization. Now this is going to be an alternate to the layer normalization module that we are doing. And what this does is normalize the vector the incoming vector by the root mean square of it. So it's literally divided or multiplied by the inverse of the sum of squares uh the square root of the sum of squares. Right? So this is the formula. So the RMS of X is going to be the square root of the mean squared plus an epsilon. And this we add just a very tiny number to avoid zero division error. So uh x is divided by the RMS. Now the g term here is a weightage term which in this case uh we'll just use uh torch do once. So all of them will have the same weightage. So here we can see uh the incoming x is first squared and then the mean and then the epsilon is added and then the square root is taken. It's just one chain of pyarch and then from there it is divided by x and multiplied by the weights which again has no effect in this scenario. So that's rms. Now why are we using this? It's slightly more efficient because these operations eventually in the GPU can be fused together and we don't have to keep track of mean and variance as in uh the layer norm. Uh we'll just have to do this in one shot. Now the second thing I want to discuss here uh is swiglu activation. So um in modern LLMs uh the relu or the gelu activation that we have seen before is not used anymore uh especially in the feed forward network. What we see is the swish activation or the swigloo uh is what is called. Um so you have an incoming uh x and it goes through a linear projection to get a. So W1 is a linear layer and then the X also goes through a separate W2 which is another linear layer and then the activation is applied upon the output of propagating X through W2. So that's uh in this case the silo activation function or the sigmoid linear unit which is literally the number multiplied by it sigmoid. So as you can see here the silo function is also known as the swish activation function which is the number multiplied by its own sigmoid. This is what the curve looks like and then optionally uh we can go for a dropout. So this particular uh this particular module is what we're going to use going forward for our feed forward network in our transformer architecture. So we've covered RMS now and swigloop. Now the other thing that I wanted to explain before we could dive into implementation and there are a couple of more of these. Uh one of them is KV cache. So the idea is pretty simple. In the core transformer module, we have a Q, we have a K transpose which results in a Q transpose which is then multiplied by a V to get our final attention weights. So let's say token one comes in and what happens is it interacts with the transpose of token one pushed through a different set of linear weights and then what we have is the token T1 interacting with the token T1 that's the outcome and then in the V we also have T1 which then results in the attention one. Now the A1 is then further propagated through uh the transformer block to finally get a token which then feeds into uh back into Q. Now the whole idea is uh before KVach the token 2 comes in and the entire processing from token one to token 2 and all its interactions they were happening in one shot redundantly. So the idea is if I've already processed token one can I cache it and save a bunch of computation. So what we do for that is we'll cache in the K component and the V component. Hence literally the name KV cache. And then all we need to do here is build a new row. And then from there we do not need to store the attention output here and the and we do not need to store the Q here. So here when the when the token 2 comes along I can literally replace it here and then all I need to do is build my KV cache with one more element and then here the T2's interaction with T1 and the T2's interaction with T2 is what I need to store. So I'm literally adding a row here and then from there T2 goes here and this literally can be replaced by A2 which then eventually gives me token 3 which again I'm going to use to build my Kcash and my Vcash and then I'm going to add a row with T3's interactivity with each tokens. So this literally is what is called a KV cache and it saves a bunch of computation. Now we've built a small module here to uh to kind of compare the difference between what happens when you use cache versus when you don't. So stay tuned for how this is implemented in code and what are the actual performance gains and this especially applies obviously in inference mode when we're trying to generate token by token. So it's an efficiency in the inference step. So we're going to look at uh KV cache. Now the next topic I wanted to cover uh was group query attention. Now before we could do that we actually have to understand uh KBC cache uh the the non-functional aspects of it and and what is the impact of using this. So let's say uh I'm going to cache the K and the V. So that's two of them I need to cache. And then let's say inference happens at FP16. So I've got two bytes for each floating point number that I want to uh cache in. Now from there uh let's say uh a typical 30 billion parameter model will have typically 48 layers. So uh I literally need to do this for 48 layers and then from there uh inside we're talking about uh manipulating embedding lengths as part of the KV cache. So let's say this will be 71 68 embeddings. And then let's say we are trying to do a very modest sequence length of 1,000 tokens. And then finally, let's say we're trying to do a batch size of 128. Now, with this setting, if you just plug in the numbers and multiply them uh and I think uh the bat sequence, the sequence length, the embedding length, the number of layers, the precision as well as uh two for the one for the K and one for the V is all that we need. Uh I'll fix it in retrospect um if I'm missing anything here. So this will about you do the math it will come down to uh about uh 170 uh GB. So what what you're uh doing as a cost of efficiency is actually uh sacrificing on a lot of memory and eventually you will have to take these kind of decisions uh over and over again because uh memory and compute are always a trade-off. So this presents a problem. So KV cache wanted to solve the computation problem but introduced a new problem which is the memory overhead that is required to build it. So in this case what we are next trying to introduce is how do we reduce our KV cache and how do we make the attention module more efficient. So in this case uh we are using multi-headed attention where every query head maps to each key head which then is interacted with the values head. Now instead of that one extreme proposal can be can we do multi-query attention where all the query heads map to one single key head which then goes to one single value head. Now this is where we see the most gains. So it is most efficient but also there's a significant accuracy loss and this sort of serves as a nice middle ground where I can take uh one key head to tend to one group of queries and hence the name grouped query attention. And this can be two this can be four. In this diagram, the mapping is for two queries going to one key and one value. So if you do this smartly, you will notice that picking the right group size can literally take that 180 170 GB and has the potential to almost reduce it by an order of magnitude. So this is an important implementation that we're going to go through as well. So that's grouped query attention. Now from there uh we introduce um another uh optimization to the attention mechanism and that's a sliding window attention. So we're only going to focus on this and this uh we're going to skip the history lesson of how this came to be. Uh but I'll drop in a word or two about attention sync. So what we are using is tense attention where the current token uh in order to be tended to can go back and look at the entire context window that is available to me. Right? So this is the dense attention mechanism that we are already aware of and this has a quadratic complexity. That's the problem. So if I go over here, uh the streaming LLM as they call it is literally sliding window attention with an attention sync. So what we do is uh we always use the first few tokens. So the first few tokens are always part of the context window and then we use a sliding window attention for uh the rest of this longtail. So we have first few tokens always getting used here and then there's a sliding window over here. Now in this diagram it's depicted that the attention sync is literally one token but in practice we tend to use the first few tokens as the attention sync. Now uh intuitively why is this called the attention sync? Now it turns out that if you take the entire context window and then just shrink it down to this sliding window attention context window over here then what happens is let's say I do not want to pay attention here and I want to pay more attention here. What happens is uh it's bound to disperse its probability/attention across these three tokens only. Now that leads to a lot of errors and a lot of catastrophic forgetting. Uh so what the authors have proposed here is they'll uh use the first few tokens and they'll anchor it always in the beginning of the context window. So what happens is as you're sliding through the context window, uh if you do not want to pay unnecessary attention to a particular token, you can quote unquote dump all of the probabilities out to the BOS token or the beginning of sequence token or the initial token and that way you can pay the right amount of attention to the right token as you're sliding across. So that's one part of the intuition. The other part of the intuition is I want to compare the sliding window attention mechanism to the receptive field of CNN's. So let's say you're trying to process an image through a CNN and layer 1 it looks at a particular receptive field. Now layer 2 will actually look at a receptive field of layer 1 and layer 1 has looked at the receptive field of the source image itself. So what layer 2 is looking at is actually a summarized and a condensed version of a bigger receptive field that L1 has gone through. Let me try and explain this with an example. So let's say you have uh the incoming sentence. The cat sat on a mat. Now what happens if we slide this through the layer one of transformers. So obviously uh the first element of the position will just pay attention to itself. The second part will look at both these interactions. So it will have information or interactivity stored about the tokens the and cat. And then third it will have the cat sat. And now if I'm not using the entire context window or clipping my context window to only let's say three, the sliding window attention is three. Now it will look like we're starting to lose context. So, the cat sat on sat on a then on a mat. So, compared to using the dense attention mechanism, it looks like we've lost a lot of context. But this is only in layer 1. So, what happens in layer two? we have the so just the first token interacting with itself. Now in the second part we have the cat interacting with the so essentially we have some information about the cat. Now in the third we have the interacting with the cat interacting with the cats. So again essentially we only have these two or three tokens to deal with. But look at what happens here. So at this stage if I'm going to tend to only three windows so my window size is three I've already covered the cat sat on now what happens in the next step I go from these three tokens so I'm essentially processing the cat sat on uh so by By the time I get to the next layer itself, even though my sliding window attention is of size three, I'm capturing the interactivity of five tokens here. Right? And again, for simplicity sake, uh each word is a token here. We all know that's not true. Uh so when I get to the last stage, then all I'm using is just losing. It's just one token. So you see by decreasing the sliding window or uh decreasing the context window and having a sliding mechanism in place as I go through subsequent layers and I'm sure by the time we get to L3 um I would have captured a lot more context probably the entire context right so the core idea here is since we have a lot of layers in our transformer through the receptive field mechanism of propagating layer after layer sequentially I'm still capturing a lot of information and I can live with a sliding window attention mechanism. So now if I come back to the whole KV cache situation what we are going to implement eventually is called rolling buffer KV cache. So we are not interested in implementing a full-blown KV cache. What will happen here is I would have cached tokens T1, T2 and T3. Let's say there are three attention to uh sync tokens, right? And then um in the next step I will do T4, T5 and T6. And then when it goes to the next step, my T1, T2 and T3 will remain constant. But through a sliding window mechanism, I will actually evict token 4. And then what will be left in my KV cache is T5, T6, and T7. Right? So this is going to be my KV cache. And you see where this is going. This will always be constant while uh this has a sliding window operation where tokens in the middle get evicted. So this is going to be a rolling buffer KV cache. And this again if implemented correctly with the right hyperparameters can further reduce your KV cache by an order of magnitude without compromising a lot in the accuracy. So that's the core idea of sliding window attention mechanism. Now the next thing I want to discuss is how we're not going to use uh absolute uh positional embeddings anymore in our transformer architecture. So first of all the transformer by itself is position invariant. So that's a problem especially in language because uh the cat sat on a mat and the mat sat on the cat are two different uh implications right so um we had to introduce an absolute positional embedding for that particular reason to preserve or to embed positional information in some meaningful way. Now from there what I can say is the absolute positional embedding has a problem which is that each positional embedding that I come up with is independent of each other. So if token one has a positional embedding it is different than token 500 it is different than token two. Right? So this presents a problem where the relative position matters a lot. The distance between two tokens should be pivotal in deciding the amount of attention I want to pay across those tokens. Right? So, uh, enter relative positional embeddings and what this is trying to capture is a bias term that literally represents distance. So, let's say if you have uh I'm representing each uh row as a token. So I'm in going to introduce these uh biases. So I have B 0, B1, B2, and then B3. And then T2 goes from B minus1 to B 0 to B1 to B2. And then this goes from B minus 2 to B -1 to B 0 to B1. Now what is happening here is B1 for example is trying to represent the bias associated with distance one. Let me explain that with an example. So if I have four tokens here. Now the distance between A and B is going to be represented with B1. B and C is going to be represented with B1 and C and D is going to be represented with B1. So any tokens that are one distance apart is going to be uh represented by this bias term. Now the distance between A and C B and D is going to be represented with B2 and the distance between A and D is going to be represented with B3. Now the problem is when you every time a new token comes in you'd actually have to add stuff to T1 T2 and T3 as well while you're building this bias matrix right so um in KV cache the desired behavior is that if I have process token one I'm going to cach it and I'm never going to think about it forever right so what relative positional embeddings is forcing us to do is actually go and modify each and every uh caching mechanism that we want to have for so if a fourth token comes in I'll have to uh add a B4 here and a B3 here and a B2 here and then add my row for uh token token 4 here. So we've got -3 - 2 -1 B 0 and then like so right. So uh what we want to do here is essentially is that if I've processed uh the positional embeddings of token one I just want to be done with it. I never want to revisit it. And that's why uh this is more of a transitional term to understand rotary positional embeddings. Uh and you can see that um the T5 bias which uses relative positional embeddings is quite slow although slightly more effective than absolute positional embeddings because it captures the relative positional embeddings. It's slightly more effective but again as the input length increases it becomes slower and slower. Now rope is the mechanism which captures the best of both worlds. It captures the stability of the uh of the absolute positional embeddings but it also captures the relative distance in a meaningful way that relative positional embeddings was offering. So combining these two together rope will solve both of our problems of uh stabilizing KV cache as well as capturing relative distance. So quick intuition of how that works. So let's say you have a two-dimensional word vector here and this is called this let's say the vector for dog. Now depending on where uh this token appears in the sentence. So let's say it appears here. Now what rope is trying to say is that uh there's an angle theta with which I'm going to rotate the vector dog and this is going to be the m is going to be literally uh depending on the position of the token. So very intuitively uh if dog is the second token in the sequence then I'm going to rotate it by 2 theta and this becomes the vector with the positional information embedded. So if a new token comes along here let's say then its rotation will not affect dog's rotation. So I don't have to go back and fix anything over here. And so let's say chill comes out to be here. And I can even capture the relative positioning information through the angle through which a particular new token is rotated. Right? So every token that comes along gets positioned in this n-dimensional vector space and is then manipulated or rotated through an angle of theta weighted by the positional information depicted here by M. And then that will end up representing the positional information of that token in absolute terms but also relative to other tokens in this n dimensional space. So that's the intuition behind rope and this is how it's implemented. So what we have here is a notional representation. So this is what it should look like and the way it's done in practice is that the incoming vector. Let's say you have a vector of numbers here. They're taken two at a time. So this particular operation is applied to these two. And then this particular operation is applied to these two. And this way it happens in pairs. Right? So uh each pair here is represented by this. But we never want to build this matrix in practice because it's just too inefficient. That will blow up memory. uh a simpler cleaner way to represent it uh is down below where I have uh x1 through xt and then uh I have this inverted sign and then I've got the cost here and the sign here. Now what we can do here is we can cache these up front the cost and the sign. So this starts getting called the rope cache. So we can build a separate cache and this is not to be confused with KV cache. I can premputee because this is not going to be something that will be learned over time but I can premp compute it for my entire context window for all positions that I'm going to see the signs and the costs. So that every time an X comes in, a vector comes in, I'm just going to get instead of computing the cost of each angle, every time I'm just going to get the corresponding values from my rope cache and then multiply it uh with the corresponding numbers to get my output. Now another aspect of this is uh while in the previous transformer the positional embeddings was incorporated before it went to the block in this case uh we are going to apply rope after it enters the block uh and on Q and K separately right it is not applied on V it is applied on Q and K. So with that I think uh we've gone through a bunch of these concepts and we are ready to implement and see the implementation of a more modern LLM architecture. So the aspects that we saw uh is RMSOM swigloo KV cache group query attention sliding window attention with attention sync and finally rope. So I hope kind of these five or six new concepts uh we've picked apart and try and u understand them separately and now when we finally look at code uh everything should start making sense. All right so let's go through the code. These are the stuff that we're interested in implementing. As always uh the entry point to our part three will be orchestrator.py. We've got a ton of these implementations here for the different modules that we've discussed. So, first things first, I'm going to run a bunch of tests just to see them going well. And then I have this generation mechanism where uh so since we've already spent a lot of time uh discussing the changes in the architecture in this part, we won't be training a model. uh we're literally going to initialize it with random weights and see whether it runs or not and how the forward propagation happens and how the generation happens compare it with or without using KV cache what's the impact uh so that we can just hyperfocus on these aspects and not convolute it further with some things that we already know like the training module part right so uh we're going to do demo generate.py Pi we're going to use RMS norm we're going to use rope we're going to use swigloop going to use sliding window attention of 64 with a sync token size of uh four so essentially our context window which was 128 before just became 68 and I'm trying to generate 200 new tokens right so with that we can get to demo generate.py Pi this is what it looks like. So some of the arguments we've passed already. Um the group size is for the group query attention. So I've taken a default of two here. So for every two query heads there will be one KV group. That's that's what it means. So we've got RMSOM rope swigloo sliding window sync group size tokens and uh CPU. And then from there we choose the device. uh we're still using the bite tokenizer and then this is where the model magic happens. We'll get to that in a minute. And then from there uh we're making a dummy prompt um just hard coding a dummy prompt and then we'll generate this with and without KV cache and then um compare the outputs in terms of uh quality and time. So uh let's get to the GPT model modern implementation first and then we'll come back to the inferencing part. So here I've got the GPT modern implementation. So if we go through the hyperparameters then we have uh vocab size the block size uh the number of layers number of heads number of embeddings whether to use dropout whether to use RMSOM swigloo rope and then max pause uh again this is something that we'll use to uh build the position of the rope cache sliding window attention sync NV head so I've renamed the group size to NKV head here. Uh this is literally going to be the group size. So uh this is going to be interpreted as the group size. So uh I'm passing the args.roup size in demo generate.py to the NKV header. Right. So what are the different components in the init phase? So we've got the block size, we've got the token embedding, we've got the position embedding, but we're not going to use it. So it's just there for um display purposes, right? So we are not going to use the positional embeddings in this space. Uh we've got dropout and then blocks is again an all too familiar module list, but this time we're going to use the modern transformer block. Uh and then if RMS norm is turned off, we're going to continue using layer nom. But if it is turned on, we're not going to take any action. That's uh why it's n. And then finally our head will be the linear layer as always. So uh we can take the forward propagation here and then go back to how uh the transformer block model is implemented. So the indices come in in terms of batches and each batch has a sequence of tokens. As always I'm checking if u the sequence of tokens is less than the block size. You can again uh choose to clip it if you don't like uh this particular implementation. So uh I'm building my positions here. So the positions will be nothing but an a range of 0 to t. So for each sequence that comes along uh let's say sequence one has uh 30 tokens. So it'll go from 0 to 29. Uh the idea was to use it with positional embeddings but that's turned off now. You can turn it back on. and then uh we get to the the KV cache part. So um what I'm saying is we're iterating through the blocks. So this is a forward propagation and each block we're going to iterate through it in this for loop. So I is the index of that particular block and the BLK contains the block itself which is the transformer block model. So with the cache we're saying none if KV cache list is none else we pluck out the particular index from the KV cache list. So the idea is KV cache list is a list of KV caches for each layer in our transformer. So if I have six layers this will be of length six. So depending on which block we are accessing we want to pluck out the right KV cache and use that as input to our block. So to our block we are sending x the thing that we want to compute. We are sending the cache that we've previously plucked out. And then uh we sending the start position. So again I've kept this because uh I wanted to understand the interactivity between the sliding window and clipping of the block size etc. Most of the times for all intents and purposes this will be zero. We'll see if uh if this has to be different. I think in some cases the start position will be different but we'll we'll get back to that in a minute. So in our new caches we uh append. So while processing a block I get back another set of cache. So I kind of append that uh in a new cache. Right? So once I've processed all of the blocks, I'm going to go through the layer normalization in my head, the logits, the target, and then I'm going to return the new cache that I've got. So for every time I forward propagate through my model, I'm going to return the new set of caches that I've computed back to the user so that they can build the KB cache from there. Right? So I think it's time that we go deep inside the transformer block model. And this is what we've got here. So this is a block where the different components are. First of all, we've got the norm. So if RMS norm is turned out uh turned on, we're using RMS normalization. Otherwise, we'll just use layer normalization. We have in fact seen the implementation of RMS normalization already. Uh there's no point in repeating it. So from there we've got ln1 which is the normalized layer of the embeddings or the normalized output of the embeddings and again if RMS norm is turned down uh this will result in RMS normalization otherwise we'll go back to layer norm and then the causal self attention model and then from there one more round of normalization and here I'm using the swigloo so again if use swigloo is turned on we will use the swigloo uh mechanism and you can see that swigloo is nothing but again we've seen this before uh x pass through a linear layer which goes to a x pass through another linear layer and then the swish activation applied on it which goes to b and then the interaction between a and b is further passed through a third linear layer and then an optional dropout step. So that's the swiggloo activation if use swigloo is true otherwise we fall back to the previous implementation of uh the gossian error linear unit. All right. So uh here in the forward propagation what happens is and uh this will be invoked every time we go through one for loop over here. Right? So you've got to correlate these things where each block is getting processed and this call will invoke a call to the forward propagation here. So what is happening here is I get an X, I get the incoming KV cache and I get the starting position. So from there what happens is um the X is normalized and then sent as an input to the attention mechanism. uh I pass in the same KV cache. I relay basically the KV cache that I'm getting from above and then the starting position and then what I get back is a the attention output and the KV cache. So I'm going to do the skip connection thing where I'm going to do X= to X + A and then I'm going to push X through another normalization the feed forward and then do the skip connection thing again. F finally return the X and the KV cache. So here now we need to go deeper inside causal self attention. So this is going to be the meatiest uh implementation of the lot because uh this is where a lot of things have uh changed. So let's go through it step by step slowly. So the causal self attention is getting invoked through the block over here the attention mechanism and the input that we are pushing through is the X the KV cache and the starting position. So if you see here the embedding the number of heads the dropouts whether to use rope which is by default true the max position which is again in this case I've taken uh 4096 the sliding window attention sync NV head which is nothing but the group size and these are the inputs right so in the initialization step in this block many things will look familiar. So I'm doing asserting the embedding uh the global dimension of the D model should be divisible by the number of heads that you choose and then from there I'm assigning N head to N head uh N KV head to uh so again uh this is a protective mechanism where uh if you have passed uh the group size then I'm going to incorporate it or I'm just going to use the N head which essentially means uh this will be an implementation of multi-query attention right so now what I'm trying to assert is if you have passed a group size then that in turn has to be divisible by the n heads so if you go back to the implementation of group query attention multiple queries will be mapped to one key and value right so now for that if you pass the number of groups then that has to be divisible by the number of heads you've got otherwise the mapping won't happen. So the group size I'm deriving from here will be the n head uh divided by the n kv head. So that will be each uh the the size of each group or the number of query heads each group will each k group will tend to. Right? So uh that's the number of kv heads we've got. And then uh d head as always will be the quotient of the number of embeddings or the embedding dimension now with the number of heads we've got. So that will be the dimensionality of a particular attention head. Right? So uh we just have to put the GQA part in. Apart from this some of the um some of the checks or validations that we have is pretty much the same. Now here if you see things have changed slightly. So in the QKV mechanism uh what we have been implementing is we used to take uh thrice the size of a linear layer of uh an N head and a D head and then split them post facto after processing it as a single linear layer. Uh but we can't do that anymore because multiple query heads are mapped to one KV group. So what I can't have is Q, K and V having the same length. So I'll have to process them separately. So what we have here Q is just like the implementation before because we're not doing anything to Q here. So it is going to be the number of heads times the dimensionality of each head. And so Q has its own linear layer unchanged from before. Now the k and v however have slightly changed. So instead of uh the linear layer outputting n head * t head I'm going to have it output n kv head * t head. So it's going to be only the number of groups that I have configured for. And these are exactly the same. So we we could probably have done some optimization here where I do twice this and then split them back. But I've chosen to kind of keep this a little clean since uh all of what we're going through is a little more involved already. Then we have the projection layer that goes to n embedding to n embedding and the dropout. And then we set some properties. So use rope true or false rope cache. This is where we are going to uh premp compute and store the sign and the cost positions and then use them to calculate uh rope. Then the max position. So this again will be used for precomputing the rope cache uh sliding window and the attention sync something that we've seen before. All right. So uh let us discuss the forward propagation of the causal self attention model. So the incoming X will be split into BTC and then the first thing that happens is uh I've literally called this function maybe in it rope because it will depend on whether we are using rope or not as that's a configuration parameter. uh we are using rope so this function will get invoked and what we're trying to say is uh if use rope is true and rope cache is none then I build uh need to build my rope cache where I'm saying uh build the rope cache according to the dimensionality of each head so uh remember uh we'll we'll apply rope on the Q and the K not the V so I need the D head and then the max position uh which I've initialized as 4096 and then uh pass the device then build the rope cache from here. Uh we'll get into how this is implemented in a minute. I just want to uh go through the forward propagation. So after the rope is initialized we've got the QV the computation and this will sort of look familiar. So the view and the transpose is something that we've done before. So we've got self.W of X and again if you remember WQ is the linear layer of the Q and then we make it go through the K. We make it go through the V. So these are the linear layers where now they'll start interacting with each other. Now if I have use rope turned on which I have then I'm going to calculate the positions through a range. This is something similar to the positional embeddings we've already done right. So if the starting point is zero and I'm looking at t particular tokens then I need to arrange from zero to the number of tokens. So if I have 100 tokens, this will go from 0 to 100, which will uh result in position 0 to 99. And then from the rope cache, I'm going to get the the costs and the sign premputed for all the angles and for all these positions from my rope cache. And then I'm going to apply the rope on the Q and on the K. We'll we'll get to implementations in a minute. uh I just want you to understand the whole flow along with the KV cache and sliding window first and then we can double click on the rope part. So just imagine this is a black box where we are going to open this up but uh right now a single instance of rope is getting applied to the Q and the K the incoming Q and the K with the cost and sign that we have premputed. All right. So, first things first, if the KV cache is not none, then we are going to concatenate the existing KV cache with the current K that I have computed over here. So, we've computed the K and the V. And if the KV cache is not none, we're going to further append to our KV cache through the torch. Torch.cat cat method here. Now, if the KV cache is none, then I'm going to start and build it. I'll assume this is the starting point. Now, this is where we take it one step further and we go from making it a KV cache to a rolling buffer KV cache. So, if the sliding window is not none and K all size 2 is greater than the sliding window plus the attention sync. So I'm trying to gauge whether I need to evict tokens here or not. So the whole idea of the KV cache is that the KV cache is the dense attention mechanism but the rolling buffer KV cache is going to be a subset of that which is the attention sync and the sliding window. So I'm just trying to ensure that the sum of sliding window and the attention sync if greater than the current uh length of the KB cache then I'm going to go in and then I'm going to start evicting the tokens. So the attention sync is a configuration parameter. In this case, I think we have set it to four. So from the entire KV cache, which I've already built over here, from the entire K cache in this instance, I'm going to only pluck out the first four tokens. And again, four is the attention sync. And then I'm going to concatenate that with the sliding window that I've configured over here. So going to take the entire batch and then pluck out and go back just as far as my sliding window. In this case, this is going to be 64, right? So I'm going to take the first four tokens and the last 64 tokens and then concatenate them together to form my rolling buffer KV cache. So this is the idea of the rolling buffer KB cache. So we went from KV cache over here to rolling buffer KV cache which is a subset of it. And then from there uh I'm now trying to implement the GQA part. So if the NKV head is not equal to N head then I need to do some things to it. So u if they are the same I'm going to assume I'm doing a multiquery attention right. So uh I literally do not need to do anything. Literally the K of attention and the V of attention becomes the K all and V all. Right? But if not I actually need to repeat interle. Now what is repeat interle. So repeat interle is an operation that when you specify how many times you want to repeat a particular uh input it gets repeated that number of times. So in this case again you have to understand that multiple queries are getting tended through by one set of K and B. So for that for that multiplication to be effective I need to unpack or broadcast my KV group and replicate them across the number of queries that I have so that I can do the dot product. So I'm going to simply broadcast all of the KV groups that I have to unpack and replicate themselves to the same amount of dimensions as the number of queries. Right? So once that is done finally I can go through the dot product the scaled dotproduct attention and then from there uh take the output transpose it continuous view uh we've seen this before get it through the projection layer and again if the KV cache is not none I'm going to now uh construct my new KV cache otherwise the new KV cache becomes the old KV cache. or rather the new KV cache becomes the new K and V that I've computed in this step. So I'm starting to build my KV cache and then I take the K new and the V new and then build a data class out of it. So if we look at the KV cache over here then you'll see that it's a data class of size um or again it has a K and a V of size BHT right so this is going to be my implementation of KV cache the only thing that we have left is the implement mentation of the rope. So let's go through that. We'll go through two things here. Um the applying one single instance of rope and then the entire rope cache implementation. So let's get down to it. So first things first, I'm going to assert that the head dim coming in must be divisible by two and then uh set my base my head dim and my device. And then I'm going to build as per my position. So the position here I think I've set it as 4,96. Uh now what will happen here is in the build phase. I'm actually building my uh rope cache. So I've set the max position. Now the rope is actually the inverse or here literally I'm trying to calculate the thetas right and the thetas will be the 10,000 to the power of the position and this is what it looks like. So torch dot a range we start from zero we head till the head dimension and then we take the floats we convert that to a float divided by the head of dimension. So this calculation is literally going to be as mentioned in the paper for calculating the angles and then from there uh I'm going to take the positions. So toss a arrange until the max position. So place where I want to build the cash on uh do an outer. So this literally gives me uh if I do this between t and the inverse frequency for each position it will give me uh head dim by 2. So this becomes the matrix and for each of that matrix I'm going to take the cos and sign. So this becomes my rope cache. Now what I'm interested in doing is that if I want to apply rope to a particular tensor what I'm going to do is again uh my validations aside I'm going to build my cosen sign so that they match the dimensionality. So I'm going to add two more dimensions. X1 becomes the even positions. So this piece literally takes the last dimension and goes from 0 2 4 6 8 and then this piece does the e uh does the odd positions. So it's 1 3 5 7 9 and so on. And then from there I'm going to take the x1 part and multiply it with the cosine x2 part and multiply with the sign and then there's a negative sign here. uh same calculation goes here and then I'm going to assign my output matrix back to these values where now the positional information has been embedded. So if I were to correlate this with what we have seen as the implementation of the uh rope you will see that it is consistent with the implementation that we are trying to go through. So the m thetas that we have calculated uh literally are the implementations from the paper itself. Uh we'll go through the why the m theta1 is uh here positioned as 1x 10,000 in a minute. I think that's what the paper says. Yeah, there we go. So uh this is the general form section of the row former paper. And as you can see the uh the theta calculation over here has been given by this particular equation that we have calculated. Hope that makes sense. So once you go through this uh a couple of times yourself hopefully things will make sense. So we we've also gone through the application of how to apply rope once. So which brings back to our causal self attention model. I think we have gone through all the aspects of it. So we covered RMS, Swigloo, KV cache, sliding window attention with attention sync and rope. Uh so all the aspects are covered. So we can come back to the implementation and then from there uh one last trick that I wanted to show you was I have a generate function that uses the KB cache uh KV cache with all the bells and whistles included and I have an implementation that does not. So we're going to compare the output uh once you run it. So in the generate phase again my top kit top p filtering stays as is. So uh what will happen here is I'll take the indices and then for the first time I'm going to seed my entire prompt. So if my KV cache is uh so I'm checking whether it's the first token or not or it's the first time that I'm processing or not. If it is so then I'm going to take the entire context window that was provided to me from the prompt and then I'm going to process it. But if not that means I'm generating a new token one by one. So I've already prefilled my cache uh with all of the input prompt. All I'm interested in doing is looking at the last token so that I can um build my KV cache row by row and save a bunch of computation. So this is where the starting position changes as well. So uh if this is going to be uh the first time that we are processing the prompt then the starting position is going to be zero otherwise the last position of the KV cache will become my starting point. So this is where the start position is not always zero. This is an important part to keep in mind. So from there uh we forward propagate it through our model. we get our loits and our new KV cache and the next logets is going to be the last token and this behavior we've seen this already and then um the next logets is going to be further filtered by the top key implementation and then uh I'm going to get the probabilities and this time I'm trying to keep things more deterministic so instead of relying on u a tiny any number uh manipulating my uh my deterministic outputs. I'm going to directly hardcode a greedy output here. So I'm saying if the temperature is zero then just give me the argmax else we continue to sample. So from there we go uh the concatenation of idx and the next id and then we further process. So this is the generate with KV cache. Now without KV cache the top part remains the same. Uh this also holds true for uh the first part but it's also made to hold true for every part. So every time we're going to generate token we're going to take um go back to the block size and then process the entire thing. Right? So um this again we are manipulating so that it matches the cache path. Uh we're going to get the loits we're going to get the next set of uh new logets from there. Do the topk filtering. Do the probabilities and softmax argax for temperature zero blah blah blah. So everything else remains the same except for the fact that uh from the second token onwards we here are going to be leveraging KV cache while uh we're not going to here. I've written a bunch of tests as well. Uh I leave that as an exercise where I'm just uh validating the different shapes of the different mechanisms of where I thought I struggled with. So we're going to run our script now and then we're going to pass the demo. So it goes through a bunch of tests and you can see that um we've both generated again this is an untrained model. So the outputs won't make sense. We've generated 200 tokens and the first time through we were using the KV cache. So that happens in 07 seconds. The second time through we were not using the KV cache so that happens in uh.13 seconds which is almost twice the time right? So it matters a lot uh in terms of the speed and it's a remarkable achievement. So that brings us towards the end of part three. Now what we have gone through is a bunch of meaty stuff. We started with RMS normalization. We then uh checked out swigloo. Then we understood KV cache. Then we understood uh how it blows up memory and the need to get into grouped query attention. Uh then from there the sliding window attention with attention sync. So all of them put together put less pressure on the KV cache thereby blowing up the memory lesser and lesser uh which led to rolling buffer KV cache and then finally uh we went from transformers being positionally invariant to introducing absolute positional embeddings to them not having the relative embeddings. Uh so we introduced relative positional embeddings and then from there finally rotary positional embeddings which combines the best of both worlds concerning absolute positional embedding their freedom to cache and operate independently and the relative part which also is embedded in rope. So all of that put together uh we constructed an elaborate attention mechanism along with KV cache sliding window everything. Uh so hopefully uh things are starting to make sense now. All of the code is already available in the GitHub. Um I strongly encourage you to follow along or maybe try and build your own based on the explanation and your understanding. Hope you had a lot of fun learning this part. Thank you very much. Hi. Hello and welcome to part four uh of the LLM from scratch series. And in this part uh we're going to cover various aspects of training the modern architecture that we've just gone through. So we'll start from uh switching to bite level to bite pair encoding tokenization. We'll go uh learn gradient accumulation and mix precision. Uh AM is something that we've covered already. We'll go through gradient accumulation and the implementation of it. We'll look at learning date schedules and warm-up. We'll look at checkpointing and resuming. And we'll also look at in details tensorboard logging with an option to switch to weights and biases as well. But weights and biases will be out of scope for this. So, as always, we start with our orchestrator.py and the layout in front of you. And all you need to run this demo is run python orchestrator and pass a demo argument. Then from there what happens is the tests run and the train and sample.py run. We probably won't be covering sample.py because it will be self-explanatory especially after covering the previous parts. We'll hyperfocus on train.py because I want to just unpack some of the aspects of how a production rate training actually happens. So we'll jump to train.py and here we've got our arguments. So we've got the data argument, the out directory argument. Uh we'll be using bite pair encoding with the vocap size, block size, the number of layers, the number of heads, the number of embeddings, dropouts, everything all too familiar. This time there will be some training parameters in addition to what we have seen before. So bat size epoch steps the learning rate the warm-up steps whether to use mix precision and how many steps to accumulate the gradient accumulation part then there'll be a logging step whether to use tensorboard which will be by default and then how many uh every how many steps do you want to save your checkpoints and how many checkpoints do you want to keep. We have the device and then we have from the out directory we're grabbing and checking if a checkpoint exists. So this script has the ability to resume training if you choose to do so. So if I have a checkpoint present the checkpoints will get loaded otherwise training will start from scratch. So we start from the checkpoint and then we load the particular checkpoint if it exists and if it does not have a config we will error it out. So in order to not guess some of the parameters adjoining parameters in the model apart from the weights like the vocap size the end layer the n head the n embedding etc. It's mandatory that we save our model not only with the weights but also with the state dictionary of all of the configurations. So we're looking for our tokenizer in this tokenizer_dir.txt and then once we find our tokenizer we are simply going to load it and that will take us to our save tokenizer directory. So we come to the part of the tokenizer. Now we initialize our tokenizers and our tokenizer directory. And if we have the checkpoint which simply says if it exists or not, if the checkpoint path exists or not, then we'll load the BP tokenizer and then we'll load the saved tokenizer directory from this. From there we assign a couple of variables from there and if it does not have a checkpoint it loads a BPE tokenizer and then trains it based on the data path that you've uh passed in the argument and then it creates a tokenizer directory. It creates the directories if they don't exist already and then it saves our train tokenizer. So here what is going on is if you have a bite pair encoding tokenizer it works on the data parameter that you pass. Please note that this is the same data that we're going to use to train our LLM as well. Usually these are different. So you want to use a different data set for a tokenizer because you want to include more aspects of language, more aspects of nuances like programming languages trying to capture indentations and u different nuances in syntax. So your data typically for the tokenizer for training the tokenizer will be different. Now while we do call this uh training the tokenizer there is no training in the machine learning conventional aspect. So all we are doing is collecting statistics and merging them together. So Wikipedia has an excellent example on the bite pair encoding and how it works. So we start with the stream of strings here or stream of characters here as an example. Now since the most frequently occurring pair is AA, we replace AA by the term zed. So right now if you look at this stream of characters, our vocabulary size will be the letter A, B, C and D. But now if we are deciding to merge zed to represent or merge double A to represent to be represented as zed our vocab size increases from four to five but we need lesser number of tokens to represent our stream of characters and then we continue on this journey to u uh take pairs and we can set cutoffs based on what should be the minimum frequency I should be able to see a pair. before I decide to merge them. And all of that put together iteratively, we can have a stopping point where uh I can hit my frequency threshold where I should be able to see a particular pair of uh characters at least a certain number of times or I've hit my vocabulary size limit. So in this case, we will be using u 8,000 as our vocap size. uh the number of bytes that are present in the UTF8 in the bite tokenizer will be 256. So we will see 256 bytes getting progressively merged to form an 8,000 vocabulary size uh bite pair encoding tokenizer. So we come back to our bite pair encoding tokenizer and follow the implementation. So this is our bite pen encoding tokenizer and this is how it is implemented. Now for a change we will be importing a library and not be implementing our own bite level BP tokenizer. There are two reasons for this. One there's no actual gradient descent and back propagation happening. So uh implementing this would derail our focus from getting into the actual aspect the BT aspect of training a model. Uh second uh the tokenizers implementation has been done in Rust. So it's incredibly fast and uh I highly recommend that this should be used in a production environment and if you do want to double click on this topic further Andre Karpathi has an excellent excellent material has an excellent video on how it actually works where he builds one from scratch. So with that uh we're going to initialize our BP tokenizer. So we're saying if our BP tokenizer is none then uh we need to install our tokenizers. We set the vocap size. We're going to set some special tokens. So this will be the BOS and the EOS token. The beginning of sequence and the end of sequence token. The padding token so that if we pass a batch and the batch should be of same length then we can use this token to pad it. any unknown token or if any token we want to mask explicitly. So the training starts over here. This is what it does. All it requires is a data path. Now we determine whether the data path is a directory or just a file. If it is just a file then we're going to u wrap it in a list. And then if it is a directory, we're going to pluck out all of the txt file whether nested or not from this particular folder and make a list of txt files here and use that for our training. So all we need to do here is tokenizer.train pass the vocabulary size the minimum frequency and the special tokens and this will ensure that our tokenizer is trained. So once we train our tokenizer, we would actually be able to view some of the aspects of the tokenizer. And this will be way more clear once we run it. But since I've run it already, I'll just show you some of the output. So here is our vocabulary.json. Try and format it. And we see that we've only got about 7,600 uh tokens. This is because we might have hit our stopping point. with respect to the minimum frequency of the occurrence of a pair. So if I scroll right to the top, the special tokens come in first and then all of our UTF8 tokens come in. So the first 256 plus the special tokens would be dedicated to single characters only and then from there uh as I scroll through you can see that merges and patterns have started emerging. Uh my best guess is this particular character stands for a space. So space should gets a token ID of 754 and so on. If we want perspective on what all has been merged and this again will be data dependent on what data you provide to it. Uh we've provided a tiny English text full of poems right so it'll take a look at this and it will be leaning towards uh collapsing a lot of these multiple spaces. So this can be same uh this can be set towards Python code as well. If you just provide a lot of Python code, all of the indentation will start appearing as a single token. What you can also do is check out merges.txt uh which I've written. So here it talks about all of the pairs that got merged. So here we can clearly see a space and a space got merged to create a double space. Further a double space and a double space also got merged. And then from there different characters started getting merged. And then as we scroll through we will see that characters of uh more than one length are starting to come together over here. So h a d a i r a s e and so on. I've seen a couple of these before where uh it starts to catch nice suffixes. So for example, CTIO N was a particular token that got merged. So as you can see there are seven occurrences of the term CTO N. Affliction, actions, production, correction. Right? So you can uh easily look at a tokenizer as something that do not mean words. So they're not a replacement for words. They're just smart statistics so that we can optimize our LLM to use as less tokenizers as tokens as possible more effective. So we train our tokenizer and then we save our tokenizer. So this is the implementation of the save module where I'm simply taking the folder structure given to me and then saving the different u aspects of the tokenizer. So there's nothing really to explain over here. So we'll move on. Now if I don't encounter a BP tokenizer then I'll finally fall back to my bite level tokenizer. From there I'll ready my data for the training. So I'll make a loader over here where I'm passing my data. This again is the same data that we use for tokenization which is usually not the case. So if I go towards making a loader then we'll look at a text PPE buffer that is getting created which is defined over here. So what we are doing is reading the entire text file again in memory since this is a tiny uh file in a production grade setting. Again you will deal with a much larger data set and you would probably load this uh iteratively in batches and create a data loader out of it. So in this case we load the entire text in memory and then uh create or encode the entire text like so with our tokenizer. So this time we'll be using our BPE tokenizer. We'll encode with our tokenizer that we have trained. We have a couple of implementations of the length and get get item as well. These should be things that we have seen before. So uh with the length I want to specify that uh I want to return the max of zero and the number of elements that this particular sequence has minus the block size so that if I pick a random integer and try to create a batch out of it by adding the block size then I'm still protected and I don't spill over my sequence of tokens. From there I have x. So this will be a random integer that will be passed where I'll take that integer till the context window and then right shift it to get my y and then simply return x and y. So we've got our data loader over here and then we return a data loader which is again uh this will be a lazy load implementation. So as and when we iterate through a data loader it will at runtime ask for each batch one by one. So we get back to our data loader which is again a lazy loader. This won't store the entire data in memory. Then we get to our build model config. So here what we are trying to do is that if we find a checkpoint that has the config element present in it and the vocabulary size of the model is equal to the vocabulary size of the tokenizer then all is good. If it is not we're going to raise an error and here if I don't have a checkpoint I'm going to create the model configuration from scratch based on all of my arguments. So the uh n embeddings, the n block, the layers, etc., etc. Uh whether to use swig loop or not, rope or not, usual bells and whistles that we have seen in the previous part. So we're going to build our config from there. So taking a look at this here, it simply returns all of the desired configurations that we are well aware of and have seen this before. [Music] Now this section we do a bunch of declarations. So this should be an all too familiar model initialization. So we're not going to go into the aspects of what a GPT model uh is constructed with because we've already taken care of that in the previous part. We have an optimizer and we've used this optimizer before with similar parameters. So this should also be pretty familiar. Total steps we're taking as the minimum of the steps parameter passed or the epox times the length of the train data loader. With warm-up we want to do the minimum of the warm-up steps or the total number of steps divided by one. Now we've got a scheduleuler over here warm up cosine LR and we pass in our optimizer the warm-up steps the total steps and the base learning rate. Let's double click and find out what is going on over here. So this is our warm-up cosign LR implementation. So what we have here is in the initialization phase we assign the optimizer the warm-up steps the total steps and the base alert. And in the step phase I've got two aspects to it or two conditions. In condition one I am in the warm-up stage. So this specifies that the current step is less than the total number of warm-up steps that I have configured so far. So what this will do is linearly increase our learning rate. We'll check this out in a minute. If not, then we fall back to our cosine scheduleuler. So if you look at this, the base learning rate has been selected as I think 3 eus 4. So let's pull up a terminal and check out what happens. So we've got our base learning rate. So if I pull out a quick Python shell. So we've got our base learning rate which can be assumed to be something like this 3 - 4. And then at the first stage I'll be multiplying with my step number divided by the total warm-up steps. Let's assume that to be 20. So if I do this I get a tiny number as the learning rate. Now as the warm-up steps increase the learning rate increases gradually and then eventually what will happen is as the warm-up steps are complete we will approach towards this being the actual base learning rate that we had set originally. The core idea of this is u you want to take the guesswork out of the base learning rate as much as possible. So what you want to start with is not directly with a base learning rate, but you also want to uh settle your model into that base learning rate by starting with a much tinier learning rate and then working your way up on a linear schedule. Now once you reach your base learning rate, you're actually going to do the reverse. So we're going to progressively in a cosine schedule step down our learning rate so that our training is stable. So that's our warm-up cosine learning rate scheduleuler. Now here we've got automatic mix precision combined with the gradient scaler combined with the gradient accumulation steps implementation. So what is gradient accumulation steps? Now, typically you want a higher batch size so that your model could converge faster. But then you're restricted by the number of GPUs and the memory it has. And that's why what happens is if you set a batch size of 128 and the entire batch cannot forward propagate and back propagate in one shot, you can set gradient accumulation steps to be let's say four. So what will happen is the data will forward propagate and keep adding the gradients in many batches of four. So let's say uh row one forward propagates and I compute the gradients. The next sequence comes in the next sequence gradients will get added to the se to the previous gradients. So it gets added progressively. So by the time you get to the fourth mini batch, you've accumulated your gradients additively and then you back propagate in one shot. In that way, you can use a higher batch size. So if we look at this implementation, we've got an initializer with the optimizer and the gradient accumulation default to one. uh we'll probably pass two or four over here. Um whether or not to use automatic mix precision our scaler and this is something that we have seen before in the previous parts how to use or what the gradient scaler does. And then from there uh since we are accumulating a mini batch of gradients the loss gets added over time. So it should be actually divided by the number of accumulation steps that you have and then it should be back propagated. Now this is an important function should step. So the idea is PyTorch has a dotep function which actually does the back propagation without which you will only keep accumulating gradients. So in that case what happens is you should avoid calling the step function in PyTorch during the training phase for as long as you have your accumulation steps. So here I'm trying to specify whether the PyTorch step should be invoked or not and we will be using this particular function in our training step. We will check this out. And then the actual step where we're stepping through the optimizer doing the actual back propagation and updating uh the weights and the parameters of the model. And if we have AM enabled then we do it via the scaler where it scales up the loss in order to avoid gradient underflow. So we come back to our train.py that's our grad implementation. And then from here if we have a checkpoint or if our model stopped training in the middle because it encountered some error or we decided to stop it, we can load the checkpoint and then resume from there. This is the logging part. So here I'm initializing the logger. So here arguments.log log defaults to tensorboard and if it is tensorboard I've got a DB logger class which is defined up top and here I'm going to uh name my run as uh a particular prefix concatenated with the time stamp and then I'm going to determine the directory for it and then this is where the summary writer comes in and will be aail available for me to write my logs. Now I will also be writing histograms. So I'm capping or uppercapping the maximum number of elements not to blow up my tensorboard. And then there'll be this run directory. So the log function will be a pretty busy function that will get invoked from various aspects of the the training. I think it's best to check this out when we actually watch the training happen. And there are some other aspects to it. We're going to log the histogram of some of the weights like the QKV and watch their stability. Uh we're going to log some text as well so that we can sample our LLM across every few steps just to check how the quality of the generation is improving. And then I've got a bunch of placeholders for uh hyperparameters and stuff as well. The weights and biases implementation on the other hand is extremely minimal and is left as an exercise. So we come back to train.py. We've initialized our tensorboard logger and we are logging our hyperparameters to tensorboard through passing our logger. The arguments which contain the hyperparameters. So this will be the bat size, the end layer, all of it and the total number of steps. So if we look at this particular implementation then uh if it is not tensorboard then we want to return because we don't have uh any other implementation for this apart from tensorboard. Then we create a dictionary of all of the hyperparameters that we know of. So the vocap size would be coming from our arguments dovocap size and so on. And then all we need to do is logger.h params meta total steps will be different. So this dictionary will contain all of our hyperparameters of our model and this will contain all of the the total number of steps for which we have trained. So from there we can come back to our u train.py. Now here uh if again tensorboard logger is enabled what I'm doing is taking a random batch of data from the train loader. So this will give me one batch of data X and Y and then log it. So just as an initial just to see that what is the quality of generation before training our model. So we've got the logger, the model, the XP and the uh YB and then we're going to log that particular graph uh wrapped with the tensor only version of the model which essentially means that uh here I've got a very quick initialization of the model in question. uh we put it in eval mode and then we forward propagate and then if out is an instance of list or pupil for uh every token that we have generated over here we return it right so uh got a nice clean wrapper over here and that's our logging bit again we'll go through the different logging aspects once once we get to it now this block is uh coming from real world pain. So what will happen more often than not is uh if you're going for overnight training uh the whole process seems to die on you uh more than once actually. So what this will do is every time your process is about to die, let's say you're running a program on your terminal and you press Ctrl C and it says keyboard interrupted and then the program dies, right? So what happens behind the scenes is actually a SIG term or a signal to terminate is sent to this particular program and it turns out you can actually handle that request. So in this case what I'm saying is if I get such a control C request or a termination request I'm going to run my on_term function which will turn on a flag which when turned on will actually save a checkpoint in our while loop. So if you see this while our model is training I'm checking at every iteration whether the save requested flag is true or not and if it is then I'm actually saving a checkpoint over here. So that's what this thing does. All right. So let's go through the training loop. So we set the model to train and then while the current step is less than the total number of steps that I have configured, I'm going to invoke an iteration of our data. So XB and YB, a batch of X and a batch of Y. If the current step exceeds the total number of steps, then we're going to break. If the s requested flag is true, we're going to go for an atomic save all. We'll look into this function in a minute. I'm noting the time over here because I also want to log my throughput or the tokens per second. So at this point I'm taking a snapshot of the time. I'm transferring my data to my device. Now with AM enabled I do a forward propagation and then I do an AM based backward. So this will simply invoke our AM grads backward function which we have already seen before here. So based on whether we are doing an AMP based training or not will invoke the scaler or simply do a backward prop. So here we get to invoke a should step function. This is where we actually check whether the current step is a multiple of the gradient accumulation step or not. So now if we reook at that function then we're checking that this particular step is it completely divisible by the accumulation step and if it is we will actually do a step. So we'll actually do a scheduleuler step if this is enabled. and then increment our step. Now we've also got to save every parameter. So let's say if this is configured to 50, we'll just save a checkpoint every 50 steps. So atomic save all we pass in the model, the optimization or the optimizer, the scheduleuler am object, the step, the current step, the out directory, the token directory and the keep last K. What it does is it actually so since we are saving every 50 steps and let's say you're going to train 500 steps you don't want to blow out your disk memory um by actually having a bunch of these intermediates files saved. So we have configured keep last K as two. So it will only maintain a rolling buffer of the last two set of weights. and then the config build that we have seen before that contains all of my hyperparameters. So again with the tensorboard logger we're going to uh do a checkpoint and now let's look at the implementation of atomic save all. So with all of the input that we have seen we're simply going to save a checkpoint with the model the optimizer the scheduleuler the am and all of the details here. So that's how the PT will be written or the PyTorch file will be written. So we get the last checkpoint from this particular function. So from our out directory we'll get the last checkpoint and then we'll also just return a hard-coded model last. And then uh we'll rename the last step to the model last so that we know that this is the last checkpoint. And then from there, this is the implementation of the uh key blast key. So we're sorting our directories with all of the PT files that we have and anything that is not within the scope of the key blast key will just be evicted. So this is a way to delete these files. Uh we'll see this in action once we train our model. So that's our atomic save all. Now in the logging step we have every 50 steps we're going to do logger.log the step the loss the learning rate and then we're also logging a bunch of these runtimes. So this will be uh the throughput or the tokens per second that I want to measure. So if I'll take a look at log runtime I'm also taking a date snapshot and then subtracting the previous time stamp that I got from here. Uh now within this time period a total of XP number of tokens were processed. So I can calculate the tokens per second and then if you're using a GPU you can go for a um the the amount of memory that was allocated by CUDA and then I'm logging all of the metrics here. So we'll come back and now we're looking at model stats. So with model stats what we are doing here is first of all we get the parameters of the model and then we take an L2 normalize or L2 normalization of the entire set of parameters of the model and just to track it over time and see that how they have increased or decreased. So this will be interesting to look at goes to show how much stability our parameters are undergoing. We'll do the same thing for gradients. Now we are u doing a lot of uh processing with respect to the grad scaler as well. So this will be an interesting metric to track as well. Now we're doing a torch.nome on the gradients as well. So we're logging both of them here. And if I have histograms enabled I will also have histograms of the gradients and parameters. So here we're not doing the histograms of the gradients and parameters but we'll do it for the QKV. Now here I've got a maybe log attention function where again I'm passing the logger the data so that I can recomputee some of the attention mechanisms and pluck out some intermediary variables. So this is our implementation of the maybe log attention. So here uh we are actually getting a batch of the input data and this we are doing is just because we are importing our model from the previous part and we'll have to just run it again so that we can intervene in the middle and hack into or hook into a bunch of these intermediary outputs. So we've just decided to uh run one forward propagation again to get the particular tensors uh for the QV. So we've got uh x it goes through the token embedding and then uh for every block it goes through the qv and then finally the logging of the histogram and the logging of the L2 mean of all of these QVs happens over here. So once we see this things will be clear and then I've got a very cheap implementation or an approximation of the uh FFN so that we can approximate that how after the first block the token comes back to the second block to be processed. And then finally the log samples is simply every few steps we are going to literally generate samples and log some of that text so that we can view it. Atomic save all is something that we've seen before. So we're doing the final uh atomic save all so that our model final checkpoint of the model can be saved. And that should do it. So without further ado, let's actually run a training step and see things in action. So we'll exit our Python shell and come to part four. Now you see that I've already got some data from previous runs. So it's better that we remove all of them and then start from scratch. So this is going to be our implementation. Runs a bunch of tests as usual. And here I've got a runs folder and a tokenizer. And you'll see that uh the model.last is always a last snapshot of a particular uh uh step. And then uh every 50 steps the model is getting rolled over so that we are not storing more than two instances of our model. So once the 250 comes in the 150 got evicted. Once the 300 comes in the 200 will be evicted like so. So that goes for a nice implementation. And once we've completed our model training the checkpoints will be saved. We're going through uh just to generate stuff as well. And in our orchestrator.py, we've got the command to run our tensorboard. So let's check that out. So we've got our log directory as runs/p part 4 demo. So we've got our tensor board running. First things first, I would like to look at my hyperparameters. So with the bat size, the block size, the dropout, epochs, gradient accumulation and the total steps is something that we have logged. Now if we come to the time series, we will be able to see the loss uh decrease over time. So that's good with the cosine decay after the warm-up. We are seeing that it's gradually decreasing over time as per our cosine schedule. We've got our total number of steps as 300. We've got the different QKV histograms and their L2 means. We'll look at histograms separately. So here we'll be able to gauge whether they're able to follow a normal distribution and are uh being stable or not. So you'll be able to see that for the block zero the K histogram it started off with a much wider band in the 100 step but by the time it got to the 300 step it's much more narrower. That means the variance has reduced which is exactly what you're looking for in a training run. Similar things are happening for uh Q, K and the different blocks that we've got there. In the system metrics, we've got the GPU memory allocated and the throughput tokens per second. So that's all good. And in our training, we've got the global uh L2 normalized version of the gradients and the L2 normalized global um version of the parameters. And also notice that these have decreased over time, which means lesser the variance, the more stable the training, the more smoother the convergence. So we've got our scalers over here. nice clean view to look at. We've got our distributions and this is what I'm more interested in the histograms where the QKV blocks have been logged over here and at every step we can see that the variance is reduced. So this is good. Now in the text I want to show you how uh at every logical number of steps we are actually logging what the model is outputting. So at step 50 this was the quality of the generation and you can see that by the time we get to 300 uh the quality would have kind of sort of increased a little right so more training more stability better generation so that's all there is to it on the tensorboard logs now I want to show you a couple of test cases just to play around and show you some of the capabilities that we have built So, I'm going to remove all of my checkpoints once again. And what I'm interested in simulating now is actually some of the painful aspects or the negative testing aspects of u our venture. So what we'll do is uh we'll start training and by the time we see a couple of checkpoints we will actually press Ctrl C to stop the training and observe the behavior. As always we run our tests and our runs have started. So we have the first checkpoint come in. we'll see them rotate a couple of times. So, we have the 100 checkpoint and let's say the let the next checkpoint come in. And now I stop the training by pressing Ctrl C. So, I've had a keyboard interrupt and you'll see that when I was about to stop the training, it was trying to process its 180th step. And because of our sick term implementation, we will not fall back to our 150th step. we can actually have the step during which our training failed. That's powerful. So it saves a bunch of uh computation. So now I run this thing once again. I'll get a notification that it has detected that it is it has been trained for 180 steps and it will just resume from that. So let's watch that in action. So buried beneath the bunch of logs we've got this. So it is resuming at 180. And you will see that the rollover is happening now where uh we've gone from evicting the 150 and the 180 to the 200 to now we have completed the training entirely. So this is good. So some of the capabilities that we have developed in this combined with the modern architecture that we have seen in the past would actually become some of the or will take you close to some of the production grade implementations that you will do for training large language models. Hopefully this part was helpful as well and you got exposed to some of the training aspects that are less discussed. I would say uh some of the model architectures are more discussed but babysitting the training process is also uh a lot of pain. Hopefully you learned something and had fun while doing it. Thanks a lot. Hello and welcome to part five uh of the LLM from scratch series. Uh in this part we are going to cover mixture of experts. Now, we've spent some time over the last two parts modernizing the architecture as well as going through how a production grade training might work. In this, we're going to concentrate on one more piece of modernization of the core LLM architecture from the Mandela transformers. So, we've taken care of modernizing the attention block. This time we're going to focus on uh the feed forward network that happens outside the attention block but within the transformer block. So we're going to cover some theory on expert routing, gating networks and load balancing. We'll jump to the implementation and then from there we'll try to watch a worked out example to figure out how it all comes together. So first things first, what is mixture of expert? Now here we have a rough diagram of the architecture of mixture of experts. Now as you can see all of the uh attention blocks have been collapsed in. So this can be thought of as the more modern version of multi-headed attention with the group query attention and the swigloo and the rope and all the bells and whistles that we have discussed so far. But from there instead of feed forward network what happens is there's a gating function over here or a router which takes the output from the attention module and then decides to route it to one of these or one or more of these experts. So this router will have probabilities emitting from it and based on that we are going to route it to one or more of these experts. Each of these experts is a multi-layer perceptron or simply a linear layer and then from there the whole thing goes forward. What's the point of doing this? Now first of all it out of the box gives you more parameters. So the more the parameters the expectation is that the more sophisticated the model will be. But instead of a dense implementation where every token and its parameters propagate through every MLP that you have got configured over here, we have a more sparse implementation and this sparsity ensures that during inference only a fraction of these weights are activated thereby giving you more throughput with more parameters. So even though you can have a similar model with a dense parameter, it's more beneficial to have an MOE setup. This is because less number of activations means lesser computations means more speed. So during training time, the model fine-tunes itself to pick the right combination of experts. So you can think of it intuitively as one can be the coding expert, one can be the language expert and so on. uh the part where this particular experts tends to certain tokens is not within ourselves. It will be within the learning process and then from there the whole thing propagates and we have more layers of this stacked together. So it'll look like the model will have a lot of parameters but only a fraction of them will be activated during inference time. So that's the intuition or the inspiration behind learning mixture of experts. So we'll jump right down to the implementation part with that short introduction. So we've got the orchestrator.py and along with it we'll look at all of the uh mixture of experts components that will lead up to a hybrid architecture which can be a combination of a dense and a mixture of experts module. So got my unit tests here and a demoe.py. We are not going to train a model here. We aren't even going to come up with the entire LLM. We're just going to pluck out the MLP piece and then double click on it, zoom in on it, and then just use the inputs and outputs for that particular module. Now, since we've already seen the entire architecture evolve a lot over part three and part four, there's no point in cluttering by building on top of it further. It just goes for a cleaner way to understand what mixture of experts are if we just pick it apart and understand it. Putting it together is the trivial part. So we've got the demoe over here. Now here we have some of the arguments that are passed to it is number of tokens we want to generate, number of hidden lay or hidden neurons you want to have within your MLP or the linear layer, the total number of experts and out of that you only want to pick the top K experts CPU implementation and then the device. Now here I'm simulating a batch of sequence of tokens coming in. So if you have configured 64 tokens to come in, I'm actually splitting them over two batches so that we can simulate what happens in a batch forward propagation. Now here is our architecture. Now once we invoke that we're going to forward propagate it. And I've just got some helper code here that will help us visualize the shapes. But uh this is where the entire architecture is hidden. So it takes as input the number of hidden neurons you want to process and the number of experts along with the top experts you want to pick for processing a certain token. So in this case we are using topks one. I think we will override that by two. Okay. In this case, we've got top K as one and we've got a total of four experts and we're trying to process six tokens with uh for the MLP the hidden neurons will be 128. So here we've got the MO implementation where a mixture of experts layer has been implemented here. Now uh there are many implementations available. We are following what is present in this particular paper. So we're going to follow the switch transformers paper over here which was one of the more original implementations of the mixture of experts module. So we come back and we watch the initialization. So the dim parameter is nothing but the D model or the number of dimensions uh that will go in to this particular layer. And then we have the total number of experts, the topk experts, the topk gate, this is going to be our router or the gating function. And then we'll have a module list of all of the experts. So as per the number of experts, we are creating a module list of the expert MLPS that we're going to configure. So we've got the dimensionality input going in and the multiplying factor is going to be how much more hidden neurons we want against each piece of input. So we've configured this as four. So that means a 128 embedding coming in will have 128 * 4 number of hidden parameters in the MLP. So we're using swigloo and drop out if necessary. I think this is turned off. So let's look at the top gate. So here we have the meat of what is going on in the mixture of experts module which is the routing or the gating function. So first of all we assert whether the top K picked has to be at least greater than equal to one and has to be less than the number of experts. Then we assign the number of experts and the K and our WG to be a linear layer that takes in the input and then outputs the number of experts. So this is going to be a simple linear layer the gating function and we'll treat the output neurons as the probabilities of moving towards each expert. So we've only got top K as one. So this will emit four piece of probabilities and then from there we're going to pick the top one. So once you propagate the input through this linear layer, you're going to get the logits and then you're going to get the probabilities via the softmax and then once you have the probabilities of those four experts, you're going to pick torch.topk as per your configured K. and then you will get the top k val and their indices. So here what we're doing is splitting the probabilities by size zero and size one. So one important thing to notice is that we are actually going to pass our batch of tokens as a single dimension. So it's usually split by the batch the tokens and uh the channels. So here it's going to be tokens which is going to be a product or the squeezed dimensionality of the batch and the sequence in each row. So with that we'll get two sets of probabilities by forward propagating it through our data. Now we're going to calculate the importance which is going to be the mean of all of the probabilities along the expert dimensions. We'll watch this all in a minute when we run this. I think things will be clearer by then. So, uh here what we are interested in is calculating something called the auxiliary loss which says that this is going to be another loss component in my LLM which penalizes my architecture if it skews towards an expert. So when you go and implement a mixture of experts architecture, what happens is it gets prone to what is called the celebrity problem or all of the tokens start getting concentrated on a very few number of experts. So that the ex the expertise is not diffused well over the model. So for that what we need to come up with is an actual loss function that penalizes for imbalanced routing. So we'll again watch all of this and follow this through an example in a minute. So we're going to take the fraction of tokens assigned as primary. So we're taking the first dimension over here. Uh we're going to calculate load. And then load is going to be again the dispersion or the diffusion of the tokens through the different experts. And this is the formula for the auxiliary loss. It's the importance times the load for each expert summed up multiplied by the number of experts. Now, since we've got this print function, I'll probably comment out uh before pushing this. So, you'll probably need to turn it back on. Uh this will give us a lot of visibility into the probabilities, the importance, the hard one load and auxiliary loss. And then finally, we return our top indices, top K valves and the auxiliary loss back. So once we are back we have a module list of expert MLPS. So let's study that. Now the expert MLP here is uh pretty simple. So if I have swiggloo turned on these are going to be my different components. There's a linear layer. There's another linear layer. There's a silo activation or a sigmoid linear unit activation. And then there's going to be another linear layer for the out and a dropout. And since we've seen this before that the input X will go through input one set of weights. It will also go through input two set of weights and activation and then A and B will be multiplied together and pass through another out uh linear layer which will then go through the dropout. So this is going to be the implementation of our feed forward layer. So you see this is just one MLP layer. We will have four of them since we have four total experts. So we'll have four of them assigned as experts. Right? So what happens in the forward propagation? We receive a batch of data and then we squeeze down the batch and the sequence dimensionality into one and then we flatten it out. Now once we flatten it out, we propagate it through our gating function and we get the indices, the weights and the auxiliary loss. Now from there, in order to calculate my Y, I need to iterate through every expert and find my slot for the top K expert. Make sure whether I'm looking at the top expert or not. And for that I'm going to select all of the activations from the output of my gating function and then pluck out and impose that on top of the zeros that I've configured here. it would I know I'm going a little bit faster with this but I really want to get to the example part where we can actually visualize a lot of these propagations and that will make it a lot of self-explanatory instead of explaining or dry running line by line through theory. So finally we change our y to uh the BTC dimension and return our y along with the auxiliary loss. So at this stage what can be done here is with the auxiliary loss when it goes down the workflow we should actually add this auxiliary loss as part of the overall cross entropy loss that we have and then back propagate from there. So that is going to be our implementation and then uh once we run this we will finally go through uh the hybrid function which is again going to be a pretty easy implementation once you start understanding it. So without further ado we'll run our solution. So we're going through orchestrator.py which runs the tests and prints a bunch of stuff and we have the output ready. So we started off with six tokens and we have four experts with the top K as one expert. So this is the first tensor that we see which is actually the probabilities. So what is happening here is we have each row represented as a token. So we have since we are processing six tokens over two batches all of them have been concatenated and squeezed out into one single dimensions. So if you have three tokens per batch you will actually see six tokens here which is what we've implemented. So if I were to go back and confirm that you will see that I'm taking my random input and dividing it by two to simulate two batches. So although I'm processing six tokens, I'm representing them as two batches of three tokens each. And then when I get to uh the function before passing it to the gate, I'm actually flattening out my input like so. So I get a BTC dimension tensor, but I'm reshaping them to S cross C so that the batches and the tokens can be one stream and I get to know which expert to pass through. So you see here that this is a 6x4 matrix. So each row represents a token and each column represents an expert. So if I were to pick apart this cell then the way you interpret it is that.1651 is the probability with which you should route token one to expert zero. Right? So if I pick a random u random cell over here then this represents the probability that the token two should be routed to expert two or the third expert in this case out of the four that we have right. So now this is how the probability map looks like. Now if we move further down the line on how the gating function is behaving then from the probabilities we are actually calculating importances. So this importance basically means how much tendency that a particular expert has to process a token. Right? So uh it's a simple calculation. So we're going to take the mean of the probabilities along the zeroth dimension. So if I look at all of the importances printed out, I will see that it is a tensor of four. Four, it's a vector of four elements representing each expert. So this is the propensity for a particular expert to process a token. It's simply the probability uh it's simply the mean of the probabilities. So if we hand calculate this very quickly. So we've got.1651 * 237 plus 1776 3395 2751 and 28 57 or 07 the whole thing divided by 6 and we get our probabilities as 2452 8LE 3 for the first expert. So this has been approximated to 2453. So this is the propensity or the probability with which an expert will process a token or expert one will process a token and you've see you can see that it's pretty diffused by the state of things and then uh the hard one is actually getting picked as now even if you have uh top k configured as two doesn't matter what we're interested in grabbing here is the fraction of tokens assigned as prime primary. So for this token if I see the different probabilities I can see that this is the highest probability. So with top K as one this will be this expert will be hard assigned this particular token. So that's why this tensor shows two which is the index of the third expert. So 0 1 2. So this is the highest probability. Now if I look at the second token I can visually see that this has the highest probability. So the number here should be one in the third as well. This has the highest probability. So one comes up again and so on. So I've actually got the index of the expert which became the primary processor of a particular token. That's what I've got with hard ones. So we've figured out what the hard ones do over here. And then from there we need to understand what the load is. So it's pretty simple. I take the number of tokens processed by that expert and then divide it by the total number of tokens. I'll repeat that. So I'll take the number of tokens processed by that expert and then just divide it by the total number of tokens. So in this case if I were to pick apart expert zero then I can see that it has processed only one token. So 1 divided by 6 is what I'm going to assign as my load for the first expert. We'll quickly check that up and the math adds up. Now in this case for the second expert however we've got two tokens processed by the second expert. So this should be 2x 6 that is 1/3 which is.33. Now the same goes for the third expert or the expert at index 2 as well. So that's why its weight is 3. And for the last one we've only processed one token. So the load on this particular expert is this. Right? So we've got the different importances of an expert and we've also got the different loads on the different experts. So all we need to do is plug in with this formula. So E * importance time load sum and then whatever that comes out to be. So in this case the auxiliary loss comes out to be 1 triple034. So in order to really nail this out, right? So what we might need to do here is let's take in a Python shell and try processing it from here. So here I've got let's say the load is going to be absolutely diffused. So the load is going to be absolutely diffused which means every expert has equal probability of processing a token. Now if we assume this is the case then this becomes the best case scenario that you can have. Now if we multiply element by element for a and b this is what we get i * j for i comma j j in the zip of a comma b. So we've got this and then we take the sum of that and then we multiply it by the number of experts that we have. So which in this case is E which is four. So the auxiliary loss in this case comes out to be four. Now if I have the opposite end of the spectrum where it is completely getting concentrated on one single expert. Right? So I've got A and I've got B completely getting concentrated on one particular expert. Oops. All right. So, we've got our A and B. The same calculation goes and then I'll get a four. So, the loss is higher for a greater concentration on one particular expert and the loss is lower when the probabilities are diffused evenly among the different experts. So that's the whole situation with our gating function. The gating function is trying to route each token to the deputed number of experts. And then from there once that is done for every expert and for every slot in the top K that we have configured we check whether uh whether the index has been selected by that expert or not and then we process it accordingly and we've got a placeholder here that we override with the particular activations coming from that expert. So once we have that things look pretty simple and you will notice that the output shapes will be coherent with the particular processing that we are doing over here. So which leaves us finally to discuss the hybrid approach where I can take my vanilla dense FFN and then combine it in some capacity along with the mixture of experts. So we can blend the dense FFN with the MOE output and we can parameterize with the help of the alpha hyperparameter. So the alpha hyperparameter will be a number between 0 and 1 which pays different levels of importance based on how you configure it. So as always we've got the dimensionality of the input the alpha. So here I'm paying equal importance by default. uh the multiplier will be how many more or what multiplier of hidden neurons do I want to have against the input the swiggloo the experts the top k and the dropout right so we assign the alpha and we come up with the hidden uh number of neurons for our linear layer and we've got our over here and our dense layer over here so in this case I'm using a vanilla dense layer you can go for swiggloo based implement mentation as well. So when we forward propagate, we run the input through our dense and through our mixture of experts and then combine them by weighing them with the help of the alpha parameter that we've configured. So in this case again uh equal importance will be paid since our default is.5. So you can go for a hybrid implementation as well. So if you come back uh we read very minimal again used very minimal necessary force to understand theory on expert routing gating networks and load balancing. I really wanted you to have that experience while we were running the code and looking at the output and not get stuck in a bunch of theory. So we've also looked at how uh layers are implemented in PyTorch. Now this along with the part three which was modernizing the architecture and part four which was the scaling up of training will form the meat of what it looks like to have a production grade LLM trained uh training stability and communication and along with uh combining the with the dense layers for hybrid architecture. Right. So with that I think we are done with the part five as well. Uh I hope all of this is still making sense to you. We were very light in the theory and we picked apart a very small module and study it in details. So hopefully you enjoyed this part as well. See you soon. Welcome to part six of the LLM from scratch series. And in this video we are going to focus on the second stage of the LLM training which is the supervised finetuning stage. Now all of our five parts have been dedicated to studying the first part where we go through the pre-training which is again the sizable chunk of the internet and the majority of the time and energy spent on the whole process. In supervised fine tune however you will pick up a subset of very highquality data set and then from there you will fine-tune the model further to behave it like an assistant. Now base models in general are just document completion agents. So if you ask a question what it will try and do uh it will try and complete the document. So it might ask a question in reply because it might think it is trying to complete a questionnaire. But what it does not know is that it needs to behave as an assistant and needs to answer the question asked. This is where the SFT stage or the supervised fine-tuning stage comes in and the model starts behaving like an assistant. So without further ado, we're going to jump onto the code. Now the training and the model architectural aspects of it will be exactly the same as we've seen in the previous parts. All of them still apply. It's just that the data is going to be structured differently and propagated differently. So we'll see all of that in action. So uh without further ado, we'll jump onto the code. We start as always with orchestrator.py. I've got a couple of unit tests written and a train sft.py pi that we will use to train our model from the last checkpoint that we train. So this is we're going to treat this as our base model and then further fine-tune this model to come up with a new model and then we're going to sample from it to watch the quality of the results. So I've got our uh train s.py that as always starts with the arguments. So we start with the data and then we start with the checkpoint which is going to be the checkpoint of the last trained base model where we want to save the saved model the supervised fine-tuned model the number of steps the bat size the block size the end layer the n head and n embedding should all be familiar the learning rate CPU and the directory of the tokenizer so the tokenizer between these two models is going to be common we choose the device and here is where we load our data. So in this case, we're going to start our finetuning with a very tiny sample data set and then we'll see how we can potentially scale that up to a bigger data set. So I've got load tiny HF module over here which has a data class SFT item that contains two properties, the prompt and the response. Both of them are strings. So it goes in and tries to load this data set from the internet. So this is a standard finetuning set from the internet. We can have a look. So this is the data set that we are going to operate on. It is going to be a 50,000 row data set. Uh we will not have the resources to train on each and every one of these. So we're going to take a peek at what this data looks like. In its essence, it has about four columns. The instruction, the input, the output and the entire text over here concatenated. So the instruction is the actual uh task that you want the assistant to do and if there are any inputs required, you provide it over here. So for example, a program to reverse a link list with the input as list with 13579 and then the output listed as what you want the answer to be or what you want the assistant to answer with. So we're going to use all of these columns to create an instruction data set and then further fine-tune our model. So with this data set, we're going to grab the instruction input and the output. We're going to concatenate the instruction and the input and structure it as our prompt. And the response will be the output that will be expected from the assistant. Now if we choose not to load this data set, we are going to have our own data set. So if you have trouble connecting to the internet or this data set is not available anymore, what you can choose to do is train it on a tiny data set. We are going to do this part first anyway because when you come up with these architectures, what you essentially want to do is uh try and overfit on a very tiny data set to see how it's behaving to see how the loss is going down and you're getting all of the expected results and then you scale it up. So, we've got the data set loaded. We're going to check some of the data out. We're going to print it. And then we're going to create a list of pupils from this. And we have this length curriculum which will help us draw more samples from these list of pupils. So for a particular I if I'm asking for a next item I will stop the iteration or I will basically grab the ay if index and then return it. So this just helps us get more data to create our batch and then forward propagate and back propagate. A lot of processing is happening at this stage. So we've got the SFT collater over here where we pass in the block size and the tokenization directory. Let's check this out. So it initiates with trying to load the BP tokenizer. Now if you provide a directory, it will try and load from that directory. If not, it will try to create one from scratch. And as always, it fall backs to the bite tokenizer 256 vocabulary. I've got a property here that you can grab and check the vocap size. And then there's the encode function which leverages our bite pair encoding tokenizer to encode text that you've sent and encode it back to a list of uh a list of tokens, token ids rather. So once you get the text, you just do self.enccode encode and then return the ids as a list. Now I've got the collate function where a lot is happening. So let's go through it. Now this collate function will receive a batch of data and as we know each row in that batch will contain a prompt and a response where a prompt will be the concatenation of the instruction and the input. So what I'm interested in doing first is taking the entire prompt and then formatting it. Now we're going to format our data in a particular format. So here I've got the template for it. We are going to have a BOS or beginning of sequence token and then in the next line we're going to have instruction like so. And then we are going to add the actual instruction for that row. So this will again be the concatenation of the instruction and the input. And then it will have a response which will be the output expected out of the large language model. And then finally we end it with an EOS token or end of sequence token. I've got a data class here which has the instruction and the response. And for the various formatting, if I have a response associated with a particular row, then I'm going to structure it like so. So the instruction and the response are going to be plugged in from the example class over to this string template and then returned as a string that contains the instruction and the corresponding response. Now format prompt only is where you're looking to only format the prompt which will be devoid of the response. So here all you're looking to do is taking the incoming instruction and then structure it to be placed over here and then the response will be blank. Now note that if you're trying to format the prompt this way, it also adds an end of sequence token which we want to get rid of. So why do we have two such functions? Now the first function format example is actually the end to end example of the X and the Y where I have the input instructions as well as the expected output. But when you do inference, you don't have the expected output, right? So you only have the query coming in from the user and all you do or all you want to do is format it as per the template and then you want to come till here and then pass it on to the large language model so that it can return the completion response. So this will basically fill up this entire section after it has been trained during inference. So we have the format prompt only. So let's come back. So what we are doing over here is grabbing the prompt and then formatting the prompt only by passing the prompt and then replacing the EOS or the end of sequence token with a blank. So typically what you would expect over here is that while you will have a row of data that has the instruction and the response, we're plucking out the prompt and only uh we'll try to structure that part. In that way, we will be able to segregate between our X and the Y. In fact, I think this will be beneficial if we run this in debug mode and watch all of the data in action. So let's actually do that. So I will have a collater coming over here put a break point and then for orchestrator I've got my launch.json JSON configured. So I can simply run this and since this runs for the tests also I will let this pass and then we'll analyze it the moment it comes from train.py. So there's a train.py Pi that came in a batch of data has been uh collated and then we get the first piece of text. So this batch is a list of pupils where one element the first element of the pupil is the instruction and the second element is the response. So since we're using a sample data set we will be seeing only these. So there will only be three data points configured over here. So the prefix text is only going to be the instruction part of it. So here we can see that and if we really and if you really see this instruction is that it has the beginning of sequence token and then the instruction and then the response blanked out. So now what the next piece does is it formats the entire example which has the x and the y which is the instruction and the output. So we're going to grab our text and watch it in our text data set. So this contains the response, the expected response as well as the end of sequence token. So we have the prefix text and the full text. Now we're going to encode the entire text. So we can watch these IDs unpack over here. Now these are going to be the prompt ids of this entire text. So our beginning of sequence token has an ID of zero and our end of sequence token has an ID of one. And we can check this out by grabbing the vocabulary that we have chosen to use on our part 4. So if you see ID0 a beginning of sequence and a special token is true and with the id one I have an end of sequence token. So we get the prompt ids over here as well. So these will be the ids for the various prompts or this particular prompt rather. So while this is for the entire response, this is only for the instruction without the response and the end of sequence token. So you see you have the response over here and the end of sequence token which is missing over here. All right. So from there we step over. Now I've got the length of my prompts which in this case is 24. So I have 24 elements over here. I'm going to take a copy of the X and the Y. Now what I'm interested in doing is I wish to configure Y from this so that it first of all gets rid of the entire prompt. So I'm going to uh get all of these elements which are part of the original user input and then replace them with minus 100. Now the minus 100 is the ignore index in PyTorch. And what it will do is that once you pass a minus 100 as the index, PyTorch will actually ignore this particular positions while calculating the loss. So what you want to calculate your loss on is the completion tokens that the model output not it trying to complete the user input which is already available at my disposal. So I'm going to grab all of these input tokens and replace them with minus 100. So this is going to be a for loop and then the y is actually going to be a shifted form of the x. So if you have the entire text as your x, you want your y to be shifted like so. Right? So for every input token X, you want the corresponding next token to be Y. So what we are doing here is right shifting all of our tokens and then finally we'll be left with the last token which we again need to ignore because it will not be part of the response. So we are right shifting over here and then we are masking or ignoring all of the prompt tokens over here. So once I do that my X is going to be the entire input plus the output and my Y is just going to be the outputs only. In that way I will be able to calculate the loss for these two particular tokens in this example and then I will be able to back propagate from there. So we can run it till here. And then what you see is that the Y now has been converted and it only has tokens turned on in these areas where I'm expecting. So for the token 203 I will be expecting a response of 22. for the token 22 I'll be expecting the response of one which is the end of sequence token and then this gets ignored and so does everything before that which is going to be our actual prompt. So this is going to be our y and x is going to be the entire input plus the output. And in this way when we correlate this x with this y we will only compute the loss in these positions so that we can judge our model based on the completion tokens only. So that's the way we're going to structure it. And then finally for this batch we have the x and the y coming in. So going to disable this break point right now and I'm going to try and run till this line and then we can continue. So our collate function run and what it has will take a final peak into this. So this contains the entire batch of data. Now what I've also done here is this is going to be all of the input along with the EOS sequence. Now since I've got a block size of 256 in order to maintain consistency across batches because different prompts will be of different lengths I need a padding token and I've chosen two as the padding token. If we go back again and visualize our vocabulary, you will be able to see that I have a special padding token configured over here whose ID is two. So you can take a look and this is exactly what it will behave like. All right. So this is where I'm saying pad all of the tokens till the block size to two and then for the rest for the Y it will be minus 100 the ignored index for PyTorch. So here we've got the XB a batch of input and output padded and then we've also got YB which is going to be a batch of all of the indexes. So for the first example that we saw only these two tokens will be turned on. For the second example we have these and for the third example we have these. Right? So everything else will be ignored. So we're intentionally doing it with a very tiny data set so that we can really really visualize what is going on. So we transfer it to the device forward propagate our preconfigured model that we have imported from part 3 and we haven't attached all of the bells and whistles. So this will have KV cache but for example we haven't configured sliding window attention and attention sync in this particular instance. So we go through the logits and the loss. We're going to step backward and this is where you can also come in and say you want to have a mixed precision training. You also want to uh enable gradient accumulation so that we can go for a better batch size. Again I have tried to avoid all of the clutter so that we can hyperfocus on the data structure for the SFTPS. So we're going to step over here and for every 20 steps we are going to log the loss. So once we do that we're going to hit toss.save and then we should be good. So this should ensure our training completes and um we will have the supervised fine-tuned model at our disposal. So with that, let us actually proceed towards a full-blown training rather than running it in debug mode. So we're going to stop it. We've pulled up our terminal and then we're going to train on a sample data set. So it goes through the tests where I've written tests for formatting and the masking happening. So u I faced a lot of bugs while configuring the test masking part because uh you had to right shift the y and then from there you also had to make sure the right tokens were padded. So first things first we print a sample of our data and then we're going to load our train tokenizer from here. We're going to load our last checkpoint of the base model from here. We fine-tune it through a bunch of steps. We save it again. Since this is only three examples, it is going to overfit. And then from the Python sample SFT.py, we are going to sample uh the data. So I'm asking what are the prim three primary colors and the response comes as red and then various questions that have already yielded the correct answers even a coding question. Now we'll check these out in detail. So let's proceed and see what the sample SFD does. So now that you have the trained model, it's essential for us to understand that you also need to sample or you also need to run inference on it and there's a lot of data processing involved to make sure that it is in the right structure so that the assistant can respond in a certain format. So here we will pass the checkpoint of our uh trained or fine-tuned SFT model. We're going to take the prompt from the user. The block size and layer and head embeddings are all part of loading the model checkpoint. How many tokens that we're looking for? And notice that we've also you'll notice we've also implemented early stopping. So if while generating tokens the model encounters an end of sequence token, we're going to stop there. we're not going to generate more tokens from there. This is because our uh data has been configured to stop at EOS. So if the EOS goes as input to the model, it might output junk from there. It might things might not make sense. And of course, I'll also have to provide my tokenizer over here. So I'm loading my checkpoint. I'm running my SFT collator or uh initializing my SFT collator. GPT model is the exact same thing as we've done in part three. And we're loading the state dictionary putting it on eval mode. And then the prompt that I get from the user will go as an argument to format prompt only. And if you look at format prompt only, it provides you with this format. It will give you everything till here but it won't give you the response since that's what we're interested in generating but then it will also provide this as the end of sequence string is a part of this template. So what we need to do over here is remove it so that the model can go on to completing documents and then if it generates an US token we're going to stop. So we're going to take the prompt text and encode it and we get back a list of ids. We're going to convert that to a torch tensor. And then we are using the generate function over here with all of the input index, the max new tokens that are configured, the temperature, and the top key. We've seen this all before, so we probably do not need to double click on this. Now the model will split out an output which we will convert to as a list and then if it is coming from our tokenizer we will decode it and then we will print the generated text. So it's pretty simple all of the things that it is trying to do. So if I run it again and since we've already trained our tokenizer we don't need to wait for it. So our sample SFT.py PI grabs the checkpoint from the last train model with the block size, end layer, end head and embedding that are familiar. Now I wanted to add a quick note on the context window and uh this I'm sort of injecting after I'd recorded the original video because I had forgotten to mention it. So you will notice that uh during the SFT phase you can actually have a different block size or a context window compared to the base model. This is because the matrix the way they are configured it works out. So the same model with the same weights can be trained in two separate stages with two different context windows. What does this do? First of all, it's a nice feature to have. Second, with a base model with a lot of data and throwing a lot of compute, you might want to train it with a lower context window. But during the SFT stage, when you're only going to be working with a subset of the training data, not in the scale of millions, but in the scale of thousands, you might want to increase your context window at that point. So this is where a base model trained on 8K context can become a 64K context model during the SFT phase. This helps us seamlessly expand on the context window without compromising a lot on the cost and compute. And then for the prompt we're sending what are the three primary colors and notice that this is also part of our data set and we're only using a sample data set. So first prime number what are the primary colors device name which points to direction. So we've got our data set over here and we've got some questions configured over here. Some of them are from in sample, some of them out of sample. And you can see that since this is only trained on uh two or three data points, uh from in sample data, it answers correctly. But when things go out of sample, it still tends to fall back on any particular token, right? It does not answer questions correctly. So was asked to reverse engineer the factorial code and that's all it gave. Right? So we're going to scale it up and see uh if it changes or not. But before doing that I really also wanted to zoom in on the sample SFD and how this works. So we're not training anymore. We're just interested in understanding how the sample SFD works. So I'll put in a break point over here and then we'll try and debug it so that we can visualize it together. So uh we tried to run the sample SFT directly which won't work. We'll have to start from the orchestrator. Then after running the tests we are in sample sft.py. So we're going to step over. This is going to be the checkpoint and where I've got the model loaded from the previous run. I'm going to grab all of the configuration. So the vocap size, block size, layer embeddings, etc. All of the rope and swiggloo and all of the stuff. This is going to be our collator and this is going to be our model. We're going to load the state dictionary of the model. So basically the weights going to put it in eval mode and then prompt text will be the arguments doprompt but formatted. So if you look at it, the prompt is the raw question that was asked. What are the three primary colors here once I step over the prompt text becomes the instruction fine-tuned version. So it has the beginning of sequence token, the instruction, and the response with a placeholder for the LLM to complete it. So we're going to encode all of those ids, convert that to a tensor, and then let's actually step inside model.generate. So we're going to step into this and see what happens. So we are inside the generate function where we got a model. We got the input tensor. We got the max new tokens that we want to sample. The temperature, the top K, the top P, the EOS ID. So this is something that we are seeing over and above what we have seen in part three already because we're interested in early stopping. Then uh we are not using sliding window and attention sync. So from there we do our imports put the model in eval mode. The prompt is going to be the idx. So this is going to be the sequence of token ids that we have as our input. Our kv cache is going to be initialized for the two layers that we have as none. And this is going to be the first generation. So this condition will get invoked not this one because we haven't produced any token yet. So we're going to step over the ID condition pretty much contains all of the input ids against this prompt. So this is going to be the beginning of sequence token. This is going to be new line and then three hashes follow like so. And then there's it goes on like this till the last new line which is over here. Right? So the starting position for this will be zero and then finally we forward propagate with no KV cache since this is the first invocation. So we get our logets. Now since I get logets for each and every prompt uh and its corresponding prediction, I'm only interested in the last completion token. So I grab the last one, step over it. I'm going to do my top K top filtering. So uh we will be able to see the difference on how the top case stabs everything to zero. So let us grab our next logets and try to visualize it here. It's a tensor full of logets for uh should be of length 8,000 because that's our vocap size. And then from there after the filtering the next set of logets now pretty much everything becomes infinity. there'll only be three tokens which are uh turned on. So in order to really confirm this, so I've got the next logets over here. But if I filter it out by next loits that are greater than float of minus infinity then I will be able to get three such loads. Right? So uh our temperature is not configured to zero. it's 02. So there will be uh a sample drawn from one of these three tokens as per their probability distribution. So this is where we compute the probabilities. So if we step over and look at it from our debug console, it will also be a bunch of zeros. But then if you filter out there will be three probabilities where uh one particular token has an abnormally large propensity and then uh there are two very tiny such probabilities. So from there I actually grab my next ID based on the sampling technique and the next ID is going to be 744. And then if we go look at our vocabulary for 744. So the question asked was what are the primary colors and it seems to be that it has correctly predicted red. So from there we check whether it's an EOS token or not. So the next ID is not going to be the EOS ID in this case because 744 not equals 1. So it does not enter the break and it continues to generate. Now this time however we are going to have KV cache at our disposal. We can again confirm this by checking out the KVS. A KVS is a list of two KV cache elements, one for each layer. And each KV cache data class actually contains two tenses, one for the K and one for the B. So from there we step over. Our starting position is this time going to be 31 because we had 30 tokens in the input and we've generated another token, another completion token which makes it 31. So our model is going to start generating from the 32nd token onwards. The KV cache is filled up and this time we are only going to send the last token as the input. The rest of it will come from KV cache. So we go from there. the next set of loget probabilities and then the top K filtering the probabilities and then again if we look at our probabilities now this will be concentrated on one token again having a lot of propensity to appear so our next ID now becomes the tensor one which is the EOS token so this time we will break out of our loop and it actually enters break because uh it has hit the EOS sequence and then from there it will return the entire sequence and we're back to sample SFT.py and then from there uh it takes the output decodes it and then prints the generated text onto the console. So we can let the program run from here. disable the break point. And if we come back to our terminal, we will notice that all of our output has uh appeared. So that's the part where uh we have completed one tiny sample training. Now what we're interested in doing is actually scaling this up slightly. Now, uh, in for all intents and purposes, you can actually, uh, do a full-blown instruction fine-tune data set, but we still don't have the resources on our laptops train an entire SFT model. So, what we're going to do is give you a flavor of the scaled up version. So, we've got the uh, alpaca data set for the IFT or instruction finetune or the SFT, supervised fine-tuned data set. We're going to actually load that by setting sample data set equals false. And then we're going to load 24 data points from it. And then uh we're going to sample three questions this time again that was part of this data set. So while our toy data set answers the question what does DNA stand for with either compass or red or stuff that it has seen in its training data before? Uh this time the training data will be aligned to the hugging face data set and it should give us the right answers from here. Now for that obviously we need to train our model because the data has changed. So we'll put it back on training. We'll clear the console and then rerun the entire thing. It runs the tests, starts running the train. Uh it has printed the a batch of data. So it has these a bunch of questions and answers configured and then from there uh it starts uh optimizing. Now we started off with the high loss but we can see that the loss is healthily decreasing. Uh and then from there once we sample so what are the primary colors this time it says the three primary colors are red, blue and yellow. What does DNA stand for? DNA stands for deoxy ribboucleic acid. And then even if you ask it to write a program it will actually do so as part of your response. Right? So uh as per the instruction fine-tune data set that you have configured, we are able to see that the model has started responding. Now uh not anytime soon will this become will this particular model become intelligent. We've only got two layers of uh transformer blocks at the end of the day. But hopefully you get the flavor of what a production grade SFT looks like. Now in the real world you would probably take a base model that someone else has trained and you might want to go for a Laura finetuning or you might even want to take the instruction fine-tuned version of a model from the internet and go for a Laura finetuning. There are just too many good resources on the internet that already covers Laura fine-tuning. So we've decided to skip this as part of this uh particular series and after all this is called LLMs from scratch. So we want to check out each and every element and module that happens from scratch and not build on top of anything um as much as possible right uh so with that we come towards the end of uh this part so in our supervised fine-tuned model uh we covered the instruction data set formatting the causal LM loss with masked labels the minus 100 ones uh the curriculum learning that structures it into pupils for instruction data and of course uh evaluation with the sample SFD. I hope you had a lot of fun learning this. Uh thanks a lot. Welcome to part seven of the LLM from scratch series. And in this video, we're going to explore the third phase of our LLM training, which is the reward modeling part. Now the idea over here is once you've pre-trained your model and then supervised fine-tuned your model to behave like an assistant, what you want is the model to be free of biases and you want to align the model to your preferred responses. So for this it's a two-part process. First things you need a reward model that will output a scalar that will depict a reward for a particular instruction prompt and a response pair. So if there's a question that was asked and there are two answers one has a higher reward than the other then that reward is more preferred than the previous one. So the core idea is that typically you will have an instruction based on which you might have response one and response two. Now based on this you might want to prefer this one over this one because this might be more neutral in tone or you're trying to calibrate your model to a certain persona. whatever your human preferences. So as per your human preferences, you have an instruction prompt and you have multiple responses against that instruction prompt. And what you're trying to do is choose one over the other. Now, this is a stage where you will typically have uh data sets that are a little north of the instruction finetuning set, little bigger than that. And here is the stage where you'll spend a lot of effort maybe hiring contractors and experts and even having advanced degrees in their fields so that they can label the data properly. Now what it serves as an outcome is that you will have a model that takes in an instruction prompt and a response and outputs a scaler depicting the score for that particular pair. So for implementation what we are going to do is that we are going to go to the route of the encoder only transformer. So while you can totally totally uh use your LLM. So let's say there's your existing LLM with the decoder only architecture stacked up. What you can do is after the layers are done maybe snip it over here and then add a classification head or uh uh a linear a linear head that only outputs a scaler. Now this is possible. What we are going to do in this video however just to give you a diverse flavor we are going to take this part of the transformer and then implement it. So this part is the encoder part. Now what this enables us to do is that this the difference between the encoder and the decoder mainly is the masking part. So uh the decoder cannot have access to the future tokens, right? So but the encoder can. So when you're looking at an instruction prompt and a response, you want to have access to the entire thing. Even if you're sitting at token one or token two, you can have look ahead. So that's why uh we will prefer implementing the encoder only architecture and then from there once the encoders are stacked up we will look at main pooling and then finally a linear head which will give us the scalar scores and then we'll back propagate couple of things we'll learn along the way so we'll learn Bradley Terry loss and we'll We will also look at margin ranking loss. We will use uh Bradley Terry loss in this but we will also look at what is we've also implemented margin ranking loss in this. So just to summarize this is what we are trying to achieve. So a prompt goes in it goes through the token embedding and the positional embedding. We are going to uh keep the implementation of the encoder only reward model to the vanilla transformer architecture. And then from there uh I'm going to attach a linear head that simply gives a score. So this is going to be our preference score. And then we'll calculate the loss as per priaryity or margin ranking and then back propagate. That's going to be our code architecture. So we'll jump onto the implementation now. So we have our tests as usual and then train rm.py PI where we are going to train our reward model for 300 steps batch size of 8 context length of 256 two layers two heads and embeddings uh Bradley terrier loss and we're going to reuse the tokenizer so you see all of the hyperparameters that we're using are highly highly aligned with the previous ones that we have seen before uh usually this might not be the case so if you're trying to uh implement an encoder only reward model you would have a much more shorter or uh much more truncated version of the original LLM uh orders of magnitude lesser because the task at hand is to preference score which literally gives out a scaler and you don't have to go through the nuances of perplexity and outputting one token at a time right so uh the best way I have seen as per the last part is to uh run it in debug mode directly so we're going to put a break point here and then propagate through it ourselves so we have our orchestrator running and then this is running for the tests. So we'll just skip it and wait till we reach our uh uh train rm.py. So we've reached our device over here and then from there we forward propagate. Now in this load preferences uh the style of collating the data is going to be very similar to the previous part. So we'll just step into it and look at it. So we've got a split. This can either be train or test. And then this time we are trying to load anthropics uh RLHF data set. Let's take a look at it. So here's anthropics RLHF data set. There are two splits train and test. And for each one there are only two columns chosen and rejected. So if I were to look at one of these, there's an entire context, the instruction and the prompt and even in some cases the history of the conversation. And then all you have to do is uh rate one over the other. So this is the chosen one. This is the rejected one. And this is as per the alignment or the preferences that you're looking for in order to build your own large language model. So some might like a more fun persona, some might want a more neutral persona, some might want to avoid political biases and so on. It's really based on human preferences. So from there we get the chosen and the rejected columns and then we append it to our preference example. So if we just run through this We've loaded our data and then we're sampling our data. So the chosen one is this and the rejected one is this. So some of the passwords won't be appropriate to show you over here. And we keep on appending the items. And this we're going to continue doing for I think hundreds of rows. And if not, we've always got a fallback. And I would prefer you use this one first and overfit and make sure that everything runs uh till the time you scale it up to the entire data set. So we've loaded our preferences. Now we skip through it fairly quickly because we have seen this implementation before. So we organize this into triples where uh the prompt will always be blank because we've chosen a data set where there is no instruction prompt and then the task and then the response. Right? So uh all of the uh the instruction prompt and even the conversation history has all been concatenated into one and that's why the prompt is going to be blank. I've intentionally kept a placeholder because if you have a data set that has a separate column called prompt, you can kind of use this as a scaffolding framework. So we get to the pair collator now. Now this is just the declaration part. So we'll take a peek at it and this will be very similar to the collator that we have seen in the previous part before. So we have uh the block size loading of the tokenizer and that's all the elements we need. we have the vocap size property. The encode will just be uh a wrapper around the encode function of our tokenizer and then in the collate function for every prompt chosen and rejected in our batch. So we are unpacking this particular tupil from the batch we are formatting our example as per the uh the prompt chosen and the prompt rejected. So if I were to look at the format example, you will see that it is the exact example from the previous part. So here I've got the formatterers.py and this will format example as per the template that you send. So as long as the example getting sent has an instruction and a response strip, we should be fine. So we format our example like so. So there's the positive text and the negative text and we format it as per our SFT template. And then uh we encode and then we take everything till the context window. So if the text spills over the context window, we'll just clip it there. And if it doesn't, we are going to pad it with our padding token, which we already know is two. So if we again go back to our tokenizer quickly, you will see that the padding token has an ID of two. So we try to pad till the block size again if it is less than the block size it will pad with twos till our block size. So we have the positive and the negative here convert it to a tensor and then we return them. So I'm going to skip over this because this is just a declaration and this is our reward model. So it has certain elements and they are very simple to understand. So we have the vocap size and the block size, the token embedding, the position embedding. Again think vanilla transformer and then there's a transformer encoder layer. Now literally the difference between this and what we've implemented in part two part three for understanding vanilla transformer is that the masked layer is missing. So this gets access to the entire attention block including the future tokens. And if you see some of the hyperparameters are also very similar to what we have seen in the past. So even if I were to open the package up, it literally says implements the original architecture described in attention is all you need. So we're going to uh reuse PyTorch's uh transformer encoder layer and then we are going to stack it up stack up one such layer over n number of layers as per however many layers I pass. So this uh we will pass as two in our case. So what this does is transformer encoder is a stack of n encoder layers. So I will have to pass the architecture of my single encoder layer and then specify how many such layers I'm looking for. There's a layer norm and then there's a linear head. Now we'll check this out during the forward propagation. So let's come back. So that's a model declaration. It's a pretty simple one. Encoder only transformer with eventually a scaler head for measuring our preferences. So we enter into the training loop and then we grab a batch. Now if we were to look at a batch, this is what a batch looks like. So it's again a list of pupils. Uh we have seen this kind of format before. So from there we get to the collate function. Now let's actually dig down deep to see how the collation happens. So we've hit the collate function in our pair collator and for every triplet we go in our prompt will always be blank uh because that's our data set. So we've got the positive text and the negative text. So here is where we have the positive text and then here is where we have the negative text. Now notice that these are uh aligned with the SFT format where I have an EOS BOS token and then from there I have the instruction as well as uh the response. Right? So from there I'm going to tokenize my positive and the negative ids. Uh we continue doing this for all the examples in our batch and then we entirely encode the positive and the negative. So if I were to see this the positive shape. So these will be each will be a sequence of tokens. So uh there will be 256 tokens as per our batch size. And then the batch size itself is 8. So our position or positive shape should be 8x 256. And the same obviously goes for the negative one as well. So this is returned and then we transfer it to our device if you're using a GPU and then we forward propagate. So let's actually check out what happens inside. So we get an 8x 256 sequence. So our batch size is 8 and our token sequence is 256. For calculating the position embeddings, I'm going to do an a range over uh 0 to t. So this will literally be a vector uh that has 256 elements starting from 0 to 255 and then from there we're going to unsqueeze it. That means adding one more dimension. So you see uh this becomes the pause uh array becomes a 2D array because I've added the unsqueeze and it's literally 0 to 255. So let's proceed now with that. I'm going to uh take the inputs, calculate the token embedding, take the a range, calculate the position embedding, and then add them together. This is also something that you've seen in a vanilla transformer. Now there's an important part here. Although we do have access to future tokens since we are going to pad each and every sequence with the pad uh with the uh pad token with a special pad token. We don't want the model to look at a pad token and make decisions. We want the model to ignore it. uh we are only adding the padding because we want to adjust all of our dimensionalities to represent so that everything represents the context window and then the matrix math can happen but we don't want the model to concentrate on the padded tokens. So that's why we make a mask and we are aware that as per our uh as per our tokenizer the padded token is actually has an ID of two. So we create a mask and this will produce a boolean where uh it will say that if we see a padded id then we turn it on we say it's true otherwise it'll be false. So if you'll see this tensor uh elements towards the ends uh are all true. So from here we forward propagate through the encoder and then we ask it explicitly that source key padding will be this. So it's aware that what the padded tokens are and it learns to ignore it while connecting or while calculating the attention scores. So we forward propagated through that which is again our encoder layer here which is a stack of transformer encoder layer a vanilla implementation of the encoder uh layer from attention is all you need paper. So we've done already uh all of this already before. So we're going to proceed. Uh then a layer normalization. So if I were to now look at H and what it is. So uh there are eight data points coming in. Each of them has a sequence of 256. Now each token itself will have an embeddings that are 128 vectors. Right? So at this stage the H should have a shape of 8x 256 x 128. So as expected we are seeing that 8 256 128 and then if we forward propagate it through our encoder the shape should not change. So that's good. If we layer normalize the shape will not change. Now from here I'm creating a mask so that I can do what is called a masked mean pool over tokens. Now if you have seen a use case in rag uh before then this might be familiar. Let me try and explain. So we've got a batch of eight coming in. So each of this can be thought of as a sequence of 256 tokens. Now each token in turn has an embedding length of 128 each. So there are 128 elements here. Now what I'm interested in doing is that I'm coming up with a pooling layer and this is again very popular for uh if you want to if you want to have multiple chunks in your retrieval augmented generation architecture and you want to uh go through mass pin pooling to represent them as a single embedding or in this case uh reward modeling where uh the entire 256 sequence can be represented by a single embedding so that we can forward would propagate more efficiently. So we have 256 such propagations. Each of them is a vector of 128. Now what I'm interested in doing is that I'm going to take away all of my masked tokens. So let's say a few tokens here and after that these are all masked. So I'm not interested in looking at them. So for the non-mass tokens I want to calculate the mean or the average. Now once I do that what I will have along this dimension I will have a single 128 embedding vector that represents this sequence. So although it was supposed to have 256 tokens, I've condensed all of that by averaging all of the embeddings out to represent them by a single embedding which has the dimensions of 128. And then from there we are going to attach a linear head to these 128 vector embeddings which will eventually output a scalar which is my preference score. So let's check that out. So I'm creating a mask where I'm flipping the bits for the padded mask. So uh in this case if a token was padded then it was true. And in this case if a token is padded it will actually be turned off or it will become false which when represented in float actually means zero. So that when I apply my mask to my age, the tokens that are padded, they automatically turn off. All right. So if I apply the mask now, so for every token, you will have a boolean which says true or false. So if I look at a mask shape, I'm looking at 8x 256x1 where it represents each token whether it is padded or not padded. And then when I multiply this matrix with our H 8x 256 x 128 uh I'm going to get or what is going to happen is I'll still get a 8x 256 x 128 but wherever I see a mask token the 128 embeddings will all be turned off and converted to zero. So if I were to look at H*S mask then you will see that whatever tokens are not the pad tokens are still visible but then whatever tokens are pad tokens are all converted to zero so that when I now take the pooling it doesn't matter uh that like these won't have any say in the calculation of the mean right so now I'm going to take the sum along dimension one because uh again if I look at the H shape this is the dimension one the 256 and what will happen is you will have a representation for a single sequence. So you will have eight representations of 128 vector embeddings each. So each sequence of 256 tokens gets compressed to one single 128 vector representation. So I'm going to go through this 8 sum and now if I look at 8 sum it should be 8 by 1 by uh my apologies. So if I look at the dimensionality of 8 sum it should be 8 by 128. So it will have 128 vector representation for each sequence in my data point. So from there uh I'm actually uh calculating the average. So I'm taking the length and for taking the length I'm going to sum up my mask. So what this does is that whatever is false gets counted as zero but whatever is true gets counted as one. So I sum it up. I will get the count of all of the non-padded tokens. So mask dots sum will give me all of the count of the non-padded tokens. And if there are no non-padded tokens, I'm just taking a minimum of one to avoid zero division error. So the length in this case, if you look at it will be 256. So the first sequence does not have any padded tokens. That's what it means. So, so if I were to look at the len then for each of these eight sequences then you will see the number of non-pattered tokens that are there. So here we go. So the pulled will be the h sum divided by the length. So we're normalizing taking the actual mean. So the pool now will be 8 by 128 but this time normalized by the length of the non-padded tokens. So from there I've got a head which is nothing but a linear head that takes in an n embedding which are in our case is 128 and then gives out one single scaler which represents the u preference score. So we've got R over here. So you see R will be eight scalers representing the preference scores for each of the sequences in my batch. So I'll return from there. So we do this for R positive. The same goes for R negative. And then uh we do another forward prop for the rest of them. So we might want to come back and take stock of where we are. So we want to come back to our uh training loop. And now we'll see that uh the R positive and the R negative for a batch has been set. Now in an ideal world, what you want to do with these preference score is basically compare them. and you want your positive uh example to have a higher preference score than your negative example. So this is higher than this. So we're good. This is also higher than this. So we are also good here. And you see whenever we won't have u this particular situation where the ROS is greater than the R negs. So for understanding that let's go through the Bradley Terry loss. Now here what I'm interested in doing is I'm taking the difference of R positive and R negative. Now if it is the case that uh R poss is greater than R neg then this diff will be positive and then I'm negating a positive that means this will be negative and soft applies this particular function which has this curve and you can see that for a negative number the output tends towards zero. So it's a smoother uh function compared to your relu. Uh and you can see that if the positive is greater than my negative. So let's come back and study this carefully. If the positive is greater than my negative, it makes the difference a positive number and I'm passing minus div. So I'm passing a negative number and negative number means the loss will be zero. And if the margin between the positive and negative is further above minus2 then it will be a hard zero otherwise it will have some loadings over here. So I'm trying to incentivize my model to have a large margin classifier and have a lot of score difference between the positive and negative. So that's Bradley Terry loss. Uh now there's another loss called margin ranking loss where we have uh so we are passing it over here the r positive and the r negative we know for a fact that we are passing the r positive first so we set the y equals 1. So whenever the first score has to be greater than the second score the uh the y has to be one or it will be flipped if um the opposite is the case. Now the margin ranking loss is governed by a simple equation. So we take the max of zero and minus y * x1 - x2 plus a margin. Now the margin is optional. It can have a value of zero. In fact, uh the default is zero. Now let's look at what happens here. So uh since we are expecting that x1 should be greater than x2. So our r pause should be greater than our r neg. We can assume that y is a one. So if y is 1 and this yields a positive number then this becomes negative and the maximum of 0 and a negative number is actually zero. So our loss is zero. Now if it were flipped if x1 has a lesser score than x2 this will be a negative number and the negative will cancel out with this negative. So this whole expression will become positive and then that will be the output of the loss resulting in a higher loss. So that's the way it's structured. So whichever loss you want to look at, you want to take the positive loss and uh positive uh reward score and make sure it is greater than the negative score and you want to have a loss function designed to incentivize that behavior. So in this case, we're going to uh let's really go into it and check it out. So we've got our R pause tenses for the batch of eight and the RNET tenses for the batch of eight. And then from there we step in we've got our diff. So uh in this case whenever it's negative when the diff is negative we will have uh a loss calculated. So soft plus mean uh we are actually uh taking all of the calculations of the losses. So we should probably hand calculate it on our own to check out what is going on. And you can see that uh whenever the difference is uh large we actually incur uh quite uh a loss. So for example minus.1101 yields a loss of 74 but then smaller differences also incur some loss because of the soft plus. So if the difference had been large enough then we would have incurred zero loss. This is why we're using soft plus so that we can use a large margin classifier. So from there uh backward and then back propagate and then we print out everything and this should take care of our entire training loop. So once we have our training done let's actually evaluate what happens in the eval. And it's pretty simple. So we'll let our uh training continue. So the training has started for us. So every 25 steps I'm logging the loss and you will see that um it is jittery but overall it has a trend of uh learning. So we've gone through our training loop and now we are in the evaluation loop and we're going through our device. The load preferences this time will be dictated by the split that we got which in our case uh we've got two evaluations. So one for train one for test. So this is going to be our train val loads all of my examples. So we've got the triples over here. The collator this time I'm loading the checkpoint and then loading the state dictionary putting it in eval mode. Doing it for batch of 16. So I'm collating my positive and negative and then from there forward propagating and then whenever the R poses greater than R neg I'm assuming it's a correct example. So I'm counting my totals and then doing it for however many batches I have. In this case I only had one batch because the batch size is 16 and I only have eight examples. And the accuracy is that we are correct six times out of eight which is nice. So we print it. Then we uh look at our terminal. Then we'll see that. Okay. This is I think the second valve. So we'll just let this continue. And all of our checks are complete. So for the unknown pairs we are at an accuracy of 50%. For the known pairs we are in accuracy of 75%. Now if I were to show you this as one full flow then things would start making sense. So we've got the test for the BT loss and the reward forward. I'm training. So here it will go through the different steps. Uh we're only training on 16 data points I think. So we can't expect much and the loss will be jittery. But in in general the reward model piece is uh again anything to do with the RLHF the reward model or uh the PO part is going to be sort of very tricky to uh converge. But we can't judge the numbers based on this because again only 16 examples two-layer neural network. Um but again if you take this and apply it on a larger data set things would start making sense. So you see that while our training accuracy is uh 75%. Uh this is even worse than a coin toss. So didn't really learn anything but uh hopefully with scaling up the number of layers and the number of examples a lot of things will happen. So this brings us to the end of part seven where uh we went through preference data sets, the reward model architecture, the loss function and uh we really went into the debug mode for all of the for analyzing all of the shapes. Uh hopefully you had fun learning this one. Thanks a lot. Hello and welcome to the last part of LLM from scratch. And in this part uh we're going to cover reinforcement learning specifically the proximal policy optimization technique. If you've made it this far, congratulations. Uh there's just one more part to go and then you'll be completely familiar with all the phases of a production grade LLM and how they are trained in the real world. Now before we start uh there are some terms we need to get out of the way and in no means is this going to be an exhaustive lesson on reinforcement learning and its corresponding theories. Uh but we'll have to know enough about these terms in order to go implement our PO. Uh the whole uh subject of reinforcement learning is pretty vast in theory. So we will barely be able to scratch the surface but uh of course as and uh as and when these terms spin off uh I'm sure there'll be further reading to do in this area that is left as an exercise won't be covered as part of this uh video scope. So the first things first uh I want you to be fluent with some terms we use throughout the video. So the state now in the context of LLMs the state is the current state that you're in the current text context. So let's say you come in with a prompt and then you start the LLM inference or the generation the prompt itself becomes the state. So if you have 30 tokens coming in as the prompt, those 30 tokens become the state. And then when you generate another token, that new token gets added to the existing 30 tokens. And now the 31 tokens go at go as input to the next step. So the second state of the LLM becomes those 31 tokens. So the state will change over time. Now the action is the actual action of the model predicting the next token. So this will be at the 30th step once the prompt comes in. When I predict the next token, it's called the action. Now at any given stage, I will have as per our vocap size 8,000 options to select a particular token. So every action associated with the state will be considered when we are trying to do proximal policy optimization. So it's a specific context and the specific next token chosen in it. Now the policy is literally the LLM that we will build and update to align it to human preferences. So this is going to be the actual decoder only LLM that we have built so far. Now the value is the predicted total future rewards that I will get given a particular state. Uh reward is a scalar score from literally coming from the reward model. So this will be a proxy to the human preference or the human score that uh we'll get given a prompt and a response pair. So now let's say uh there are there are a few tokens going in as input and then I predict the next token and the next and this constitutes uh my prompt and response. Now we will receive a reward here at the end of the generation by the reward model. But at this stage the value of uh this particular state and action or this particular state will be the estimated future rewards that I'm going to get. So at this stage we will try and estimate what reward we might receive if we continue this generation using this policy. So it's going to be a future prediction or an estimation of my cumulative rewards. Now we know for a fact that we only get a reward at the end of the answer or at the end of the generation. So we'll try and estimate that through the value. Now this in our implementation will literally be one more node to our LLM. So our policy will now have two components. the LLM that does the actual generation plus a value head that is a scaler that literally predicts value. Now the returns is the discounted sum of future rewards for the rest of the generation. So simply put if I complete a particular generation the rewards that I get at the end of it or at the end of a batch can be summed up and can be uh considered as the total returns. So uh our value or the estimated prediction of the total rewards will have will have to be as close to the returns as possible. So this is where uh we'll use the value and the returns uh to calculate the value loss. So we'll watch all of this in action. The Q value is the expected return if we emit a token at that point. So again uh given any point I will have 8,000 options and I can uh I can consider the fact that given these 30 tokens what if I generate token one what is the Q value there what if I generate token 2 what is the Q value there and so on we'll obviously not be doing all of these calculations and it'll all be an estimation that we'll see in a minute now there's a new term advantage that is simply going to be the Q value minus the estimated total value at that state now the KL diver is a general term. Uh if you're familiar with stable diffusion or uh variable autoenccoder, you might have seen this. Uh but in general, if there are two probability distributions, then it measures how divergent these two distributions are. Uh then finally we have uh the policy and the reference. Now these will be literally two LLMs. So the policy LLM and the reference uh LLM will start at the same point which is the frozen SFT weights that we uh we will import from part 6 and then from there it will go through a reinforcement learning iteration and the policy will keep getting updated and the reference will be frozen in time. So what will happen is after every update step I will actually measure the scale divergence between my policy and my reference model and I might actually even take some actions based on how far apart these two probability distributions are. uh what reinforcement learning does to the LLM is that it aligns it to the human preference and in pursuit of doing that it may diverge far away from the SFT fine-tuned stage. So scale divergence is a measure to number one track it and then number two using that we may clip our gradients we may clip our updates to reflect uh to reflect stability and to achieve better convergence to not deviate a lot from the SFT uh to not have what is called catastrophic forgetting. So uh with that quick set of definitions we'll jump on to the code uh we'll keep revisiting these definitions as and when required so that uh you can keep remembering and uh all of these terms have a high recall in your head. So we have a train and an evaluator. We'll start with the train. So as always we'll start with the debug mode so that we can actually visualize our inputs going through uh the various pieces of code. So here uh we have our different arguments uh very generic the top ones especially. So the policy checkpoint will be our SFT frozen weights. The reward checkpoint will be our reward model that we trained on the last part. The number of steps the batch size block size and a couple of other new ones that we'll explore in a minute. So we go through the device. The RLHF tokenizer is literally the BP tokenizer that we have been using throughout this exercise. So nothing needs to be explored here. So we'll take a quick peek. It is literally if you pass a bite pair encoding directory, it loads the tokenizer and we will reuse the tokenizer from the previous parts. So here is where uh we load and then we have now the policy LLM. So if we go back to our definition, this is going to be our policy LLM that will start from the SFT frozen weights and then we'll keep updating it. So if I take a look at it, it literally has two components. The actual LLM that we have seen before and then there's a value head that will measure the estimated future rewards given a state and action for that generation. So at every token generated we will have an estimate for the value and the loadits coming in from the LLM. So once we again see this in action things will be clearer. So this is my policy and I load the state dictionary from the SFD frozen weights. I do this exact same with my reference. Uh this uh will have frozen weights. So I'm immediately setting requires grat false and putting it in eval mode. This will never change throughout the exercise. This is just a baseline for me to compare how much I deviated uh from uh the SFT frozen weights. Then I'll run till this line to avoid the for loop. So I put it on eval mode. Then I proceed. So in this block I'm loading my reward model and this is coming in from the checkpoint of uh part 7. So this if you remember is an encoder only uh model that uh gives a scaler given a prompt and a response pair of what it thinks should be scored by the human the optimizer. And then here it's an all too familiar sampling of prompts that we're doing this time from the alpaca data set. This is the same data set that we had used for our supervised fine-tuning where we have the instruction and the input. We concatenate them together and there is a response associated with it. So we only need uh to sample 16 prompts from here in order to do a mini batch training. So we'll proceed. All right. So we've gotten our prompts. Let's take a quick peek at it. Now what I've done is I've only plucked out the corresponding prompts so that I can run the generation against these. So uh I don't have the answers with me right now over here. So I've got the different questions which are the different uh combinations of instructions and the input for a particular batch. Now in this case I've set a batch size uh for four and we'll track it throughout the exercise. So I step into my training loop immediately. Uh feels weird to again uh go through a bunch of terms and directly jump onto the training loop but I will unpack each and every one of these terms in context as I go along. So I've got my batch prompts over here. uh I'm checking for the length and uh if it is less than the bat size then I'm immediately uh just multiplying the number of prompts that I have. So in this case I do have four prompts so it did not go into the if condition. Now I'm going for a format prompt only by removing and after that I'm removing the uh EOS token. So this will be ready for generation. So if I look at texts now, it will be exactly my prompts but this time they will be formatted with my SFT uh format. So I've got the instruction and I've got the placeholder for the response and now I'll be running my LLM for the generation. So here I'm literally in a for loop for each of the input prompt that comes in. I'm doing a policy generate and appending all of it together in one single batch. So I'll run it till here. And now we've got four pieces of generation. So let's actually check them out. So I'm here in my debug console and out ids contain all of the token ids that contain the prompt along with their uh response that is generated by our policy lm. So if I look at out ids the first one or rather this will have four elements. Let's quickly confirm that because our uh batch size is four. So this has four elements. Now if I pluck out the first element and look at it I will see a list of tokens. And now using our tokenizer I can actually decode it. And you will see that this contains the instruction and this time the filled out response. So if I do this for a couple more in the batch then you'll see that what are the primary colors? Three primary colors are red, blue and yellow. Now these are the generations that we have seen before in our SFD stage. So they're aligned as per that. So we do a first cut model prediction here. Now in the next stage I will be uh generating my reward for each of these generations. So let's go ahead and explore that. Now in my batch prompts I've got my prompt and now in my out ids I've got the prompt plus the llm response. So I'm going to separate them out by creating a boundary. So given the prompt I know how many tokens constituted my prompt and if I subtract that from the out ids I will get how many response tokens I got and then just decode those response tokens and then compute the reward given the prompt and the response text separated. So let me quickly run these couple of lines. So this full contains the entire response and then from there the prompt ids contain just the prompt and the boundary is the length of the prompt ids. So if I go from boundary onwards and slice the full array I will get my corresponding response ids. And if I decode just that I will get my response text. So if I were to look at it, you will see that response text only contains the answer to our uh query. So in goes the prompt and the response text and uh out comes a reward scaler. So let's actually uh go inside this particular function compute reward and see what happens. So in this case I'm dynamically importing the uh part six formatterers and then formatting my prompt and response uh so that it again adheres to the uh instruction fine-tuned or the SFT format. So the prompt here is the raw prompt and the response text here is the actual response that came in. So I'll have to format them again in the form of here. So they will have again the instructions and the response hashtag along with the uh VOS and EOS tokens respectively. So now I go and encode them and with this encoded pair of prompt and response formatted I can actually forward propagate through my reward model. So this reward model if we've seen this before uh let's quickly get into it and then check. So in comes uh a particular sequence of tokens. So the batch size is one because we're doing this one at a time. Uh we have 99 such tokens. So we have the position embedding here. Uh it calculates the age the pad tokens if any and then it goes through the mast mean pooling that we have covered in the uh previous part. And then finally the R head comes out where R is going to be a scalar and this is a reward for this particular 10 response. So we'll come back and we have computed the reward and we're going to plug this out from the tensor and return it as a float. So if we see R scaler now it'll just be a floatingoint number that represents the reward. Now in my data list I'm appending the full set of tokens or token ids the boundary so that I know from where to split my full response and the reward associated with it. So if I run this whole loop for the batch of four that I have I will look at data and then I will see that they are a pupil of three elements. The first element is the entire prompt plus response token ids. The second element is the boundary beyond which the response tokens lie. And the third element is the reward. So I've got them organized like so. And then we move. Now in this section we'll be doing a lot of list slicing gymnastics in order to make sure we honor the context length. Let me try and explain that with an example. So let's say we have a 256 uh context window. So this is going to be our context window. Now, let's say I've got 250 tokens as the prompt and then the response has 10 tokens. Now, if I'm going to put them together, then I'm actually going to get 260 tokens, which is exceeding my context length. So what I need to do now is I need to actually drop four tokens and then readjust my boundary. So uh the boundary here would have been 250 and then the tail end of the 10 tokens would have been my response set of tokens. But since this is exceeding my context window, I will actually have to clip this so that I'm only left with 240 prompt tokens. And I have all of my 10 response tokens. Uh my apologies. So I have all my 246 prompt tokens and I have my 10 response tokens. Put them together. I still honor my context window. So the idea is that uh you don't want to snip the response tokens by any stretch of the imagination. Those uh extra tokens that need to be evicted will go away from the prompt tokens. So if I quickly go through this, the policy context here is actually 256. So you see this is 256 and then from there uh I'm taking the minimum of u the number of elements that are there in this particular batch's uh first token and then the policy context. So this is the max length that I'm going to see in this particular batch. So my max length is going to be 159. So the idea over here is in this batch of four sequences 159 is the highest sequence that was seen. If anything beyond that uh the way we saw in our example of the 260 tokens, it will be clipped to 256 tokens only. We've got our batch. We've got our uh sequence mask, last index, and rewards initialization. We'll uh look at how these get filled up in a minute. So, first things first, for the first prompt going in, I've got all of my tokens over here. So, this will constitute the prompt and the response. So, in our example that we saw, this can potentially be beyond 256, can touch 260, 270, whatever. Now here we are taking the Lulful and the max length. Now I know for a fact that uh the max length that I saw in this batch was 159. So this will always be a minimum of the number of tokens that I'm seeing in this particular example and the max length. So L in this case will be 99 only. Now from there I do not need to drop any tokens because L full is 99 and L is 99. So drop becomes zero and I'm going to take the max of uh again 0 and 0. So I'll be left with a zero. So this essentially depicts that since the prompt and the response combination is already part of my 256 context window, I do not need to evict any tokens. If this were the case that it exceeded the context window, then uh the drop will be more than zero and we would have to evict tokens eventually. So the sequence is the actual sequence that was generated uh by the model and that also includes the prompt. So again if I look at the sequence it's a bunch of zeros right now since it was initialized like so. And since we are going through the first loop the first element of the sequence vector or sequence matrix will be filled up. So this will be filled up by everything that I've seen so far including the response which ends at the EOS token and then from there everything else will be padded uh to two. So this is my pad token and everything else is padded to two. So if I take a look at my sequence matrix once again I will see that everything has been padded to two. So from there I'm creating a mask where the modified boundary if my context window was exceeded or if not this B still acts as a generic term for the boundary. So from here everything that was supposed to be part of the response becomes true and then everything that was not supposed to be part of the response becomes false. So the response becomes true. Now everything to the left of it was the incoming prompt which I don't want to track uh when I'm measuring rewards or I'm when I'm measuring values or any other metric that I'll measure. I'll measure it against the generated tokens uh in the LLM and not the incoming tokens that were fed as prompt to the LLM. So u everything to the right of the response will be the padded tokens which also will be turned off. So again if I look at the first element of mask then all of these truths are the response token of the first sequence in my batch. That's what it means. So everything to the right are padded tokens. Everything to the left are prompt tokens. And then we move like so for the rest of uh for the rest of the batch. And then if we relook at everything else. So we'll clear the console. And then now if I relook at my sequence, everything will be filled up. So this will include the prompt and the response. And then a bunch of padded tokens. And you will see that the last example in this batch had 159 tokens. So it does not have a pad token. And then from there if you look at the mask then everything that is true here was part of the response tokens and everything that is false on the left was the prompt. Everything that is false on the right are the pallet tokens. So this is the way it's structured and now I will use all of this to eventually uh do further processing. I will now need to calculate my log probabilities. So for doing that I need the policy model and the sequence of tokens. So let's go inside and check this out. Now why do we need log probabilities? Now if you look at the definition we've pinned down before the scale divergence measures the divergence between two probability distributions. Now what we will do eventually is we'll keep updating the policy model and we'll freeze the reference model and then we'll need to measure the difference or the divergence between the policy and the reference model in order to gauge how much deviation we are going through so that we can either clip it or we can keep track of how much we are diverging from the SFD weights. Now in order to do that a way to calculate this is to simply have the ratio of the log of probabilities. So uh you have the log props of the policy model and you have the log props of the reference model and then you simply take a ratio. Now we know for a fact that log has a very interesting property. So log P by log Q is simply log P minus log Q. And we're going to leverage this property as we go along. So we go inside calculating our log probabilities. And there's some gy gymnastics to uh kind of working this out as well. Uh I really want you to follow this carefully. Uh if required, freeze the video. If required, rewind and play back. It's essential that you understand this. It's not exactly a difficult concept, but it's just that it goes a lot of back and forth with the tokens and the positions. So, it might be a little involved. So, I get my input sequence. So, in this case, if I check out X.shape, I get my batch of 159 tokens uh with the prompt and the response and the padded tokens all filled up. Now I'm going to forward propagate through my language model. So again if you remember the model is actually a policy with value model which has the LM part which is the large language model and the value head. Right? So we'll come back to this. So I'm going to forward propagate this. And what happens here? So the logits will be again as the LLM goes if I am passing 159 tokens for each of these 159 positions I'm going to get 159 elements back and each element will have a vocap size of 8,000 uh elements which will signify the loget for each token in our vocabulary. So I'm expecting the shape of this loits to be 4 + 159 cross 8,000. So let us quickly confirm this. Uh so we're good here. So a batch of four 159 tokens each and 8,000 potential token logets at each position. Now in order to calculate the log probabilities, I will need labels associated with it. And as usual, we have been doing semi-supervised learning by treating the next token as our labels itself. So we're going to shift our labels to the right. Now if you look at the implementation of shift labels, it's literally uh starting from the second element to the rest of the element. So my labels will start from the second token all the way till the end. So if I have 159 tokens going in, the labels will fetch me 158 tokens. So those are the right shifted tokens. So if I propagate this then my labels will be 4 + 158. Now where did this 8,000 go? If you look at this carefully, we plucked out all of the elements till the last element. So let me sort of try and explain this with an example. So let's say I have a sequence of token one, token two and token three. Let's add one more. So this is the logits. Now uh for the labels part I'm going to right shift everything. So here I'll have t2, t3 and t4. That's it. Right? Okay. So it has three elements only because I took labels first was the logits starting from the second element. And then the reason why here I have 8,000 candidates, here I have 8,000 and here I only have one is because if we come back and see this then uh I'm actually doing a gather log props which does a lockpather. So we've got the documentation of torch.gather here. It takes in an input the dimensionality and the index and it will actually act upon the index on that dimensionality for that tensor. So I've got a tensor here just as an example. So I've got 1 2 3 4 it's a matrix and then I want to act along dimension 1 and then I want to provide these indices. So 0 comma 0 for the first one will ensure that the output becomes 1 comma 1 and then 1 comma 0 for the second uh for the second position becomes reverse 4 3. So this is how we can use gather to actually reshape a tensor as per how we want it. Now let's check out how we are going to use this in our use case. So I'll try and pull it up and show you with an actual example. So if I take this one for example and take the shape now this will be be 4 + 158 + 8,000. So while the loit shape was 4 + 159 + 8,000 this will be 4 + 158 + 8,000. Since we're letting go of the last element, as we've already seen before, that's not relevant to the generation when you right shift your labels. So, we've got 4 + 158 + 8,000 over here that goes in to the uh to the gather. Now, this loits are then uh then undergo a log softmax. So, this literally gives me the log probabilities. So if I were to get inside this function, I'm literally getting my log probabilities over here. So this log P will also be a 4 + 158 + 8,000 but the values will be different. So if I were to look at the previous ones, these are raw ojits. And if I were to look at log B, these will be the log probabilities. And you'll see that uh they're different in value, right? So this is the log of the probabilities. So we first apply a soft max then apply a logarithm and torch has a function that does it in one step. So we go do that. Now this log B I'm interested in getting the log probability of a particular position as per my label. So let me try and explain that. So if I were to look at my lock B or rather the shape of my lock B, this has uh a batch size of four, 158 tokens each and 8,000 vocabulary. Now let us grab the first batch. So if I grab the first sequence rather then I have 158 tokens with 8,000 candidates at each position. Now let us grab the first element from there. So I will have the 8,000 candidates on the first positions. Now the log probability that I want to pick up. So if you see this, there are 8,000 elements or 8,000 candidate log probabilities. I need to pick one. So for each token position, I need to pick up one value for the log probability. And that value will be provided by the token ID of the label or of the right shifted label. So now if I go look at my label, it is 4 + 158. Now if I pluck out the first sequence then it will be 158 tokens. Now at each token each value will represent the right shifted token ID. Now at this position 203 there will be a log probability present in our log P tensor. Right? So now if I go look at the log P, pluck out the first sequence, pluck out the first token and then pluck out the 203rd position of the log probabilities inside those 8,000 then I will get this value. Now this is the value that I want to treat as my log probability for that token position. So this is again uh you can keep rewinding it. uh when I read it initially, it was a little confusing to me. Uh took a little time to wrap this in my head. So feel free to pause and rewind to really nail this. So I'll repeat what I said. So the position that the label contains is the log probability position you want to pluck out of your set of log base. So log B we take and we can go through another sequence. Let's say we pluck out the second sequence and then from there we pluck out the fifth token and we have 8,000 candidates over here. Now what I'm going to do is that I'm going to look at labels for the second sequence and for the fifth position. Now this says 297. So my log P over here will simply become the 297th position out of my 8,000 candidates. So for this it will become - 255.1141. So let's actually run this gather and then let's confirm for this example that we have seen so far. So I've got my log P over here. So if I look at the shape of LO P, it will be 4 + 158. Now if I take my LP for the first sequence and for the first position I will see that it is - 48.3569 which is exactly what we had seen when we had hand calculated the log P to align with my labels. So this is and let us confirm this for the second one. So let's do it for the second sequence and the fifth position and you will see that the value over here is -25.1141 which is exactly how we had hand calculated this. So this is how I'm going to calculate my log probabilities. I'm going to take my prompt and response input tensor. I'm going to encode it. I'm going to get the loits. Calculate the lofts of max. get my labels through right shifting and then use the gather function to use the label positions to pluck out the relevant log probability values at that particular token. So from there I've got my log probabilities of my policy and for all intents and purposes this is the first training step that is running and since our policy and reference starting point is literally the same set of weights for the first iteration this will exactly be the same and the deviation will happen in subsequent steps as we optimize. So we've got our policy log probabilities and our reference log probabilities. Now the next thing and we are all set for calculating the KL divergence by the way. So this is good news. So uh if I were to revise all of what we have seen uh we took all of our prompts made that into a batch. um did a lot of list slicing gymnastics to make sure we don't exceed the context window and then from there we've calculated the lock probabilities and now we are interested in calculating the values. So again a quick refresher. Uh the values is a simple linear head that spits out a scalar that represents the potential future reward that I will get given I'm following this policy for the foreseeable future till I hit the end state. So I'll come back here and this time I'll run my sequence again through my policy and I'm only interested in looking at the values. So now if I take a look at my values I will get a 159 values and of course my batch size is four. So this will have four cross 159. Uh the value will be at a token level. So at each token we assign a value that what is the estimate of the future rewards. What is the estimate of the cumulative feature future rewards at that particular position in that token. So once we do this we'll have to align it with our shape of our log probabilities. So uh since we right shifted our labels we've got only 158 positions there. So all I need to do is take all of the values and then uh ignore the end. So I'll do that and my values becomes 4 + 158. So let's go from there. Now here I'm calculating a bunch of stuff. Let us go through it. Now when you say I want to measure the difference between log probabilities to calculate KL divergence. when you say that I want to calculate my values, I do not want to do it for the entire sequence. I'm only interested in doing it for the response tokens. So I don't want to do uh everything to the left of it which is the prompt and everything to the right of it which is the padded tokens because the value or the reward or everything that we are measuring will be against the generation or the response from the LLM. The prompt was already provided as an input. So there's no point of measuring anything there, right? So we are creating a mask where every response token will be true and everything else will be false. Now if you look at it, we've created this mask here before. So if I were to relook at it, for every sequence, all the response tokens are true. Everything else is false. And now I need to align my mask tokens against the 158 shape instead of 159. So again if you check it's 4 + 159. I need to align it uh with this so that I can only measure the actions that I can measure my lock probabilities against. So once I execute this the action mask actually becomes 4 + 158. Now something interesting happens over here. So I'm going to take my policy lock probabilities and mind you these are going to be 4 + 158. So at each token position I have uh a log probability assigned to it. Now when I apply this action mask uh this is a boolean matrix. So when a boolean matrix is applied to a matrix that has floats, what it does is it plucks out all of the positions where the action mask is true and ignores all of the rest. So if we look at our action mask, there are some trs, there are some false. Now let's actually measure the sum. And we've got 67 prompt responses on the first sequence. 19 prom uh 19 response tokens on the second, 111 response tokens on the third and 128 response tokens um at the end of the sequence and this is our batch. Now if I take the sum of this entire batch, I have a total of 325 response tokens. So this entire batch has 325 response tokens and I'm only interested in analyzing the log probabilities for these 325 response tokens. Everything else can be ignored. So I'm uh filtering my log probabilities for the policy and reference and let me run them. So the old LP will become oops will become 325 elements and so will the reference lock probability and these since uh we are running this for the first time will be exactly the same. So if you check uh - 2.0981 minus 2.0548 0548. Now this compared to the reference will be exactly the same uh as we haven't uh done any updates to our policy yet. So these will be the exact same for the first step. Now I'm also doing the same thing to my values. Again I'm only interested in those 325 response tokens for the entire batch. So here are my values. Uh oops. So this will be the old values shape which will be 325. All right. So now just like we saw the log trick where a ratio of logs can be taken as the difference since we have the log probabilities already we can measure the approximate KL as the difference in the logarithms. So we do that and here is what we got. So we will get KL as 325 elements. All of them are zero right now because uh we haven't made an update step yet. And literally the log probabilities of the policy model and the reference model are the same. Now we now look at the rewards. So we take all of our rewards which is again 4 + 159. So we will have to take the second element onwards and if you notice we only receive a reward towards the end of our generation sequence. So if I look at this rewards all of it taken from the first element onwards you will see a ton of zeros. Now for this first sequence I've got a reward here which denotes the end of sequence for that uh for this uh element and then from there I will have an end of sequence here here and here. So we've got a position here got a position here and I'm unable to find the one for the second one and we've got a position here. There we go. So we only have one value for the reward uh at the position or where my generation ended at each sequence. So I'm taking that and again applying my action mask on top of it. So I'm only interested again in those uh 325 elements. So now I have a 325 elements and I will have all of my rewards, all of my four rewards as part of these 325 elements. And then from there I'm actually subtracting the kale divergence with the help of a coefficient as well. And this keeps me in check so that I don't drift far apart from my reference model. So the policy model and the reference model their difference in log probability is the amount of divergence that they're going through. I'm penalizing my rewards for deviating too much. So this eventually when we incorporate as part of the loss and then we back propagate this will ensure that uh again there's stability in the training and there's uh we we are trying to avoid catastrophic forgetting. So the shaped r literally becomes my returns. If we go back to our uh definition then the returns is the discounted sum of future rewards for the rest of the generation. So uh we can see here that we have all of the rewards for that particular batch. So uh and then we uh kind of subtract uh a penalty term using the kale divergence and then the shape literally becomes the discounted rate or uh the discounted set of rewards for the entire batch. So this will have again 325 elements where the rewards will be discounted by the KL divergence. Now in this case the KL divergence is zero. So nothing got subtracted. So all of my reward terms are as is. All of my four reward terms in my batch are asis. So I've got an element here, here, here, and here. So I've got four positions. Rest of them is zero. Now if I go back to my definition of advantage, it is how much better that token is than the model's average continuation in this context which is measured by the returns minus the old values. The Q minus the V. Now the value was the estimation or the estimation of my cumulative future rewards whereas the returns are the actual rewards that I've gathered. So their difference will give me the advantage. Now I'm also normalizing advantage here again for stability. So we'll subtract the mean and divide it by the standard deviation and I've just kept this term so that we can avoid zero division error. So we have our advantage. Now if you take a quick look then we know what the state is. It's the prompt. The action is generation of the new token. Uh the policy is the LLM. The reference uh will be the frozen set that will always remain constant. We will never update that. The value of a state is the future rewards that we are calculating and we are estimating this through a linear head in our LLM. So we added another linear head in order to measure the values and again we will have a value loss and this will back propagate and our values estimation will get better over time. That's the expectation. The reward is literally coming from the reward model. Uh the returns we've already seen is the actual hard rewards that you receive at the end of a batch generation sequence. Uh we continue uh measuring our advantage. We now know what a kale divergence is and we've also seen how to measure it by using the log probabilities. So a lot of terms uh were thrown at you in the beginning of the video. Hopefully now it's all unpacking and coming together and then once we look at the PO loss everything will come together in one seamless way. So we've calculated our returns we've calculated our advantage and normalized it. Now we are going to uh try and look at the update step. So this is where the actual uh policy weights are updated and this is one training loop. Now in a typical PO or in a standard PPO, this highlighted block of code actually runs for multiple iterations for the same batch. So literally just think running a for loop over it so that this happens multiple times. Now for simplicity sake, we are only doing a single pass, right? So we set our policy model to train and then again uh if you go through multiple iterations of these in a for loop. So if you just do this block multiple times then the policy uh will be different for every iteration in the loop. But for the first time the logits new will be exactly equal to the logits that we have calculated previously. the values new will be exactly equal to the values that we've calculated previously. Again, keep in mind that here is where the actual policy weights update happens. So everything before that will be exactly the same. So think of this running in a for loop and in the first iteration this won't matter. This will just be equal to the previous logits that we have seen. So we grab the logets, we do a softmax, we grab the labels and then we do a log p full gather. This is the exact same operation that we do when we had tried to calculate our uh log props. So we're doing a gather and then plucking out the log probabilities of the position specified in the label matrix. Right? Then uh we apply our action mask and then we get our new set of 325 values. So in this case again since this is only a single pass everything here will be the same as the previous one but eventually you should uh learn this or uh eventually run this in a loop of four or eight iterations so that this goes through incremental updates on its own. Right? So we finally come to the PO losses. The calculation of the PO losses where uh we pass the new lo P, the old lo P, the advantage, the new values, the old values, the returns, and then we've got a couple of coefficients here that uh we'll watch once we enter this. So again, everything that we have run so far, I hope it all makes sense. So let's go inside how the PO losses are calculated. So first things first, we take the new lo P minus the old LO P and the exponent of it. Right? So again when we run this in a loop, uh the new lock P will be different than the old lock P. Since we are doing a single forward pass or a single pass over our PPO, these will be exactly be the same. So this will be a zero. Anything to the power of 0 is one. So the ratio will be one. So if you check there we go. So the ratio is a tensor of ones which has again 325 elements. Now uh I next calculate my ratio times the advantage unclipped. And then I'm actually clamping them. So uh if you look at torch.clamp clamp then we provide the input the minimum and the maximum under which everything will be clipped. So we see here that the clip ratio is actually 02. So this ratio should be clipped between8 and 1.2. So uh again uh since the ratio here is pretty much one. So we've got a 325 ones over here. So nothing gets clipped. The clipped and the unclipped are pretty much the same. Uh we take the minimum of these and then we do a mean and we store the negative of that as the policy loss. Again for the first pass uh this will be just the ratio times the uh advantage and nothing will get changed. So if you look at the policy loss you will see that it's a single tensor right since we've gotten the mean. So this will be our policy loss and then from there the values that I tried to estimate that would have been my cumulative sum of the future rewards and the returns that I actually got should be very close to each other. So I'm measuring the MSE loss here and calling it the value loss. So we go through it the value loss. Now the entropy is another interesting element over here. Now the entropy specifies or rather uh it's a term that penalizes a lot of confidence or a lot of deterministic behavior. Let me try and explain this with an example. So I'm taking the log P and taking the mean of it. So if my LLM is hyperconfident, all of my probabilities will be one. So let's say we're doing a three token generation and all of my probabilities will be one. What is the log of one? It's a zero. So the entropy here will become zero. Which essentially means nothing will get added to the final loss. So I'm calculating the total loss over here and adding the entropy to it. And if my model is hyperconident, nothing will be added. But if my model is diffusing tokens, well, let's say for three token generation, it has uh these confidences each respectively. Um, mind that they don't have to sum up to one, of course. So, I can make them a little random. And if I take the log of this for example. So if I take the ln of 43 then I get a minus.84. Now when you get a minus.84 and put a minus sign there it becomes a plus.84. So a positive term gets subtracted from the loss. So I'll repeat this. If your probabilities or your confidences are lower then the entropy is higher and it's a positive number which then gets subtracted from the loss. So what the entropy is trying to do is promoting exploration. So in reinforcement learning the two things that you need to balance out is exploration and exploitation. If you become too deterministic, you might get stuck at a local minima and that goes for more exploitation than exploration. The entropy term is trying to balance that out. So we are not using the entropy term here. We are zeroing uh zeroing it out. But it's a fun exercise to set it some value and see how it goes. So from there uh I take my difference of the logs uh the log probabilities and take the mean to get my final kale divergence. So this will be one particular term which will be the mean. So again uh since my uh old and new are literally the same policies. U this time I'll get zero but if you run it in a loop as a standard PPU does you will get different values from the second iteration onwards. So my total loss is the policy loss plus the value loss and then I'm able to control it with a hyperparameter the VF coefficient and the entropy. I'm again able to control it with an entropy coefficient that constitutes my total loss which then is outputed with the help of a data class that I have defined over here. So we have the policy loss, value loss, entropy, approx and of course the amalgamation of all of these terms into a final total loss. So we go from there. This becomes our total loss and then we back propagate and then from here now my policy model has undergone a back propagation. So it's weights have changed. So the policy model has literally now become a new large language model that is different from the reference model that we had frozen in the previous step. So I will be in a position to uh measure the log probabilities now for the policy and actually compare them with the reference and then at every 10 steps I'm actually printing out uh the different metrics that I've measured with the overall loss and this should hopefully suffice in our training workflow. So without further ado, what we will do is stop this entire sequence and then go straight for the training. So in our orchestrator, we've got a training run over here with the bat size four and all of the hyperparameters set in. I've got the uh reward model checkpoint, the SFT checkpoint, and then now we train. So we're training for 50 steps and we'll log every 10 steps. And you will see that based on the metrics, we started with the loss that seems to be uh eventually going down, trending down. Uh I've got the value loss, which again uh gets better at estimating the future uh discounted rewards, the movement of KL from one step to the other. So this is at a perstep level. So whatever I had as a policy at step one versus what I have uh as a policy now. So this is the temporal movement of the policy model. And then from there I have the kale divergence measured between the policy and the frozen reference model. So you will eventually see by by training uh this for a lot of steps that this grows over time. So it starts with a tiny number and then it eventually uh grows over time. Right. So finally we have the uh eval to look at. Um this will be again fairly simple. So let us quickly check this out. So all we need to pass here is the policy checkpoint, the reward checkpoint, the split and the BP directory. And uh just to compare it against uh what we had in the reference, I will also load the SFT model or the reference model so that we can compare uh how the generation has changed. Now obviously uh the reward model that we have is not the best reward model and calibrating to that will actually worsen the responses but the whole idea is to show you that the policy model and the reference model deviation. So without further ado, let's put this in debug mode. Let's actually turn our training off. We've done it already. And then in the eval step we have this. So we load our tokenizer, we load our policy that we have trained in this step. So uh we load that and then we load the reference model that is coming from the SFT part and this we are only doing so that we can compare right so we go through we need our reward model so there we go and for sample prompts let's actually go through one prompt and see what is going So P is a particular prompt that we are interested in measuring against and just to check out what is going on. So give uh three tips for staying healthy. All right. So we have the prefix the ids we do the x and then we do policy.generate reference.generate generate and then we get the response decoded and the response old. So if I look at the response old, I will see that this will be the exact same output of my SFT model. Uh and this also incidentally seems to uh be the exact same. So what we'll do is uh we'll try and train it for more steps and see if that changes. So I've got actually this run over here which will train for I think 100 steps and then let's check it out. So I've disabled all the break points in uh the training. We'll only look at the eval step. So we run our tests as usual and then we start the training this time for 100 steps. And then from there we can see our uh losses moving the different losses. So we've got the overall loss, the value loss, the KL movement, the temporal movement of the policy and uh the KL overall KL divergence from uh the model. So we've uh hit our eval and we'll just go through uh the area where till where we uh generate our responses. So we've got all of that till here and hopefully things will be different in this run. So I've got the response over here which uh now devolves to something else since we have uh trained our uh policy as per the RLHF framework. But if you look at the old model, it will still be the same. So we have significant deviation from where we started. So this essentially is what uh reinforcement learning from human feedback constitutes. So you've got the reward model on one side and then uh we do proximal policy optimization to align it to human preferences by using the reward model to represent as a proxy for the human preference score. So obviously again for a two-layer neural network for 16 data points u this all won't make a lot of sense. You actually need a lot of data. So if I again uh come back to this then uh the reinforcement learning part it's a pretty uh tricky model to optimize. So we've got the policy the reference and a lot of stability tricks uh that we have used in terms of computing the advantage and normalizing it in terms of using the kale co coefficient to kind of uh penalize the reward. So all of it put together hopefully this all now comes together in your head. Uh I recommend watching this video twice, thrice uh over multiple days. Maybe uh do a bunch of forward passes over this video to really consolidate this in your head and then back that up with a lot of theoretical understanding of the different concepts that we've gone through and I'm sure everything will come together beautifully to make sense. Right? So in this part we've done policy network, the reward signal, the PP objective, the training loop and some of the stability tricks. So I hope you had a lot of fun uh going through this series. Uh I did at least and uh it kind of sort of uh cleared a lot of concepts that were unclear to me especially when you come down to implementation. So I highly recommend you grab your laptops and coffee and uh code along with this entire uh entire series and hopefully you have your own takeaways from this as well. Thanks a lot for your time.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Splitting"
      ],
      "metadata": {
        "id": "NSXlFrz-QTKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(\n",
        "     chunk_size=1000,\n",
        "     chunk_overlap=200,\n",
        " )"
      ],
      "metadata": {
        "id": "3dCQU6vIQDZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = splitter.create_documents([text])"
      ],
      "metadata": {
        "id": "W_C2t4MTQ4IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egCuAWhmRDWF",
        "outputId": "80752d7d-3e21-4dfc-93ea-ed51a28f59c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "353"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunks[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amny69wyRFSC",
        "outputId": "8b262d7e-2bb0-4ebd-8fa7-de56bc9a7887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Learn to build a complete large language model from scratch using only pure PyTorch. This course takes you through the entire life cycle from foundational concepts to advanced alignment techniques. You'll begin by implementing the core transformer architecture and training a tiny language model. From there you will modernize and scale the model with production ready enhancements like RO mixture of experts layers and mixed precision training. The course then transitions to the full alignment phase where you'll implement supervised fine-tuning and build a reward model. To complete the life cycle, you'll use proximal policy optimization or PO to align the model with reinforcement learning from human feedback. By the end, you'll have the deep hands-on experience needed to build and customize your own LLMs. >> Hello everyone and welcome to LLM from scratch, a hands-on curriculum in PyTorch. Uh this is going to be a long practical journey where we build modern large language model components'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Generation"
      ],
      "metadata": {
        "id": "Qqh6Y9v_ReNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings(\n",
        "    model = \"text-embedding-3-small\"\n",
        ")"
      ],
      "metadata": {
        "id": "4xJiuohFRIlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Store embeddings in vector store"
      ],
      "metadata": {
        "id": "Ba6tNZpNSPCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = FAISS.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "E6hKsJ7NRweu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.index_to_docstore_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDeMeCaySYlf",
        "outputId": "0b1c47cf-d439-4d0a-bd5f-740397c22c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '1fb69300-ba9f-459b-99b3-aabe973c1dfb',\n",
              " 1: 'ea10ed5c-e154-4f06-a440-28165957672b',\n",
              " 2: '7e4c80ba-3fe7-408d-9b22-8faebb2e3388',\n",
              " 3: '3bc0a681-7ddb-4d8a-a49e-26037a27299b',\n",
              " 4: 'b5f1f09d-4b8a-4a5a-aa56-0e3d07ba78cd',\n",
              " 5: 'b8dbf93a-2a40-41d2-8efd-223f7f15a527',\n",
              " 6: 'dd284dec-e219-416d-b624-b25d71d4d854',\n",
              " 7: 'caea26e6-3abe-4974-a30d-d81f63507811',\n",
              " 8: '2a99d9ca-5c1d-41d4-93e2-ddd601f4a559',\n",
              " 9: '4b8470d0-86bf-4c60-a6d7-c7a4aa3cf2e7',\n",
              " 10: 'b357b644-f90b-415f-8742-03342430957c',\n",
              " 11: 'fbe6d41b-edd9-4b00-b078-089c4a52e757',\n",
              " 12: 'b7c6e6f7-b735-4eba-b6cd-f5c8ad0f96dd',\n",
              " 13: '4b0a698f-10fe-4c37-a832-92ddb72d17ec',\n",
              " 14: 'cbaf6997-9133-4e0b-881d-57756517400c',\n",
              " 15: '29ced1bc-f8bf-4c1c-89cb-86e3af7f3310',\n",
              " 16: 'efc219d5-878f-4dd5-ba1d-83e467e98542',\n",
              " 17: '9772b2fe-8a57-40d1-be6f-1aae68383755',\n",
              " 18: '9a1cf499-043f-4ee5-9dd4-db8e853d19ae',\n",
              " 19: 'ea4dbd84-0667-47fb-a770-115ed179925f',\n",
              " 20: '486a363f-b79c-481d-8e2f-098cf7220e03',\n",
              " 21: '6293f2f6-b48c-40fb-949e-122fca2aa271',\n",
              " 22: 'ac66bbe3-da4c-46b3-8053-1f0cf8ea0f14',\n",
              " 23: '9af5b2ef-8b59-4b4e-acae-5dc807324080',\n",
              " 24: '86ce4a3e-072f-47f2-ba04-b75681b69411',\n",
              " 25: '75760198-0a8b-4652-908d-b6d18daa8395',\n",
              " 26: '89cac344-8ea5-46d9-a4e9-fb468a2d5329',\n",
              " 27: '0b37711f-2637-4c12-93d8-2e7859f6df18',\n",
              " 28: 'e3b822dd-25b7-4204-8184-9233bfc6370f',\n",
              " 29: '16c2d456-ec49-462b-97a5-7a56b87d1737',\n",
              " 30: 'c2b43997-c8c5-4edd-b62c-60410e600bad',\n",
              " 31: '7706037e-c0f9-4b4a-a58f-13fc51f39634',\n",
              " 32: 'bdecadf4-b8a6-4cf3-be7f-2569e372e206',\n",
              " 33: '007ad527-5a75-4ee2-9d88-6c781011baf9',\n",
              " 34: '1b2147ab-4835-4200-aa34-cce8247864be',\n",
              " 35: '7bec3e06-9172-4451-8cfb-9face3538249',\n",
              " 36: 'd5e17062-22cb-41e0-88da-dd7ec1986c45',\n",
              " 37: 'db2d24c3-60dc-462b-b58e-deee785fa3f7',\n",
              " 38: '7f1ee5af-ffc2-4237-b0af-7e4b3fd3ec4b',\n",
              " 39: 'bdd37c4d-fdf1-44d3-9064-a51cc08c0552',\n",
              " 40: 'e000c367-a9da-4468-8278-6cf345f82c79',\n",
              " 41: '35c62841-3f25-41fc-add5-f9b3fdb95bc8',\n",
              " 42: 'b520cec0-d923-41ca-a664-411ac14c0eee',\n",
              " 43: '09b2915b-158e-4801-8ac0-1baca20b528a',\n",
              " 44: '99e36b1d-265d-4326-bbe0-0e88a217e18b',\n",
              " 45: '8e31c042-6b37-4842-a513-8dd4bd12c9d9',\n",
              " 46: '7ee9232e-1aa7-4d62-83e3-22e471231df9',\n",
              " 47: 'cb05a89d-9c65-4c8c-92bc-89f239722e32',\n",
              " 48: 'b51bf007-52a1-476d-ad7c-bb5c253d3b4e',\n",
              " 49: 'dcdf8a94-7f49-4716-89ca-c0704da7f811',\n",
              " 50: 'd623c8c0-f2f8-4f07-b146-9c80cf87e91c',\n",
              " 51: 'aad56e16-5942-4d5e-8873-dd17093efda3',\n",
              " 52: '2af01e3c-5d86-46be-9289-446856f2652d',\n",
              " 53: 'e87fd4c5-1aa1-4ce4-9db9-664c65566abb',\n",
              " 54: '84494e0b-3733-49dd-a3e9-d3c98741a2b4',\n",
              " 55: 'ead4cbe8-fa09-47d8-88c2-36936cbafad3',\n",
              " 56: '620d6a22-1542-4f8e-93ac-90ec520afb7e',\n",
              " 57: '86951dce-b4cf-4f0a-a5aa-15cce07718a1',\n",
              " 58: '207d9187-5a61-4648-b246-844d6673cf2f',\n",
              " 59: '693615e8-6a74-46ac-ac15-4742cadc1bb8',\n",
              " 60: '4e6937c5-1e58-423e-a983-7b39b2a8ca66',\n",
              " 61: '8d4cbdff-bcfd-413b-a9cc-b13bba9bc431',\n",
              " 62: '4ef61e26-ffc2-43fc-8cd4-c21885eb77f0',\n",
              " 63: 'c20e0d3b-8ca6-47e2-a275-a4e99e21857b',\n",
              " 64: '480e1e07-f1d5-4c88-8ecd-e86c26f32d4a',\n",
              " 65: '496613a9-3088-4ed0-ba26-ff4e42fdfc9d',\n",
              " 66: 'a88d5e8b-fb79-41b1-a847-321eb0995ff6',\n",
              " 67: '288468bf-5bf3-4a19-8b98-5851f45fc632',\n",
              " 68: 'eae31acc-3ab4-4d44-81a3-a2398d51189a',\n",
              " 69: '09fb1227-4734-41f9-8e4e-1d9e384d565e',\n",
              " 70: '86fe109a-cdf2-4b6b-8d46-6181cfd8fd1b',\n",
              " 71: '62c6fb2f-7ab4-41aa-9708-7da8a4f093af',\n",
              " 72: 'c24f9597-9e2e-4403-869e-4f2d4d1e055a',\n",
              " 73: '25401434-c797-4cc4-8e19-fc108ee2f294',\n",
              " 74: 'ac699251-2a61-4e80-969f-36f31943ff72',\n",
              " 75: 'bbd62749-1cc7-48f7-a609-cb084362acf6',\n",
              " 76: '213635a1-033a-486d-a08b-2af495f4243c',\n",
              " 77: '9310fb8e-2f7f-4537-83c3-a9e8023246d9',\n",
              " 78: 'ddc1ddb2-b417-45eb-84fc-397679974968',\n",
              " 79: '44e643db-6fd6-4f52-8f2a-881a1a19428f',\n",
              " 80: '79dc313c-9f27-4514-ab3d-0a9a890ed3a4',\n",
              " 81: 'e1b55a4c-d211-40ff-b25d-5ab0c5d4f930',\n",
              " 82: 'd3ab36fb-06e3-438e-973b-024a3f308003',\n",
              " 83: 'd32b6726-ddcc-4b17-a5ff-b4d2e83c670c',\n",
              " 84: '37be5b2a-d9a0-451a-8d66-87e9e35d25c7',\n",
              " 85: 'b8c8f4e9-2869-4952-934e-4f26caf064f3',\n",
              " 86: '2ac8c96d-1727-4ab2-a112-a49c539f6316',\n",
              " 87: '616730c5-eb34-47c2-b971-e8708f9dcac1',\n",
              " 88: '1028c733-db2b-4077-be1e-514a83f19c23',\n",
              " 89: 'e056564d-e9df-42e5-871d-aad94e0bfb08',\n",
              " 90: '75f784d5-61b2-43bb-a1f6-2e5f21670943',\n",
              " 91: 'eab56923-8562-44c4-a161-e819bc274a90',\n",
              " 92: 'e374a32c-c526-485f-b3a7-b2afed4aa77a',\n",
              " 93: '1935627a-794f-40a2-b2c1-03566e314f9b',\n",
              " 94: 'cc05b287-f6a3-404b-a312-bd438555dca6',\n",
              " 95: '48b7fe0d-00f4-4611-afaa-9d665b9710ab',\n",
              " 96: '1dac85eb-659e-463c-a3ab-77c56989031a',\n",
              " 97: 'ab8b9adb-5d11-44f9-8740-05c6ac590d7c',\n",
              " 98: 'b6074c8a-f84d-424f-88f8-9bcaecb89051',\n",
              " 99: '01353dc0-8f48-4916-9d91-2f0fc7385f6c',\n",
              " 100: 'b7a2ab5f-c9d6-45ad-bdc0-f5ede7724c8c',\n",
              " 101: '50f84767-561f-4f6e-b67a-92b0c17ec0a1',\n",
              " 102: 'f3d5fbea-61c7-440c-bdd3-699fbdf059e2',\n",
              " 103: 'cb5b2aa3-310d-4ade-987f-d0a372d95866',\n",
              " 104: '18a41706-4714-4690-bb36-900f7fb2ddd2',\n",
              " 105: '639e069b-7f17-4356-b99c-62b16ed42458',\n",
              " 106: '6025121e-f308-4edc-a264-ecad238f23cc',\n",
              " 107: '4dd1f818-1653-4059-b01a-e3f96363fd24',\n",
              " 108: 'c1b7d6a4-79a3-4559-946b-620475102c04',\n",
              " 109: 'a717f922-8999-4ef5-bdeb-df23d2bb6762',\n",
              " 110: 'ceebf56a-c511-4eba-baaf-918a5c580524',\n",
              " 111: '2b6bea49-279d-46bb-ba6c-1f649dfcf84b',\n",
              " 112: 'b623d656-8f0d-4ec6-a5ea-ba7259b03153',\n",
              " 113: 'ebf747e3-519e-4ab5-b86f-2eeb08b66565',\n",
              " 114: '6da06b89-adfc-462b-be0f-47f03106c109',\n",
              " 115: 'e15e016b-39d7-4a23-a654-66562a43d1ab',\n",
              " 116: '9f9da9e8-f200-4fab-af40-8974efcd79df',\n",
              " 117: 'ffbae7b5-a384-4d4a-a736-5693572b904f',\n",
              " 118: 'f467a56b-7501-45ac-9b34-976fcad650fd',\n",
              " 119: 'bd5caabd-3053-4a93-b27a-05eefed487c7',\n",
              " 120: 'a50d8162-bf84-466c-8301-8390f8a06c42',\n",
              " 121: '92daf8ba-0775-407a-8e3b-32ecad681db5',\n",
              " 122: '642c47d4-a206-46aa-bdd6-09e181f5b9a9',\n",
              " 123: 'd5742205-2dd9-45f5-9fff-69b4efc97a0f',\n",
              " 124: 'fa2c2025-271c-4146-8f2a-105959fae221',\n",
              " 125: '4d5ed0ef-e4e2-4ae2-9d65-217aec4aaa42',\n",
              " 126: 'f33866c7-9b87-4754-9c1b-a867c0e19ee5',\n",
              " 127: '8da91e1e-f7da-4f1d-9ee7-6d9d83e8f53b',\n",
              " 128: '47fd4c8f-a0ac-401d-9912-247bae221817',\n",
              " 129: '8665989d-88d7-4e10-8af7-6ad7f521a674',\n",
              " 130: '973dd68c-a675-4512-8754-4b81705dd13b',\n",
              " 131: 'f05e40d9-8d7d-49fc-bfdb-f2f812ec6581',\n",
              " 132: 'b2cca216-8e33-4f7f-be30-03333fe5ff49',\n",
              " 133: 'bd92dd3e-6d78-4c1a-aa6a-26a458e426db',\n",
              " 134: '7d4c30b0-5533-42ea-b9b1-278efbf60ac0',\n",
              " 135: '96282ec3-f18f-4c54-af91-e8d5772abfdd',\n",
              " 136: '41b59a79-b54e-451e-961c-783008ff58cd',\n",
              " 137: '3501a69f-f0a5-4551-89d2-9806494db220',\n",
              " 138: '297d065b-a414-42f9-acbe-ac338bef66c7',\n",
              " 139: 'e30850fd-14c4-43f6-8df8-1d3bfc67b5ae',\n",
              " 140: '6336f960-64f6-4cb2-ad8e-2288ff6fa3cd',\n",
              " 141: 'd08556a9-2f8e-487b-a005-6d452ef85521',\n",
              " 142: '54cbfc63-ee4f-4bf4-a1bc-d5742fcd4ed5',\n",
              " 143: 'e006b0c3-9482-4d9e-95a1-411dbd475eeb',\n",
              " 144: 'f199e927-02ec-4aab-ae6b-859363a63bfa',\n",
              " 145: '3b0e7d50-a80a-43c9-9a86-45a661572e48',\n",
              " 146: 'b3eb11b6-a0df-455c-96ac-81def7f8345c',\n",
              " 147: '9d07904b-7aee-4d65-80a1-ba73783ba8e7',\n",
              " 148: '5dc6a700-4c1d-44de-a57e-0ff7b69d70c1',\n",
              " 149: '880d7777-dced-4951-8bdb-28c00cb71c50',\n",
              " 150: 'bf3bc30d-77fa-428a-a80f-1f80d4ae9d8d',\n",
              " 151: '56a81fa5-ee69-4ab4-bce9-a1ef9c383a6b',\n",
              " 152: '806e6f5f-c79f-4aa8-84b6-f427c8fe7ff4',\n",
              " 153: 'a990ff19-6e9d-4936-830d-b936dbfc6bad',\n",
              " 154: 'f7ba0ea7-2802-4b7a-9bf8-2fd246331586',\n",
              " 155: 'ced14282-2c95-4a9c-817d-fb33cf16dbc2',\n",
              " 156: '48d6a967-b31a-49ce-b873-fb1393e87d7a',\n",
              " 157: '99385cda-4474-4d5f-84e4-8192ba38f6f7',\n",
              " 158: 'ffdce418-2ef4-4a14-9731-fa8a60e43f81',\n",
              " 159: '5a462501-a167-4248-bec3-f79e78221126',\n",
              " 160: 'fdca1dba-b550-4147-9a72-f349c332b0a2',\n",
              " 161: 'e2feb46c-b9fd-44f6-8889-52ed8bfc078c',\n",
              " 162: '2352f5de-a699-4c27-ac34-aa795624cc4a',\n",
              " 163: '16318a03-350c-4f8a-9353-5395d1e1c777',\n",
              " 164: '8db8f50e-c82b-464f-9fd0-28a47460ac9a',\n",
              " 165: 'e4af3b78-95b6-41f9-b391-acbe8f785e66',\n",
              " 166: '569add13-2c42-47ca-b229-855fc90071c8',\n",
              " 167: '80f56752-9e28-4144-910e-dd00e1352eec',\n",
              " 168: '9399fdbd-6bda-4f49-b813-d9dd45343b72',\n",
              " 169: 'c691a214-6458-45ce-9bf5-df28390c85af',\n",
              " 170: 'adf508c9-c938-409b-88a2-6b9fdad79184',\n",
              " 171: '49ebfd16-6f75-4b3f-9e68-5b3c0e8726fb',\n",
              " 172: '12e22e73-e411-4a9c-bea9-47defda7c1f9',\n",
              " 173: '93ad671c-3606-4c36-b591-26659ff5288f',\n",
              " 174: 'f3d86929-bf8a-4842-ae0e-37b7188d306c',\n",
              " 175: 'c53615c3-4627-4406-95e8-51b4b5fefb79',\n",
              " 176: '469ab834-4bd4-49a4-b9d0-4f4e8b00fc38',\n",
              " 177: 'b1d6f5f8-c886-4602-afc7-8b5778bfdae3',\n",
              " 178: 'c952b032-85f0-406e-a4e6-df765427ffe1',\n",
              " 179: 'ac5e8fb9-dc55-4154-8ff9-fe3fd63867fe',\n",
              " 180: 'ea34afc4-33f3-493c-a56c-9e9bf8d44327',\n",
              " 181: '6914879a-0c78-4391-b1e1-2a2816235398',\n",
              " 182: '60538695-b6fb-4797-add3-a97a089c80ef',\n",
              " 183: 'dd47e9b4-7a9c-4dfb-a8bb-fccdf55b8e3c',\n",
              " 184: 'a0aedf57-7d29-43d1-b35d-842d35a58c57',\n",
              " 185: '06064d0e-a0c9-460f-8873-4f7cc050229e',\n",
              " 186: '30dee59b-472a-49b5-8d68-dc4792df31c2',\n",
              " 187: '80ef4294-8bed-4b24-8561-a8da288ca3cc',\n",
              " 188: '480a833e-d953-46b5-b850-a29b582b43b6',\n",
              " 189: '5e431c70-5dda-42e8-8a99-2274c89adf7a',\n",
              " 190: '97c7937f-e22a-45a4-8a3b-58c58c123220',\n",
              " 191: 'd1d8f949-52d9-4c5e-8d68-ec7e3fbe5620',\n",
              " 192: '5cce9e27-f89f-432f-ac56-b14a2ba3094b',\n",
              " 193: '29566ebd-6277-42dc-8e6d-fd427ac44f98',\n",
              " 194: '8104c807-6bda-4c26-8a0b-1b4c7699b37e',\n",
              " 195: '4a09c6fc-2e56-4923-ab07-2615ddb2f544',\n",
              " 196: '43c12555-999b-4b4e-bcb6-ed99bd0f1d52',\n",
              " 197: '33eb0e28-4cca-47de-ade0-c344e6114a1a',\n",
              " 198: '7457fc89-71ad-465b-8a71-c474932fac17',\n",
              " 199: '511ed1f4-7c42-40bf-8946-93224fcc6655',\n",
              " 200: 'f1420247-7b3f-4fe0-aef0-74eb4f69371b',\n",
              " 201: 'cb3599c4-10fe-4ff6-a50e-e46732f88edf',\n",
              " 202: '7bf077d2-248a-442a-a534-40d4c62fa914',\n",
              " 203: '03662e59-c68b-4af2-aa1b-d879c9700679',\n",
              " 204: 'db1e722b-451b-4bf8-820c-e37a09c2780d',\n",
              " 205: '7980bb12-55ec-46c9-9b81-1be855f862f7',\n",
              " 206: 'de0dc735-885a-4e45-99af-ea405fe55b89',\n",
              " 207: '113f4eac-d513-4920-a4bf-c666e223c2c6',\n",
              " 208: '2c193a42-7361-42d1-89db-f3de1ead6b19',\n",
              " 209: '8a0f4f2d-9149-4f94-927c-1a67c5bcd15e',\n",
              " 210: 'ccdbfc9f-3054-400a-ae04-078bbe04f1ef',\n",
              " 211: '6462fc75-894b-4ea4-853e-26484e96155c',\n",
              " 212: 'c4445080-ec9f-45eb-bc05-5d6cab3d3e8c',\n",
              " 213: '1264503b-c171-4ce9-a499-dacf5637b1fe',\n",
              " 214: '24e67bb2-db1e-48af-9fc4-3418fa0adf30',\n",
              " 215: '254bad9d-55a7-4bb4-bca1-8e6c1401c4ac',\n",
              " 216: '76c9e5aa-d732-4381-9337-2b49ff0c2458',\n",
              " 217: '4d3c9fc0-7101-4439-a7a9-d404c685d565',\n",
              " 218: 'e9ee42f6-16e6-4e48-85f8-39a39db1caa3',\n",
              " 219: 'd09afb21-c671-48a2-b85f-293a3a95c340',\n",
              " 220: '7096720d-63d6-460f-bd59-9970f441b877',\n",
              " 221: '804ea9a3-693f-4bb0-a5b9-131601af84d8',\n",
              " 222: '36b7d052-ea46-4b07-a29c-9f70fa9a5071',\n",
              " 223: '57e2e46d-ea20-4ef9-8007-bdb1297de710',\n",
              " 224: 'a2ba688b-3cd3-4d8b-98e7-b579dc41c1d3',\n",
              " 225: '290ed81f-e24e-4aa2-ab57-370e21dc27ad',\n",
              " 226: 'f1a8f942-4575-40df-96b2-dfa01f3c82f5',\n",
              " 227: 'dc680fe8-9124-48e0-b698-a46ec274645b',\n",
              " 228: 'efbfb76d-bd53-43ed-b98c-41ca7bc0e1a7',\n",
              " 229: '44126564-869c-4481-90a9-27a7d01f13f5',\n",
              " 230: '65eacc3b-422f-4b80-b7c7-53feb77c2b06',\n",
              " 231: '81225bb2-1196-4f3c-b1dc-47457f5ade06',\n",
              " 232: 'f2ca28ab-1153-41e0-87b2-6059aec13305',\n",
              " 233: '54bd349d-8d53-44ec-a820-bf49773050d0',\n",
              " 234: '4886202e-c39c-4836-bb69-066681a4d4b8',\n",
              " 235: '6babc766-a200-4e20-813d-c98d0ac99c6e',\n",
              " 236: '5ae714f1-3223-43ba-b67d-18ae95760407',\n",
              " 237: '0ccda539-95c1-4503-950b-c248c64ed32f',\n",
              " 238: 'cc76a35e-9925-461b-9b18-87dad36c51ec',\n",
              " 239: 'de31a8ab-8770-49db-a87c-8f2d71ee38bc',\n",
              " 240: '39be717d-a346-4812-8ee7-1cb88935d647',\n",
              " 241: '2b4ea281-4ce2-4c49-a574-4924904e39b9',\n",
              " 242: '85e993fe-78c6-4238-b918-22823fa2cd04',\n",
              " 243: '3c812cb7-fe6a-4837-8e2f-6bafaa30c01f',\n",
              " 244: '291c8823-693a-4d60-82d0-e31e4fbbf7eb',\n",
              " 245: '88001127-eb16-4dd0-a7e7-dc55d67f1c37',\n",
              " 246: 'dcedd97a-1302-4d76-8b85-e6fd63cf2696',\n",
              " 247: '33b993e6-f3dc-4f8f-8403-09f4c48a1631',\n",
              " 248: '8b58efae-d32d-4f5f-a2dc-853156b74f64',\n",
              " 249: 'f04b9c8f-7a63-4162-90d9-9b5a92746fe5',\n",
              " 250: 'e238bdd5-49fb-4746-8111-d4c741ab4c44',\n",
              " 251: '5045da1a-cb97-410f-9a90-c0bf6009d5fe',\n",
              " 252: '3d20a898-4040-473e-935e-c01e09cb7156',\n",
              " 253: 'd0406ac1-696c-4a18-ba32-201d59a38386',\n",
              " 254: 'f9ee5988-150b-4654-b8de-d4afbced1c07',\n",
              " 255: '07486da8-d0a3-4a8e-a24f-f18dd9059ee0',\n",
              " 256: '32a4a077-412e-4e78-9301-66ca89324e52',\n",
              " 257: 'd96a8bb5-340d-40b0-b84e-116ee6066590',\n",
              " 258: '7a8726a7-3ee2-4188-9fc9-4220ea27d869',\n",
              " 259: 'aacbffcb-cec0-4616-b130-b8ddd2def51a',\n",
              " 260: 'b63ea03f-520f-45b0-9f06-95026c9163d9',\n",
              " 261: '86f40eb0-5bc3-4e30-9e8e-a8e242e0cd48',\n",
              " 262: '116d10cc-a227-462d-bc6e-4e6b14ed1a8c',\n",
              " 263: 'f1e9d879-850a-42c2-8c73-9e0253d427a6',\n",
              " 264: '3774ab00-fc5c-48e7-ae00-8b58d0b0c541',\n",
              " 265: 'b7a6bc17-f2c5-405b-b215-02a15a33fdee',\n",
              " 266: '67a8c7d7-a4d3-43a0-9602-8256c8016b48',\n",
              " 267: 'cd97cb82-ae9b-4e71-9199-d870b67a7c4e',\n",
              " 268: '46764e40-fecc-4eb8-8905-5e6237cacf5e',\n",
              " 269: '97fe4b0d-270f-4f92-8a48-129e106fe23d',\n",
              " 270: '010c4a90-a6a9-464e-a87d-2972f82115da',\n",
              " 271: '99c8bb66-4a02-45d2-93f4-59604321de0e',\n",
              " 272: 'fc83228b-36c2-4315-8dc2-22ab6b39f113',\n",
              " 273: '7775136c-d008-49b8-97fe-e2df5ad7db64',\n",
              " 274: '5359d6ab-6c16-4863-b307-6be39fac9bfe',\n",
              " 275: 'c2d0d8bf-7949-4cac-8868-ee4a63a02ec2',\n",
              " 276: '68164b03-9e92-43e7-a127-4673a8e7e954',\n",
              " 277: 'd7f3762e-4b13-4ab9-b046-14c5a22828f6',\n",
              " 278: '2735ca9b-4aaf-4579-86ec-cdd973fa6968',\n",
              " 279: '2646f5e0-7543-43d2-a877-fdbca0a7c6a6',\n",
              " 280: '3e9667ee-74ad-464b-9b0d-34c6703b72f5',\n",
              " 281: 'cabe7890-0781-4639-97ce-4264fde6307f',\n",
              " 282: '7ef46268-cea3-4e34-a3b9-192f02e12176',\n",
              " 283: '48c3ddfb-883e-4159-960c-8fb57afb7d2f',\n",
              " 284: '07955324-4b27-4c26-aa51-89dc7752700f',\n",
              " 285: 'b61e192e-63a9-4890-8f5b-126261a2b7c0',\n",
              " 286: '3e738ec3-ba01-4ef8-adbe-e387bf89d085',\n",
              " 287: 'bc705bbe-89a4-4b51-b3e0-5045c4092bf6',\n",
              " 288: '8c2ba2d0-3c31-4d8e-bf47-e364cb44eedf',\n",
              " 289: 'b6996021-415e-41fa-886d-fea56391d77c',\n",
              " 290: '19cb657c-9958-4aa9-9fbf-18ee694b39cf',\n",
              " 291: '389d2a50-e3cb-4b3d-91a4-8f61a4602ca7',\n",
              " 292: '811b084e-91ea-4a15-a7be-dc8660046c4d',\n",
              " 293: 'b39d952e-eb5b-4609-91b5-c1b2e996f2aa',\n",
              " 294: 'd1b33005-4a52-40fa-8ca9-01526eee1ba6',\n",
              " 295: '6898cf5a-5d49-47ca-a9a4-1f0748af23e5',\n",
              " 296: '4cca7c27-865d-4b2a-9e50-1847bef1f0f0',\n",
              " 297: '93ef10d3-db5b-4069-bc3a-c241891be9f9',\n",
              " 298: '0c5f0dfa-a0a6-48c6-9b93-3d7f41130bff',\n",
              " 299: '9c5bd8ff-8e48-4539-bab0-dca955712069',\n",
              " 300: '61f2067d-f512-4cd5-ab52-2cd28be94181',\n",
              " 301: '20de8292-405d-4442-8a67-2e387f50b1bf',\n",
              " 302: 'c44985ae-9443-4a5b-91b2-bf222cde255e',\n",
              " 303: '40c91271-9bf7-4ffa-8057-f2929d8ee1b4',\n",
              " 304: '62397130-a9a6-47aa-8b6f-b0105a52355a',\n",
              " 305: '71c6186f-dda8-477d-86f7-118de5f1b33c',\n",
              " 306: 'ae90a0b0-89e9-4046-b4b4-019631ddf25b',\n",
              " 307: '355a04f3-9d4d-421e-86bd-9972474e9182',\n",
              " 308: '88927482-8610-4034-b01d-06dec7428ee2',\n",
              " 309: '477da166-5ce1-441a-9c9a-60783862554e',\n",
              " 310: 'afd954be-1457-42ab-a85c-35fe9e5dd3bd',\n",
              " 311: '82606cae-be2f-4e28-8706-5f06f5b843c2',\n",
              " 312: '19d48717-ce73-49e5-bbe6-80ba65808013',\n",
              " 313: 'cd7b84e6-21d3-4939-982b-f3ebe5a071cb',\n",
              " 314: '96f7f1c7-39f9-451c-baed-1f22ac524d95',\n",
              " 315: 'b5f87de9-38b7-47b0-8f80-413f131d8015',\n",
              " 316: '7459a3e8-6d87-46aa-b9ab-f5be172b5ce6',\n",
              " 317: 'bca19114-c373-41e8-97de-d1cd1cc49a18',\n",
              " 318: '6a9ff16e-e6ce-474b-a0ac-abcc69ed7ffe',\n",
              " 319: 'dd4b03d3-a728-4795-8e9a-53453c959ad5',\n",
              " 320: '469df6ec-9846-4ab3-8b4f-51e39c28d5b1',\n",
              " 321: '817339cd-1d0d-4adb-81b2-29182b95f252',\n",
              " 322: 'c7c581bd-2212-4be3-a0f6-a176bc94825b',\n",
              " 323: 'e92dcc09-eb87-4987-909e-737142f705f2',\n",
              " 324: 'ee0fb9a5-62f0-4ffc-85f1-c81a0bd4b22e',\n",
              " 325: 'e356ecd3-9c23-4ff1-8b35-dd58e3f1b897',\n",
              " 326: 'b74d9696-3503-4cb4-b9c2-10aa4b92ac52',\n",
              " 327: '238d1b23-fc93-4a44-89e7-54fb30a6b277',\n",
              " 328: '4eef950d-09dc-453c-b37f-4acc14d9937f',\n",
              " 329: 'b7ebfbb5-2514-484c-b796-130d116442b2',\n",
              " 330: '9199b7fc-86e5-47f0-8b4c-5a8739e46a90',\n",
              " 331: '482665a1-6943-4fa7-94da-7ede8668d885',\n",
              " 332: '3194c0a6-de55-4c9f-a8b3-b681e22fa6ba',\n",
              " 333: '8ba5bda9-8e4d-4f84-af82-b0fb126bd661',\n",
              " 334: 'e867c06f-e6eb-41c7-999c-3b341462f29a',\n",
              " 335: '538eaacb-4490-4ebc-99d3-267772d672ef',\n",
              " 336: '7794f5c6-2c5e-4f12-87ab-dfd7a359eb6e',\n",
              " 337: 'fa682ed4-3cf0-4487-887a-60e396f7f0df',\n",
              " 338: 'c027bb39-60bf-45fe-9ee6-67f7cf277e7e',\n",
              " 339: '70383d3c-1a4e-43d3-b3d1-19d9fbc9b7f9',\n",
              " 340: '153a4be0-32ee-4fb4-8367-8173df95b5c7',\n",
              " 341: 'd1caa396-6b36-409f-8ce6-a760f9d2e114',\n",
              " 342: '9cf26b53-7edd-4e96-8657-e113c9390c31',\n",
              " 343: '3d0c494b-fa8a-4bc5-9a9b-3c154819cc0c',\n",
              " 344: '2730a9b6-73ee-4499-8063-484ad82a0e22',\n",
              " 345: '497e563c-0421-4cc7-9a5c-92afdf0c38db',\n",
              " 346: '0e932486-5a6c-450a-9ac7-0715d8562e6e',\n",
              " 347: '11b193b3-94e5-4d48-89ac-ee32260e7a15',\n",
              " 348: 'b47be4d1-942d-475a-9aca-e9ec51a3e061',\n",
              " 349: '5a237160-eac2-49e1-8e24-a7b5a37855be',\n",
              " 350: 'e3e33180-50ab-4e20-8536-8435f75440ac',\n",
              " 351: '1b6602bf-72d7-45cf-8704-fc702d222c38',\n",
              " 352: 'eca1eb50-66e0-4054-9df7-a4e7318c077e'}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step2: Retrieval"
      ],
      "metadata": {
        "id": "UnUMHyECU2zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve top 4 documents form vector store\n",
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\":4}\n",
        ")"
      ],
      "metadata": {
        "id": "A6vBf5wDSbef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cJsZs25VHrx",
        "outputId": "2449552b-f6b5-4302-d8b0-27ecf3712d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x795af8bb2390>, search_kwargs={'k': 4})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(\"What is RLHF\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxKz7-HAVKIg",
        "outputId": "068cf74c-7b7c-4c55-c089-a22455682b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='e3e33180-50ab-4e20-8536-8435f75440ac', metadata={}, page_content=\"temporal movement of the policy and uh the KL overall KL divergence from uh the model. So we've uh hit our eval and we'll just go through uh the area where till where we uh generate our responses. So we've got all of that till here and hopefully things will be different in this run. So I've got the response over here which uh now devolves to something else since we have uh trained our uh policy as per the RLHF framework. But if you look at the old model, it will still be the same. So we have significant deviation from where we started. So this essentially is what uh reinforcement learning from human feedback constitutes. So you've got the reward model on one side and then uh we do proximal policy optimization to align it to human preferences by using the reward model to represent as a proxy for the human preference score. So obviously again for a two-layer neural network for 16 data points u this all won't make a lot of sense. You actually need a lot of data. So if I again uh come\"),\n",
              " Document(id='f1e9d879-850a-42c2-8c73-9e0253d427a6', metadata={}, page_content=\"This can either be train or test. And then this time we are trying to load anthropics uh RLHF data set. Let's take a look at it. So here's anthropics RLHF data set. There are two splits train and test. And for each one there are only two columns chosen and rejected. So if I were to look at one of these, there's an entire context, the instruction and the prompt and even in some cases the history of the conversation. And then all you have to do is uh rate one over the other. So this is the chosen one. This is the rejected one. And this is as per the alignment or the preferences that you're looking for in order to build your own large language model. So some might like a more fun persona, some might want a more neutral persona, some might want to avoid political biases and so on. It's really based on human preferences. So from there we get the chosen and the rejected columns and then we append it to our preference example. So if we just run through this We've loaded our data and then\"),\n",
              " Document(id='75f784d5-61b2-43bb-a1f6-2e5f21670943', metadata={}, page_content=\"we've covered all the topics in great details and I hope you had a lot of fun doing it. Thank you. Hello and welcome to part three of the LLM from scratch series. And in this part uh we'll look at what we have implemented and try and modernize the existing transformer architecture. So step by step uh we'll introduce all of these concepts and also look at how we can put these together in code to achieve um one of the more modern forms of the LLM architecture that we have. So first things first we'll start concept by concept. What I want to do is spend some time doing some theory u picking up each of these concepts apart and explaining them separately and then we'll figure out a way how to put them together. So first we'll look at RMSOM or root mean square normalization. Now this is going to be an alternate to the layer normalization module that we are doing. And what this does is normalize the vector the incoming vector by the root mean square of it. So it's literally divided or\"),\n",
              " Document(id='116d10cc-a227-462d-bc6e-4e6b14ed1a8c', metadata={}, page_content=\"original LLM uh orders of magnitude lesser because the task at hand is to preference score which literally gives out a scaler and you don't have to go through the nuances of perplexity and outputting one token at a time right so uh the best way I have seen as per the last part is to uh run it in debug mode directly so we're going to put a break point here and then propagate through it ourselves so we have our orchestrator running and then this is running for the tests. So we'll just skip it and wait till we reach our uh uh train rm.py. So we've reached our device over here and then from there we forward propagate. Now in this load preferences uh the style of collating the data is going to be very similar to the previous part. So we'll just step into it and look at it. So we've got a split. This can either be train or test. And then this time we are trying to load anthropics uh RLHF data set. Let's take a look at it. So here's anthropics RLHF data set. There are two splits train and\")]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step3: Augmentation -> (query + embeddings)*italicized text*"
      ],
      "metadata": {
        "id": "XBzAYyeXV7A8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template = \"\"\"\n",
        "    You are a helpful assistant.\n",
        "    Answer only from the provided transcript context.\n",
        "    If the context is insufficient, just say you don't know.\n",
        "    {context}\n",
        "    Question: {question}\n",
        "    \"\"\",\n",
        "    input_variables = [\"context\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "7g97MWATVZeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Is the RLHF is discussed in the video if yes what was discussed\"\n"
      ],
      "metadata": {
        "id": "cgBGLQ5XXB_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs = retriever.invoke(question)"
      ],
      "metadata": {
        "id": "pHcX7Pl4XPLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrieved_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEa1VDI8XRai",
        "outputId": "aff439d2-678a-4040-e6f1-272690cfe15a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id='4d3c9fc0-7101-4439-a7a9-d404c685d565', metadata={}, page_content=\"of the alpha parameter that we've configured. So in this case again uh equal importance will be paid since our default is.5. So you can go for a hybrid implementation as well. So if you come back uh we read very minimal again used very minimal necessary force to understand theory on expert routing gating networks and load balancing. I really wanted you to have that experience while we were running the code and looking at the output and not get stuck in a bunch of theory. So we've also looked at how uh layers are implemented in PyTorch. Now this along with the part three which was modernizing the architecture and part four which was the scaling up of training will form the meat of what it looks like to have a production grade LLM trained uh training stability and communication and along with uh combining the with the dense layers for hybrid architecture. Right. So with that I think we are done with the part five as well. Uh I hope all of this is still making sense to you. We were very\"),\n",
              " Document(id='75f784d5-61b2-43bb-a1f6-2e5f21670943', metadata={}, page_content=\"we've covered all the topics in great details and I hope you had a lot of fun doing it. Thank you. Hello and welcome to part three of the LLM from scratch series. And in this part uh we'll look at what we have implemented and try and modernize the existing transformer architecture. So step by step uh we'll introduce all of these concepts and also look at how we can put these together in code to achieve um one of the more modern forms of the LLM architecture that we have. So first things first we'll start concept by concept. What I want to do is spend some time doing some theory u picking up each of these concepts apart and explaining them separately and then we'll figure out a way how to put them together. So first we'll look at RMSOM or root mean square normalization. Now this is going to be an alternate to the layer normalization module that we are doing. And what this does is normalize the vector the incoming vector by the root mean square of it. So it's literally divided or\"),\n",
              " Document(id='e3e33180-50ab-4e20-8536-8435f75440ac', metadata={}, page_content=\"temporal movement of the policy and uh the KL overall KL divergence from uh the model. So we've uh hit our eval and we'll just go through uh the area where till where we uh generate our responses. So we've got all of that till here and hopefully things will be different in this run. So I've got the response over here which uh now devolves to something else since we have uh trained our uh policy as per the RLHF framework. But if you look at the old model, it will still be the same. So we have significant deviation from where we started. So this essentially is what uh reinforcement learning from human feedback constitutes. So you've got the reward model on one side and then uh we do proximal policy optimization to align it to human preferences by using the reward model to represent as a proxy for the human preference score. So obviously again for a two-layer neural network for 16 data points u this all won't make a lot of sense. You actually need a lot of data. So if I again uh come\"),\n",
              " Document(id='f1e9d879-850a-42c2-8c73-9e0253d427a6', metadata={}, page_content=\"This can either be train or test. And then this time we are trying to load anthropics uh RLHF data set. Let's take a look at it. So here's anthropics RLHF data set. There are two splits train and test. And for each one there are only two columns chosen and rejected. So if I were to look at one of these, there's an entire context, the instruction and the prompt and even in some cases the history of the conversation. And then all you have to do is uh rate one over the other. So this is the chosen one. This is the rejected one. And this is as per the alignment or the preferences that you're looking for in order to build your own large language model. So some might like a more fun persona, some might want a more neutral persona, some might want to avoid political biases and so on. It's really based on human preferences. So from there we get the chosen and the rejected columns and then we append it to our preference example. So if we just run through this We've loaded our data and then\")]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)"
      ],
      "metadata": {
        "id": "bqcXCzlDXd4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "2POFj8wlX3jj",
        "outputId": "26125fd2-cc27-4d44-cd47-588e4bd7ba52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"of the alpha parameter that we've configured. So in this case again uh equal importance will be paid since our default is.5. So you can go for a hybrid implementation as well. So if you come back uh we read very minimal again used very minimal necessary force to understand theory on expert routing gating networks and load balancing. I really wanted you to have that experience while we were running the code and looking at the output and not get stuck in a bunch of theory. So we've also looked at how uh layers are implemented in PyTorch. Now this along with the part three which was modernizing the architecture and part four which was the scaling up of training will form the meat of what it looks like to have a production grade LLM trained uh training stability and communication and along with uh combining the with the dense layers for hybrid architecture. Right. So with that I think we are done with the part five as well. Uh I hope all of this is still making sense to you. We were very\\n\\nwe've covered all the topics in great details and I hope you had a lot of fun doing it. Thank you. Hello and welcome to part three of the LLM from scratch series. And in this part uh we'll look at what we have implemented and try and modernize the existing transformer architecture. So step by step uh we'll introduce all of these concepts and also look at how we can put these together in code to achieve um one of the more modern forms of the LLM architecture that we have. So first things first we'll start concept by concept. What I want to do is spend some time doing some theory u picking up each of these concepts apart and explaining them separately and then we'll figure out a way how to put them together. So first we'll look at RMSOM or root mean square normalization. Now this is going to be an alternate to the layer normalization module that we are doing. And what this does is normalize the vector the incoming vector by the root mean square of it. So it's literally divided or\\n\\ntemporal movement of the policy and uh the KL overall KL divergence from uh the model. So we've uh hit our eval and we'll just go through uh the area where till where we uh generate our responses. So we've got all of that till here and hopefully things will be different in this run. So I've got the response over here which uh now devolves to something else since we have uh trained our uh policy as per the RLHF framework. But if you look at the old model, it will still be the same. So we have significant deviation from where we started. So this essentially is what uh reinforcement learning from human feedback constitutes. So you've got the reward model on one side and then uh we do proximal policy optimization to align it to human preferences by using the reward model to represent as a proxy for the human preference score. So obviously again for a two-layer neural network for 16 data points u this all won't make a lot of sense. You actually need a lot of data. So if I again uh come\\n\\nThis can either be train or test. And then this time we are trying to load anthropics uh RLHF data set. Let's take a look at it. So here's anthropics RLHF data set. There are two splits train and test. And for each one there are only two columns chosen and rejected. So if I were to look at one of these, there's an entire context, the instruction and the prompt and even in some cases the history of the conversation. And then all you have to do is uh rate one over the other. So this is the chosen one. This is the rejected one. And this is as per the alignment or the preferences that you're looking for in order to build your own large language model. So some might like a more fun persona, some might want a more neutral persona, some might want to avoid political biases and so on. It's really based on human preferences. So from there we get the chosen and the rejected columns and then we append it to our preference example. So if we just run through this We've loaded our data and then\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create final prompt\n",
        "final_prompt = prompt.invoke(\n",
        "    {\n",
        "        \"context\": context_text,\n",
        "        \"question\": question\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "R7XbY6jvX443"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_umbygOSYQyO",
        "outputId": "8c186300-b17c-4a79-e749-db4d8b64b88a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text=\"\\n    You are a helpful assistant.\\n    Answer only from the provided transcript context.\\n    If the context is insufficient, just say you don't know.\\n    of the alpha parameter that we've configured. So in this case again uh equal importance will be paid since our default is.5. So you can go for a hybrid implementation as well. So if you come back uh we read very minimal again used very minimal necessary force to understand theory on expert routing gating networks and load balancing. I really wanted you to have that experience while we were running the code and looking at the output and not get stuck in a bunch of theory. So we've also looked at how uh layers are implemented in PyTorch. Now this along with the part three which was modernizing the architecture and part four which was the scaling up of training will form the meat of what it looks like to have a production grade LLM trained uh training stability and communication and along with uh combining the with the dense layers for hybrid architecture. Right. So with that I think we are done with the part five as well. Uh I hope all of this is still making sense to you. We were very\\n\\nwe've covered all the topics in great details and I hope you had a lot of fun doing it. Thank you. Hello and welcome to part three of the LLM from scratch series. And in this part uh we'll look at what we have implemented and try and modernize the existing transformer architecture. So step by step uh we'll introduce all of these concepts and also look at how we can put these together in code to achieve um one of the more modern forms of the LLM architecture that we have. So first things first we'll start concept by concept. What I want to do is spend some time doing some theory u picking up each of these concepts apart and explaining them separately and then we'll figure out a way how to put them together. So first we'll look at RMSOM or root mean square normalization. Now this is going to be an alternate to the layer normalization module that we are doing. And what this does is normalize the vector the incoming vector by the root mean square of it. So it's literally divided or\\n\\ntemporal movement of the policy and uh the KL overall KL divergence from uh the model. So we've uh hit our eval and we'll just go through uh the area where till where we uh generate our responses. So we've got all of that till here and hopefully things will be different in this run. So I've got the response over here which uh now devolves to something else since we have uh trained our uh policy as per the RLHF framework. But if you look at the old model, it will still be the same. So we have significant deviation from where we started. So this essentially is what uh reinforcement learning from human feedback constitutes. So you've got the reward model on one side and then uh we do proximal policy optimization to align it to human preferences by using the reward model to represent as a proxy for the human preference score. So obviously again for a two-layer neural network for 16 data points u this all won't make a lot of sense. You actually need a lot of data. So if I again uh come\\n\\nThis can either be train or test. And then this time we are trying to load anthropics uh RLHF data set. Let's take a look at it. So here's anthropics RLHF data set. There are two splits train and test. And for each one there are only two columns chosen and rejected. So if I were to look at one of these, there's an entire context, the instruction and the prompt and even in some cases the history of the conversation. And then all you have to do is uh rate one over the other. So this is the chosen one. This is the rejected one. And this is as per the alignment or the preferences that you're looking for in order to build your own large language model. So some might like a more fun persona, some might want a more neutral persona, some might want to avoid political biases and so on. It's really based on human preferences. So from there we get the chosen and the rejected columns and then we append it to our preference example. So if we just run through this We've loaded our data and then\\n    Question: Is the RLHF is discussed in the video if yes what was discussed\\n    \")"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step4: Generation"
      ],
      "metadata": {
        "id": "DTGd1mE9Ynqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    model = \"gpt-4o-mini\",\n",
        "    temperature = 0.2\n",
        ")"
      ],
      "metadata": {
        "id": "upKbenh5ZW2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = llm.invoke(final_prompt)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ny53kVPTYStA",
        "outputId": "a4911c7a-9d4f-420f-9158-e176bb7462d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Yes, RLHF (Reinforcement Learning from Human Feedback) is discussed in the video. It is explained that the process involves a reward model on one side and proximal policy optimization to align the model to human preferences. The reward model serves as a proxy for human preference scores, and the discussion highlights the need for a significant amount of data to make sense of the training process. Additionally, the video mentions the anthropics RLHF dataset, which includes chosen and rejected columns based on human preferences for building a large language model.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 105, 'prompt_tokens': 884, 'total_tokens': 989, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CaJo57YT5twJq2UUhJr4WhIYx4JNg', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--18f83cd6-efd8-4329-9d7e-0b066b96f573-0' usage_metadata={'input_tokens': 884, 'output_tokens': 105, 'total_tokens': 989, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a chain"
      ],
      "metadata": {
        "id": "46pgxefWbuIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import  StrOutputParser"
      ],
      "metadata": {
        "id": "Etv_bxemZTxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(retrieved_docs):\n",
        "  context_text = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "  return context_text"
      ],
      "metadata": {
        "id": "b5nkxFgJcXmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_chain = RunnableParallel({\n",
        "    'context': retriever | RunnableLambda(format_docs),\n",
        "    'question': RunnablePassthrough()\n",
        "})"
      ],
      "metadata": {
        "id": "VtxCWt23co9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parallel_chain.invoke(\"What is Mixture of Experts\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iztv7C42c2kT",
        "outputId": "0bd0117c-9b60-4613-af33-257cb86719f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': \"then from there we'll try to watch a worked out example to figure out how it all comes together. So first things first, what is mixture of expert? Now here we have a rough diagram of the architecture of mixture of experts. Now as you can see all of the uh attention blocks have been collapsed in. So this can be thought of as the more modern version of multi-headed attention with the group query attention and the swigloo and the rope and all the bells and whistles that we have discussed so far. But from there instead of feed forward network what happens is there's a gating function over here or a router which takes the output from the attention module and then decides to route it to one of these or one or more of these experts. So this router will have probabilities emitting from it and based on that we are going to route it to one or more of these experts. Each of these experts is a multi-layer perceptron or simply a linear layer and then from there the whole thing goes forward. What's\\n\\nto have an MOE setup. This is because less number of activations means lesser computations means more speed. So during training time, the model fine-tunes itself to pick the right combination of experts. So you can think of it intuitively as one can be the coding expert, one can be the language expert and so on. uh the part where this particular experts tends to certain tokens is not within ourselves. It will be within the learning process and then from there the whole thing propagates and we have more layers of this stacked together. So it'll look like the model will have a lot of parameters but only a fraction of them will be activated during inference time. So that's the intuition or the inspiration behind learning mixture of experts. So we'll jump right down to the implementation part with that short introduction. So we've got the orchestrator.py and along with it we'll look at all of the uh mixture of experts components that will lead up to a hybrid architecture which can be a\\n\\noriginal implementations of the mixture of experts module. So we come back and we watch the initialization. So the dim parameter is nothing but the D model or the number of dimensions uh that will go in to this particular layer. And then we have the total number of experts, the topk experts, the topk gate, this is going to be our router or the gating function. And then we'll have a module list of all of the experts. So as per the number of experts, we are creating a module list of the expert MLPS that we're going to configure. So we've got the dimensionality input going in and the multiplying factor is going to be how much more hidden neurons we want against each piece of input. So we've configured this as four. So that means a 128 embedding coming in will have 128 * 4 number of hidden parameters in the MLP. So we're using swigloo and drop out if necessary. I think this is turned off. So let's look at the top gate. So here we have the meat of what is going on in the mixture of experts\\n\\nHopefully you learned something and had fun while doing it. Thanks a lot. Hello and welcome to part five uh of the LLM from scratch series. Uh in this part we are going to cover mixture of experts. Now, we've spent some time over the last two parts modernizing the architecture as well as going through how a production grade training might work. In this, we're going to concentrate on one more piece of modernization of the core LLM architecture from the Mandela transformers. So, we've taken care of modernizing the attention block. This time we're going to focus on uh the feed forward network that happens outside the attention block but within the transformer block. So we're going to cover some theory on expert routing, gating networks and load balancing. We'll jump to the implementation and then from there we'll try to watch a worked out example to figure out how it all comes together. So first things first, what is mixture of expert? Now here we have a rough diagram of the architecture\",\n",
              " 'question': 'What is Mixture of Experts'}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parser\n",
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "LM9JP29sdJx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_chain = parallel_chain | prompt | llm | parser"
      ],
      "metadata": {
        "id": "XCtXP-DKeCDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_chain.invoke(\"Can you summarize the video\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "rDMon77leEwE",
        "outputId": "d7180a7d-b3c6-4a5c-afa2-7010fbd4be63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The video discusses a series of parts focused on modernizing and implementing various aspects of transformer architecture and related concepts. It covers topics such as policy networks, reward signals, training loops, and stability tricks. The upcoming parts will include modernizing the vanilla transformer architecture with techniques like RMS normalization and sliding window attention, scaling up tokenization, implementing mixture of experts, supervised fine-tuning, and creating a reward model using pair-wise preference datasets. The presenter encourages viewers to code along for better understanding and mentions the use of visualization tools to enhance comprehension of attention mechanisms. The video concludes with instructions on running code to visualize multi-headed attention.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BwrFXNxReLfw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}